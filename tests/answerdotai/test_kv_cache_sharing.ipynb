{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/miniconda3/envs/llm_quant/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-11 12:08:46,479\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/home/k/miniconda3/envs/llm_quant/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/k/miniconda3/envs/llm_quant/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.attention.backends.abstract import AttentionType, SharedSelfAttentionType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enforce xformers attention backend - changes are made here\n",
    "os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"XFORMERS_CLA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Inside LlamaAttention.forward\n",
    "# Change q to fixed input for testing.\n",
    "if self.cache_config.debug_kv_sharing: q = torch.ones_like(q)\n",
    "attn_output = self.attn(q, k, v, kv_cache, attn_metadata, compute_new_kv=compute_new_kv_map)\n",
    "if self.cache_config.debug_kv_sharing: self.attn_outputs.append(attn_output)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "    \"Reset attn outputs and attn metadatas from warmup.\"\n",
    "    for layer in model.model.layers:\n",
    "        if len(layer.self_attn.attn_outputs) > 0:\n",
    "            layer.self_attn.attn_outputs = []\n",
    "    if len(model.model.attn_metadatas) > 0:\n",
    "        model.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-10 15:31:59 config.py:365] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 10-10 15:31:59 llm_engine.py:237] Initializing an LLM engine (v0.1.dev2968+g2fa3c83) with config: model='/home/k/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/k/models/dummy-llama, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n",
      "ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 15:32:00 selector.py:122] Using XFormers CLA backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/miniconda3/envs/llm_quant/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/k/miniconda3/envs/llm_quant/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 15:32:01 model_runner.py:1049] Starting to load model /home/k/models/dummy-llama...\n",
      "INFO 10-10 15:32:01 selector.py:122] Using XFormers CLA backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.33it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 15:32:02 model_runner.py:1060] Loading model weights took 2.8008 GB\n",
      "INFO 10-10 15:32:03 gpu_executor.py:122] # GPU blocks: 5937, # CPU blocks: 65536\n"
     ]
    }
   ],
   "source": [
    "### test 1: No prefix caching, no quantization, use v1 block manager, no cuda graphs ###\n",
    "ENABLE_PREFIX_CACHING = False\n",
    "KV_CACHE_DTYPE = \"auto\"\n",
    "USE_V2_BLOCK_MANAGER = True\n",
    "ENFORCE_EAGER = True\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/home/k/models/dummy-llama\", \n",
    "                         tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "                         enforce_eager=ENFORCE_EAGER, \n",
    "                         kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "                         enable_prefix_caching=ENABLE_PREFIX_CACHING, \n",
    "                         use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "                         tensor_parallel_size=1, \n",
    "                         dtype=\"bfloat16\", \n",
    "                         max_model_len=1024*1, \n",
    "                         kv_cache_map={0:0, 1:0}, \n",
    "                         gpu_memory_utilization=0.2,\n",
    "                         debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "    # Reset attn outputs and attn metadatas from warmup.\n",
    "    if len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[0].self_attn.attn_outputs = []\n",
    "    if len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[1].self_attn.attn_outputs = []\n",
    "    if len(model.model.attn_metadatas) > 0:\n",
    "        model.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 51.28it/s, est. speed input: 154.39 toks/s, output: 51.45 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20.99it/s, est. speed input: 63.06 toks/s, output: 63.05 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# 3 tokens decode.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "for attn_output_layer0, attn_output_layer1 in zip(model.model.layers[0].self_attn.attn_outputs, \n",
    "                                                  model.model.layers[1].self_attn.attn_outputs):\n",
    "    assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "\n",
    "# 3 attention metadata for a 3 model forward pass (1 prefill + 2 decode)\n",
    "assert len(model.model.attn_metadatas) == 3 \n",
    "# Prefill phase.\n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "assert attn_metadata.decode_metadata is None\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 1.\n",
    "attn_metadata = model.model.attn_metadatas[1]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 2.\n",
    "attn_metadata = model.model.attn_metadatas[2]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 2 (fp8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if kv_cache_dtype in (\"fp8\", \"fp8_e4m3\"):\n",
    "    target_dtype = torch.float8_e4m3fn\n",
    "elif kv_cache_dtype == \"fp8_e5m2\":\n",
    "    target_dtype = torch.float8_e5m2\n",
    "```\n",
    "\n",
    "Quantiztion stack: \n",
    "- PagedAttention.write_to_paged_cache\n",
    "- _custom_ops.reshape_and_cache\n",
    "- torch.ops._C_cache_ops.reshape_and_cache\n",
    "- cache_kernels.cu/reshape_and_cache\n",
    "- CALL_RESHAPE_AND_CACHE (macro)\n",
    "- reshape_and_cache_kernel\n",
    "- fp8::scaled_convert\n",
    "- scaled_vec_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-11 07:32:25 config.py:629] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\n",
      "INFO 10-11 07:32:25 llm_engine.py:237] Initializing an LLM engine (v0.1.dev2968+g2fa3c83) with config: model='/home/k/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=fp8, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/k/models/dummy-llama, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n",
      "ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-11 07:32:27 model_runner.py:1049] Starting to load model /home/k/models/dummy-llama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-11 07:32:28 model_runner.py:1060] Loading model weights took 2.7696 GB\n",
      "INFO 10-11 07:32:29 gpu_executor.py:122] # GPU blocks: 16610, # CPU blocks: 131072\n",
      "INFO 10-11 07:32:32 model_runner.py:1383] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-11 07:32:32 model_runner.py:1387] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-11 07:33:14 model_runner.py:1511] Graph capturing finished in 43 secs.\n"
     ]
    }
   ],
   "source": [
    "### test 1: No prefix caching, fp8 quantization, use v1 block manager, no cuda graphs ###\n",
    "ENABLE_PREFIX_CACHING = False\n",
    "KV_CACHE_DTYPE = \"fp8\"\n",
    "USE_V2_BLOCK_MANAGER = True\n",
    "ENFORCE_EAGER = False\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/home/k/models/dummy-llama\", \n",
    "                         tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "                         enforce_eager=ENFORCE_EAGER, \n",
    "                         kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "                         enable_prefix_caching=ENABLE_PREFIX_CACHING, \n",
    "                         use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "                         tensor_parallel_size=1, \n",
    "                         dtype=\"bfloat16\",\n",
    "                         max_model_len=1024*1, \n",
    "                         kv_cache_map={0:0, 1:0}, \n",
    "                         gpu_memory_utilization=0.2, \n",
    "                         debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "    # Reset attn outputs and attn metadatas from warmup.\n",
    "    if len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[0].self_attn.attn_outputs = []\n",
    "    if len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[1].self_attn.attn_outputs = []\n",
    "    if len(model.model.attn_metadatas) > 0:\n",
    "        model.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 186.49it/s, est. speed input: 566.25 toks/s, output: 188.50 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.allclose(attn_output_layer0, attn_output_layer1, atol=1e-2, rtol=1e-2)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[ 0.0109, -0.0005,  0.0287,  ...,  0.0007, -0.0010,  0.0004],\n",
       "          [ 0.0013,  0.0067,  0.0334,  ...,  0.0557, -0.0334,  0.0114],\n",
       "          [ 0.0029,  0.0021,  0.0306,  ...,  0.0140, -0.0356,  0.0200],\n",
       "          [ 0.0019,  0.0014,  0.0311,  ...,  0.0135, -0.0352,  0.0198],\n",
       "          [ 0.0109, -0.0005,  0.0287,  ...,  0.0007, -0.0010,  0.0004],\n",
       "          [ 0.0013,  0.0067,  0.0334,  ...,  0.0557, -0.0334,  0.0114]],\n",
       "         device='cuda:0', dtype=torch.bfloat16)],\n",
       " [tensor([[ 0.0117,  0.0000,  0.0293,  ...,  0.0000, -0.0020,  0.0000],\n",
       "          [ 0.0019,  0.0072,  0.0339,  ...,  0.0559, -0.0337,  0.0112],\n",
       "          [ 0.0035,  0.0025,  0.0310,  ...,  0.0140, -0.0369,  0.0195],\n",
       "          [ 0.0025,  0.0018,  0.0315,  ...,  0.0135, -0.0364,  0.0193],\n",
       "          [ 0.0117,  0.0000,  0.0293,  ...,  0.0000, -0.0020,  0.0000],\n",
       "          [ 0.0019,  0.0072,  0.0339,  ...,  0.0559, -0.0337,  0.0112]],\n",
       "         device='cuda:0', dtype=torch.bfloat16)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.attn_outputs, model.model.layers[1].self_attn.attn_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:00<00:00, 2452.92it/s, est. speed input: 4920.23 toks/s, output: 2459.56 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "# output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "output = test_llm_generator.generate(78*[\"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.allclose(attn_output_layer0, attn_output_layer1, atol=1e-2, rtol=1e-2)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(torch.cat(model.model.layers[0].self_attn.attn_outputs), \n",
    "               torch.cat(model.model.layers[1].self_attn.attn_outputs), atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 156, 4096])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model.model.layers[1].self_attn.attn_outputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0109, -0.0005,  0.0287,  ...,  0.0007, -0.0010,  0.0004],\n",
       "         [ 0.0013,  0.0067,  0.0337,  ...,  0.0557, -0.0334,  0.0114],\n",
       "         [ 0.0109, -0.0005,  0.0287,  ...,  0.0007, -0.0010,  0.0004],\n",
       "         ...,\n",
       "         [ 0.0013,  0.0067,  0.0337,  ...,  0.0557, -0.0334,  0.0114],\n",
       "         [ 0.0109, -0.0005,  0.0287,  ...,  0.0007, -0.0010,  0.0004],\n",
       "         [ 0.0013,  0.0067,  0.0337,  ...,  0.0557, -0.0334,  0.0114]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model.model.layers[0].self_attn.attn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0117,  0.0000,  0.0293,  ...,  0.0000, -0.0020,  0.0000],\n",
       "         [ 0.0020,  0.0071,  0.0339,  ...,  0.0559, -0.0337,  0.0112],\n",
       "         [ 0.0117,  0.0000,  0.0293,  ...,  0.0000, -0.0020,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0020,  0.0071,  0.0339,  ...,  0.0559, -0.0337,  0.0112],\n",
       "         [ 0.0117,  0.0000,  0.0293,  ...,  0.0000, -0.0020,  0.0000],\n",
       "         [ 0.0020,  0.0071,  0.0339,  ...,  0.0559, -0.0337,  0.0112]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model.model.layers[1].self_attn.attn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:00<00:00, 1738.74it/s, est. speed input: 3484.94 toks/s, output: 5226.57 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "# output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "output = test_llm_generator.generate(78*[\"hello\"], \n",
    "                                     SamplingParams(temperature=0.0, ignore_eos=True, max_tokens=3))\n",
    "\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.allclose(attn_output_layer0, attn_output_layer1, atol=1e-2, rtol=1e-2)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_attn_outputs_and_metadatas(model)\n",
    "model.model.attn_metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 157.38it/s, est. speed input: 475.34 toks/s, output: 475.05 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# 3 tokens decode.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "for attn_output_layer0, attn_output_layer1 in zip(model.model.layers[0].self_attn.attn_outputs, \n",
    "                                                  model.model.layers[1].self_attn.attn_outputs):\n",
    "    assert torch.allclose(attn_output_layer0, attn_output_layer1, atol=1e-2, rtol=1e-2)\n",
    "\n",
    "# 3 attention metadata for a 3 model forward pass (1 prefill + 2 decode)\n",
    "assert len(model.model.attn_metadatas) == 3 \n",
    "# Prefill phase.\n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "assert attn_metadata.decode_metadata is None\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 1.\n",
    "attn_metadata = model.model.attn_metadatas[1]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 2.\n",
    "attn_metadata = model.model.attn_metadatas[2]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 3 (cuda graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 15:33:06 llm_engine.py:237] Initializing an LLM engine (v0.1.dev2968+g2fa3c83) with config: model='/home/k/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/k/models/dummy-llama, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n",
      "ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 15:33:08 model_runner.py:1049] Starting to load model /home/k/models/dummy-llama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 15:33:09 model_runner.py:1060] Loading model weights took 2.7696 GB\n",
      "INFO 10-10 15:33:09 gpu_executor.py:122] # GPU blocks: 8049, # CPU blocks: 65536\n",
      "INFO 10-10 15:33:09 model_runner.py:1383] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-10 15:33:09 model_runner.py:1387] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-10 15:33:39 model_runner.py:1511] Graph capturing finished in 30 secs.\n"
     ]
    }
   ],
   "source": [
    "### test 1: No prefix caching, bf16 quantization, use v1 block manager, cuda graph mode ###\n",
    "ENABLE_PREFIX_CACHING = False\n",
    "KV_CACHE_DTYPE = \"auto\"\n",
    "USE_V2_BLOCK_MANAGER = True\n",
    "ENFORCE_EAGER = False\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/home/k/models/dummy-llama\", \n",
    "                         tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "                         enforce_eager=ENFORCE_EAGER, \n",
    "                         kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "                         enable_prefix_caching=ENABLE_PREFIX_CACHING, \n",
    "                         use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "                         tensor_parallel_size=1, \n",
    "                         dtype=\"bfloat16\", \n",
    "                         max_model_len=1024*1, \n",
    "                         kv_cache_map={0:0, 1:0}, \n",
    "                         gpu_memory_utilization=0.2,\n",
    "                         debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "    # Reset attn outputs and attn metadatas from warmup.\n",
    "    if len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[0].self_attn.attn_outputs = []\n",
    "    if len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[1].self_attn.attn_outputs = []\n",
    "    if len(model.model.attn_metadatas) > 0:\n",
    "        model.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 227.94it/s, est. speed input: 690.65 toks/s, output: 230.00 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 102.69it/s, est. speed input: 310.07 toks/s, output: 309.86 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# 3 tokens decode.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "for attn_output_layer0, attn_output_layer1 in zip(model.model.layers[0].self_attn.attn_outputs, \n",
    "                                                  model.model.layers[1].self_attn.attn_outputs):\n",
    "    assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "\n",
    "\n",
    "\n",
    "# XFormersCLAMetadata\n",
    "#     NOTE: Any python object stored here is not updated when it is\n",
    "#     cuda-graph replayed. If you have values that need to be changed\n",
    "#     dynamically, it should be stored in tensor. The tensor has to be\n",
    "#     updated from `CUDAGraphRunner.forward` API.\n",
    "    \n",
    "    \n",
    "# # 3 attention metadata for a 3 model forward pass (1 prefill + 2 decode)\n",
    "# assert len(model.model.attn_metadatas) == 3 \n",
    "# # Prefill phase.\n",
    "# attn_metadata = model.model.attn_metadatas[0]\n",
    "# assert attn_metadata.decode_metadata is None\n",
    "# expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "# assert attn_metadata.prefill_metadata.shared_self_attention_types == expected\n",
    "# # Decode phase 1.\n",
    "# attn_metadata = model.model.attn_metadatas[1]\n",
    "# assert attn_metadata.prefill_metadata is None\n",
    "# expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "# assert attn_metadata.decode_metadata.shared_self_attention_types == expected\n",
    "# # Decode phase 2.\n",
    "# attn_metadata = model.model.attn_metadatas[2]\n",
    "# assert attn_metadata.prefill_metadata is None\n",
    "# expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "# assert attn_metadata.decode_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 4 (block manager v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-10 15:33:53 config.py:365] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 10-10 15:33:53 llm_engine.py:237] Initializing an LLM engine (v0.1.dev2968+g2fa3c83) with config: model='/home/k/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/k/models/dummy-llama, use_v2_block_manager=False, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n",
      "ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 15:33:55 model_runner.py:1049] Starting to load model /home/k/models/dummy-llama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.31it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 15:33:56 model_runner.py:1060] Loading model weights took 2.7696 GB\n",
      "INFO 10-10 15:33:56 gpu_executor.py:122] # GPU blocks: 8049, # CPU blocks: 65536\n"
     ]
    }
   ],
   "source": [
    "ENABLE_PREFIX_CACHING = False\n",
    "KV_CACHE_DTYPE = \"auto\"\n",
    "USE_V2_BLOCK_MANAGER = False\n",
    "ENFORCE_EAGER = True\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/home/k/models/dummy-llama\", \n",
    "                         tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "                         enforce_eager=ENFORCE_EAGER, \n",
    "                         kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "                         enable_prefix_caching=ENABLE_PREFIX_CACHING, \n",
    "                         use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "                         tensor_parallel_size=1, \n",
    "                         dtype=\"bfloat16\", \n",
    "                         max_model_len=1024*1,\n",
    "                         kv_cache_map={0:0, 1:0}, \n",
    "                         gpu_memory_utilization=0.2,\n",
    "                         debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "    # Reset attn outputs and attn metadatas from warmup.\n",
    "    if len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[0].self_attn.attn_outputs = []\n",
    "    if len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[1].self_attn.attn_outputs = []\n",
    "    if len(model.model.attn_metadatas) > 0:\n",
    "        model.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 206.04it/s, est. speed input: 637.58 toks/s, output: 212.05 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 110.57it/s, est. speed input: 333.76 toks/s, output: 333.53 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# 3 tokens decode.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "for attn_output_layer0, attn_output_layer1 in zip(model.model.layers[0].self_attn.attn_outputs, \n",
    "                                                  model.model.layers[1].self_attn.attn_outputs):\n",
    "    assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "\n",
    "# 3 attention metadata for a 3 model forward pass (1 prefill + 2 decode)\n",
    "assert len(model.model.attn_metadatas) == 3 \n",
    "# Prefill phase.\n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "assert attn_metadata.decode_metadata is None\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 1.\n",
    "attn_metadata = model.model.attn_metadatas[1]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 2.\n",
    "attn_metadata = model.model.attn_metadatas[2]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 5 (prefix caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.core.block.interfaces import Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enforce xformers attention backend - changes are made here\n",
    "os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"XFORMERS_CLA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-11 12:09:14 llm_engine.py:237] Initializing an LLM engine (v0.1.dev2968+g2fa3c83) with config: model='/home/k/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/k/models/dummy-llama, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n",
      "ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-11 12:09:15 selector.py:122] Using XFormers CLA backend.\n",
      "INFO 10-11 12:09:16 model_runner.py:1049] Starting to load model /home/k/models/dummy-llama...\n",
      "INFO 10-11 12:09:16 selector.py:122] Using XFormers CLA backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.33it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-11 12:09:17 model_runner.py:1060] Loading model weights took 2.8008 GB\n",
      "INFO 10-11 12:09:17 gpu_executor.py:122] # GPU blocks: 5937, # CPU blocks: 65536\n",
      "INFO 10-11 12:09:20 model_runner.py:1383] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-11 12:09:20 model_runner.py:1387] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-11 12:09:35 model_runner.py:1511] Graph capturing finished in 15 secs.\n"
     ]
    }
   ],
   "source": [
    "### test 1: No prefix caching, bf16 quantization, use v1 block manager, cuda graph mode ###\n",
    "ENABLE_PREFIX_CACHING = True\n",
    "KV_CACHE_DTYPE = \"auto\"\n",
    "USE_V2_BLOCK_MANAGER = True\n",
    "ENFORCE_EAGER = False\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/home/k/models/dummy-llama\", \n",
    "                         tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "                         enforce_eager=ENFORCE_EAGER, \n",
    "                         kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "                         enable_prefix_caching=ENABLE_PREFIX_CACHING, \n",
    "                         use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "                         tensor_parallel_size=1,\n",
    "                         dtype=\"bfloat16\",\n",
    "                         max_model_len=1024,\n",
    "                         kv_cache_map={0:0,1:0},\n",
    "                         gpu_memory_utilization=0.2,\n",
    "                         debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler = test_llm_generator.llm_engine.scheduler[0]\n",
    "gpu_allocator = scheduler.block_manager.block_allocator._allocators[Device.GPU]\n",
    "cpu_allocator = scheduler.block_manager.block_allocator._allocators[Device.CPU]\n",
    "gpu_allocator._cached_blocks, cpu_allocator._cached_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler.block_manager.gpu_allocator.cached_blocks, scheduler.block_manager.cpu_allocator.cached_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.54it/s, est. speed input: 106.86 toks/s, output: 106.85 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill with prefix cached.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], \n",
    "                                     SamplingParams(temperature=0.0, max_tokens=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PREFILL_KV_NEW', 'PREFILL_KV_SHARED']\n"
     ]
    }
   ],
   "source": [
    "for meta in model.model.attn_metadatas:\n",
    "    if meta.prefill_metadata: print(meta.prefill_metadata.shared_self_attention_types)\n",
    "    if meta.decode_metadata: print(meta.decode_metadata.shared_self_attention_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model.model.attn_metadatas) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.block_manager.get_prefix_cache_hit_rate(Device.GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\"vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\",\n",
    "            \"Briefly describe the major milestones in the development of artificial intelligence from 1950 to 2020.\",\n",
    "            \"Compare and contrast artificial intelligence with human intelligence in terms of processing information.\",\n",
    "            \"Describe the basic components of a neural network and how it can be trained.\",\n",
    "            \"Write a short story about a robot that dreams for the first time.\",\n",
    "            \"Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\",\n",
    "            \"Explain the cultural significance of the Mona Lisa painting, and how its perception might vary in Western versus Eastern societies.\",\n",
    "            \"Translate the following English sentence into Japanese, French, and Swahili: 'The early bird catches the worm.'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 40.33it/s, est. speed input: 850.30 toks/s, output: 121.40 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Prefill and decode.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([examples[0]], \n",
    "                                     SamplingParams(temperature=0.0, max_tokens=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'gcfds copp\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PREFILL_KV_NEW', 'PREFILL_KV_SHARED']\n"
     ]
    }
   ],
   "source": [
    "for meta in model.model.attn_metadatas:\n",
    "    if meta.prefill_metadata: print(meta.prefill_metadata.shared_self_attention_types)\n",
    "    if meta.decode_metadata: print(meta.decode_metadata.shared_self_attention_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(torch.cat(model.model.layers[0].self_attn.attn_outputs),\n",
    "                   torch.cat(model.model.layers[1].self_attn.attn_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 19.91it/s, est. speed input: 390.89 toks/s, output: 59.75 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Prefill and decode.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate(examples, \n",
    "                                     SamplingParams(temperature=0.0, max_tokens=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'gcfds copp\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mattn_metadatas) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert len(model.model.attn_metadatas) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PREFILL_PREFIX_CACHED_KV', 'PREFILL_PREFIX_CACHED_KV_SHARED']\n"
     ]
    }
   ],
   "source": [
    "for meta in model.model.attn_metadatas:\n",
    "    if meta.prefill_metadata: print(meta.prefill_metadata.shared_self_attention_types)\n",
    "    if meta.decode_metadata: print(meta.decode_metadata.shared_self_attention_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(torch.cat(model.model.layers[0].self_attn.attn_outputs),\n",
    "                   torch.cat(model.model.layers[1].self_attn.attn_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0136,  0.0037,  0.0108,  ...,  0.0039,  0.0063,  0.0105],\n",
       "        [ 0.0128,  0.0038,  0.0075,  ...,  0.0041,  0.0063,  0.0104],\n",
       "        [ 0.0119,  0.0038,  0.0067,  ...,  0.0029,  0.0049,  0.0079],\n",
       "        ...,\n",
       "        [ 0.0042,  0.0115, -0.0127,  ...,  0.0089,  0.0091, -0.0011],\n",
       "        [ 0.0041,  0.0115, -0.0128,  ...,  0.0086,  0.0092, -0.0009],\n",
       "        [ 0.0039,  0.0112, -0.0125,  ...,  0.0082,  0.0089, -0.0015]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.attn_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0136,  0.0037,  0.0108,  ...,  0.0039,  0.0063,  0.0105],\n",
       "        [ 0.0128,  0.0038,  0.0075,  ...,  0.0041,  0.0063,  0.0104],\n",
       "        [ 0.0119,  0.0038,  0.0067,  ...,  0.0029,  0.0049,  0.0079],\n",
       "        ...,\n",
       "        [ 0.0042,  0.0115, -0.0127,  ...,  0.0089,  0.0091, -0.0011],\n",
       "        [ 0.0041,  0.0115, -0.0128,  ...,  0.0086,  0.0092, -0.0009],\n",
       "        [ 0.0039,  0.0112, -0.0125,  ...,  0.0082,  0.0089, -0.0015]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[1].self_attn.attn_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 213.26it/s, est. speed input: 650.58 toks/s, output: 216.44 toks/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m attn_output_layer0 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mattn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m attn_output_layer1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mattn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(attn_output_layer0, attn_output_layer1)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 1 attention metadata for a single model forward pass\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mattn_metadatas) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert scheduler.block_manager.get_prefix_cache_hit_rate(Device.GPU) == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 59.95it/s, est. speed input: 241.74 toks/s, output: 181.08 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill with prefix cached.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate(\"hello world!\", SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 57.29it/s, est. speed input: 115.55 toks/s, output: 173.07 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill with prefix cached.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate(\"hey\", SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BlockSpaceManagerV2' object has no attribute 'gpu_allocator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_allocator\u001b[49m\u001b[38;5;241m.\u001b[39mcached_blocks, scheduler\u001b[38;5;241m.\u001b[39mblock_manager\u001b[38;5;241m.\u001b[39mcpu_allocator\u001b[38;5;241m.\u001b[39mcached_blocks\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BlockSpaceManagerV2' object has no attribute 'gpu_allocator'"
     ]
    }
   ],
   "source": [
    "scheduler.block_manager.gpu_allocator.cached_blocks, scheduler.block_manager.cpu_allocator.cached_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BlockSpaceManagerV2' object has no attribute 'gpu_allocator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_allocator\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BlockSpaceManagerV2' object has no attribute 'gpu_allocator'"
     ]
    }
   ],
   "source": [
    "scheduler.block_manager.gpu_allocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.block_manager.gpu_allocator.cache_metric_data.block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CacheMetricData(num_completed_blocks=0, completed_block_cache_hit_rate=0.0, num_incompleted_block_queries=17, num_incompleted_block_hit=14, block_size=1000)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.block_manager.gpu_allocator.cache_metric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.block_manager.gpu_allocator.cache_metric_data.num_completed_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8235294117647058"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.block_manager.get_prefix_cache_hit_rate(Device.GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.block_manager.gpu_allocator.cache_metric_data.num_incompleted_block_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.block_manager.gpu_allocator.cache_metric_data.num_incompleted_block_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
