{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/miniconda3/envs/llm_quant/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-09 16:43:03,115\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.attention.backends.abstract import AttentionType, SharedSelfAttentionType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enforce xformers attention backend - changes are made here\n",
    "os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"XFORMERS_CLA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Inside LlamaAttention.forward\n",
    "# Change q to fixed input for testing.\n",
    "if self.cache_config.debug_kv_sharing: q = torch.ones_like(q)\n",
    "attn_output = self.attn(q, k, v, kv_cache, attn_metadata, compute_new_kv=compute_new_kv_map)\n",
    "if self.cache_config.debug_kv_sharing: self.attn_outputs.append(attn_output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-09 16:43:17 config.py:365] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 10-09 16:43:17 llm_engine.py:237] Initializing an LLM engine (v0.1.dev2968+g2fa3c83) with config: model='/home/k/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/k/models/dummy-llama, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n",
      "ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:43:18 selector.py:122] Using XFormers CLA backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/miniconda3/envs/llm_quant/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/k/miniconda3/envs/llm_quant/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:43:19 model_runner.py:1049] Starting to load model /home/k/models/dummy-llama...\n",
      "INFO 10-09 16:43:19 selector.py:122] Using XFormers CLA backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:43:20 model_runner.py:1060] Loading model weights took 2.8008 GB\n",
      "INFO 10-09 16:43:20 gpu_executor.py:122] # GPU blocks: 5937, # CPU blocks: 65536\n"
     ]
    }
   ],
   "source": [
    "### test 1: No prefix caching, no quantization, use v1 block manager, no cuda graphs ###\n",
    "ENABLE_PREFIX_CACHING = False\n",
    "KV_CACHE_DTYPE = \"auto\"\n",
    "USE_V2_BLOCK_MANAGER = True\n",
    "ENFORCE_EAGER = True\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/home/k/models/dummy-llama\", \n",
    "                         tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "                         enforce_eager=ENFORCE_EAGER, \n",
    "                         kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "                         enable_prefix_caching=ENABLE_PREFIX_CACHING, \n",
    "                         use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "                         tensor_parallel_size=1, \n",
    "                         dtype=\"bfloat16\", \n",
    "                         max_model_len=1024*1, \n",
    "                         kv_cache_map={0:0, 1:0}, \n",
    "                         gpu_memory_utilization=0.2,\n",
    "                         debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "    # Reset attn outputs and attn metadatas from warmup.\n",
    "    if len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[0].self_attn.attn_outputs = []\n",
    "    if len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[1].self_attn.attn_outputs = []\n",
    "    if len(model.model.attn_metadatas) > 0:\n",
    "        model.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 37.02it/s, est. speed input: 111.35 toks/s, output: 37.11 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20.58it/s, est. speed input: 61.84 toks/s, output: 61.83 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# 3 tokens decode.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "for attn_output_layer0, attn_output_layer1 in zip(model.model.layers[0].self_attn.attn_outputs, \n",
    "                                                  model.model.layers[1].self_attn.attn_outputs):\n",
    "    assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "\n",
    "# 3 attention metadata for a 3 model forward pass (1 prefill + 2 decode)\n",
    "assert len(model.model.attn_metadatas) == 3 \n",
    "# Prefill phase.\n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "assert attn_metadata.decode_metadata is None\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 1.\n",
    "attn_metadata = model.model.attn_metadatas[1]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 2.\n",
    "attn_metadata = model.model.attn_metadatas[2]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 2 (fp8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:43:32 config.py:629] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\n",
      "WARNING 10-09 16:43:32 config.py:365] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 10-09 16:43:32 llm_engine.py:237] Initializing an LLM engine (v0.1.dev2968+g2fa3c83) with config: model='/home/k/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=fp8, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/k/models/dummy-llama, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n",
      "ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:43:33 selector.py:122] Using XFormers CLA backend.\n",
      "INFO 10-09 16:43:34 model_runner.py:1049] Starting to load model /home/k/models/dummy-llama...\n",
      "INFO 10-09 16:43:34 selector.py:122] Using XFormers CLA backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.36it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.35it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:43:34 model_runner.py:1060] Loading model weights took 2.7696 GB\n",
      "INFO 10-09 16:43:35 gpu_executor.py:122] # GPU blocks: 16610, # CPU blocks: 131072\n"
     ]
    }
   ],
   "source": [
    "### test 1: No prefix caching, fp8 quantization, use v1 block manager, no cuda graphs ###\n",
    "ENABLE_PREFIX_CACHING = False\n",
    "KV_CACHE_DTYPE = \"fp8\"\n",
    "USE_V2_BLOCK_MANAGER = True\n",
    "ENFORCE_EAGER = True\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/home/k/models/dummy-llama\", \n",
    "                         tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "                         enforce_eager=ENFORCE_EAGER, \n",
    "                         kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "                         enable_prefix_caching=ENABLE_PREFIX_CACHING, \n",
    "                         use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "                         tensor_parallel_size=1, \n",
    "                         dtype=\"bfloat16\",\n",
    "                         max_model_len=1024*1, \n",
    "                         kv_cache_map={0:0, 1:0}, \n",
    "                         gpu_memory_utilization=0.2, \n",
    "                         debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "    # Reset attn outputs and attn metadatas from warmup.\n",
    "    if len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[0].self_attn.attn_outputs = []\n",
    "    if len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[1].self_attn.attn_outputs = []\n",
    "    if len(model.model.attn_metadatas) > 0:\n",
    "        model.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 159.14it/s, est. speed input: 482.14 toks/s, output: 160.61 toks/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m attn_output_layer0 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mattn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m attn_output_layer1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mattn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(attn_output_layer0, attn_output_layer1)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 1 attention metadata for a single model forward pass\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mattn_metadatas) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0109, -0.0005,  0.0287,  ...,  0.0007, -0.0010,  0.0004],\n",
       "         [ 0.0013,  0.0067,  0.0334,  ...,  0.0557, -0.0334,  0.0114],\n",
       "         [ 0.0029,  0.0021,  0.0306,  ...,  0.0140, -0.0356,  0.0200],\n",
       "         [ 0.0019,  0.0014,  0.0311,  ...,  0.0135, -0.0352,  0.0198],\n",
       "         [ 0.0109, -0.0005,  0.0287,  ...,  0.0007, -0.0010,  0.0004],\n",
       "         [ 0.0013,  0.0067,  0.0334,  ...,  0.0557, -0.0334,  0.0114]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([[ 0.0137, -0.0078,  0.0293,  ...,  0.0078, -0.0088,  0.0078],\n",
       "         [ 0.0031,  0.0021,  0.0339,  ...,  0.0564, -0.0339,  0.0134],\n",
       "         [ 0.0047, -0.0020,  0.0312,  ...,  0.0142, -0.0369,  0.0204],\n",
       "         [ 0.0035, -0.0027,  0.0317,  ...,  0.0135, -0.0366,  0.0203],\n",
       "         [ 0.0137, -0.0078,  0.0293,  ...,  0.0078, -0.0088,  0.0078],\n",
       "         [ 0.0031,  0.0021,  0.0339,  ...,  0.0564, -0.0339,  0.0134]],\n",
       "        device='cuda:0', dtype=torch.bfloat16))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to fix this mismatch\n",
    "attn_output_layer0, attn_output_layer1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 3 (cuda graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:44:55 llm_engine.py:237] Initializing an LLM engine (v0.1.dev2968+g2fa3c83) with config: model='/home/k/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/k/models/dummy-llama, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n",
      "ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:44:57 model_runner.py:1049] Starting to load model /home/k/models/dummy-llama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.22it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:44:58 model_runner.py:1060] Loading model weights took 2.7696 GB\n",
      "INFO 10-09 16:44:58 gpu_executor.py:122] # GPU blocks: 8049, # CPU blocks: 65536\n",
      "INFO 10-09 16:44:58 model_runner.py:1383] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-09 16:44:58 model_runner.py:1387] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-09 16:45:31 model_runner.py:1511] Graph capturing finished in 33 secs.\n"
     ]
    }
   ],
   "source": [
    "### test 1: No prefix caching, bf16 quantization, use v1 block manager, cuda graph mode ###\n",
    "ENABLE_PREFIX_CACHING = False\n",
    "KV_CACHE_DTYPE = \"auto\"\n",
    "USE_V2_BLOCK_MANAGER = True\n",
    "ENFORCE_EAGER = False\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/home/k/models/dummy-llama\", \n",
    "                         tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "                         enforce_eager=ENFORCE_EAGER, \n",
    "                         kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "                         enable_prefix_caching=ENABLE_PREFIX_CACHING, \n",
    "                         use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "                         tensor_parallel_size=1, \n",
    "                         dtype=\"bfloat16\", \n",
    "                         max_model_len=1024*1, \n",
    "                         kv_cache_map={0:0, 1:0}, \n",
    "                         gpu_memory_utilization=0.2,\n",
    "                         debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "    # Reset attn outputs and attn metadatas from warmup.\n",
    "    if len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[0].self_attn.attn_outputs = []\n",
    "    if len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[1].self_attn.attn_outputs = []\n",
    "    if len(model.model.attn_metadatas) > 0:\n",
    "        model.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 163.54it/s, est. speed input: 495.73 toks/s, output: 165.06 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 79.58it/s, est. speed input: 239.93 toks/s, output: 239.80 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# 3 tokens decode.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "for attn_output_layer0, attn_output_layer1 in zip(model.model.layers[0].self_attn.attn_outputs, \n",
    "                                                  model.model.layers[1].self_attn.attn_outputs):\n",
    "    assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "\n",
    "\n",
    "    \n",
    "#     NOTE: Any python object stored here is not updated when it is\n",
    "#     cuda-graph replayed. If you have values that need to be changed\n",
    "#     dynamically, it should be stored in tensor. The tensor has to be\n",
    "#     updated from `CUDAGraphRunner.forward` API.\n",
    "    \n",
    "    \n",
    "# # 3 attention metadata for a 3 model forward pass (1 prefill + 2 decode)\n",
    "# assert len(model.model.attn_metadatas) == 3 \n",
    "# # Prefill phase.\n",
    "# attn_metadata = model.model.attn_metadatas[0]\n",
    "# assert attn_metadata.decode_metadata is None\n",
    "# expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "# assert attn_metadata.prefill_metadata.shared_self_attention_types == expected\n",
    "# # Decode phase 1.\n",
    "# attn_metadata = model.model.attn_metadatas[1]\n",
    "# assert attn_metadata.prefill_metadata is None\n",
    "# expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "# assert attn_metadata.decode_metadata.shared_self_attention_types == expected\n",
    "# # Decode phase 2.\n",
    "# attn_metadata = model.model.attn_metadatas[2]\n",
    "# assert attn_metadata.prefill_metadata is None\n",
    "# expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "# assert attn_metadata.decode_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 4 (block manager v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-09 16:47:27 config.py:365] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 10-09 16:47:27 llm_engine.py:237] Initializing an LLM engine (v0.1.dev2968+g2fa3c83) with config: model='/home/k/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/k/models/dummy-llama, use_v2_block_manager=False, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n",
      "ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:47:29 model_runner.py:1049] Starting to load model /home/k/models/dummy-llama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:47:29 model_runner.py:1060] Loading model weights took 2.7696 GB\n",
      "INFO 10-09 16:47:30 gpu_executor.py:122] # GPU blocks: 8049, # CPU blocks: 65536\n"
     ]
    }
   ],
   "source": [
    "ENABLE_PREFIX_CACHING = False\n",
    "KV_CACHE_DTYPE = \"auto\"\n",
    "USE_V2_BLOCK_MANAGER = False\n",
    "ENFORCE_EAGER = True\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/home/k/models/dummy-llama\", \n",
    "                         tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "                         enforce_eager=ENFORCE_EAGER, \n",
    "                         kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "                         enable_prefix_caching=ENABLE_PREFIX_CACHING, \n",
    "                         use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "                         tensor_parallel_size=1, \n",
    "                         dtype=\"bfloat16\", \n",
    "                         max_model_len=1024*1,\n",
    "                         kv_cache_map={0:0, 1:0}, \n",
    "                         gpu_memory_utilization=0.2,\n",
    "                         debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "    # Reset attn outputs and attn metadatas from warmup.\n",
    "    if len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[0].self_attn.attn_outputs = []\n",
    "    if len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[1].self_attn.attn_outputs = []\n",
    "    if len(model.model.attn_metadatas) > 0:\n",
    "        model.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 121.16it/s, est. speed input: 366.24 toks/s, output: 121.98 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 113.05it/s, est. speed input: 342.08 toks/s, output: 341.75 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# 3 tokens decode.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "for attn_output_layer0, attn_output_layer1 in zip(model.model.layers[0].self_attn.attn_outputs, \n",
    "                                                  model.model.layers[1].self_attn.attn_outputs):\n",
    "    assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "\n",
    "# 3 attention metadata for a 3 model forward pass (1 prefill + 2 decode)\n",
    "assert len(model.model.attn_metadatas) == 3 \n",
    "# Prefill phase.\n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "assert attn_metadata.decode_metadata is None\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 1.\n",
    "attn_metadata = model.model.attn_metadatas[1]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 2.\n",
    "attn_metadata = model.model.attn_metadatas[2]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 5 (prefix caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-09 16:48:41 config.py:365] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 10-09 16:48:41 llm_engine.py:237] Initializing an LLM engine (v0.1.dev2968+g2fa3c83) with config: model='/home/k/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/k/models/dummy-llama, use_v2_block_manager=False, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n",
      "ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:48:42 model_runner.py:1049] Starting to load model /home/k/models/dummy-llama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.31it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.31it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:48:43 model_runner.py:1060] Loading model weights took 2.7696 GB\n",
      "INFO 10-09 16:48:43 gpu_executor.py:122] # GPU blocks: 8049, # CPU blocks: 65536\n",
      "INFO 10-09 16:48:43 block_manager_v1.py:263] Automatic prefix caching is enabled.\n"
     ]
    }
   ],
   "source": [
    "### test 1: No prefix caching, bf16 quantization, use v1 block manager, cuda graph mode ###\n",
    "ENABLE_PREFIX_CACHING = True\n",
    "KV_CACHE_DTYPE = \"auto\"\n",
    "USE_V2_BLOCK_MANAGER = False\n",
    "ENFORCE_EAGER = True\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/home/k/models/dummy-llama\", \n",
    "                         tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "                         enforce_eager=ENFORCE_EAGER, \n",
    "                         kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "                         enable_prefix_caching=ENABLE_PREFIX_CACHING, \n",
    "                         use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "                         tensor_parallel_size=1,\n",
    "                         dtype=\"bfloat16\",\n",
    "                         max_model_len=1024*1,\n",
    "                         kv_cache_map={0:0, 1:0},\n",
    "                         gpu_memory_utilization=0.2,\n",
    "                         debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "    # Reset attn outputs and attn metadatas from warmup.\n",
    "    if len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[0].self_attn.attn_outputs = []\n",
    "    if len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[1].self_attn.attn_outputs = []\n",
    "    if len(model.model.attn_metadatas) > 0:\n",
    "        model.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 125.56it/s, est. speed input: 380.40 toks/s, output: 126.66 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 114.58it/s, est. speed input: 346.28 toks/s, output: 115.34 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill with prefix cached.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.0109, -0.0005,  0.0287,  ...,  0.0007, -0.0010,  0.0004],\n",
       "         [ 0.0013,  0.0067,  0.0334,  ...,  0.0557, -0.0334,  0.0114],\n",
       "         [ 0.0029,  0.0021,  0.0306,  ...,  0.0140, -0.0356,  0.0200],\n",
       "         [ 0.0019,  0.0014,  0.0311,  ...,  0.0135, -0.0352,  0.0198],\n",
       "         [ 0.0109, -0.0005,  0.0287,  ...,  0.0007, -0.0010,  0.0004],\n",
       "         [ 0.0013,  0.0067,  0.0334,  ...,  0.0557, -0.0334,  0.0114]],\n",
       "        device='cuda:0', dtype=torch.bfloat16)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.attn_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.0109, -0.0005,  0.0287,  ...,  0.0007, -0.0010,  0.0004],\n",
       "         [ 0.0013,  0.0067,  0.0334,  ...,  0.0557, -0.0334,  0.0114],\n",
       "         [ 0.0029,  0.0021,  0.0306,  ...,  0.0140, -0.0356,  0.0200],\n",
       "         [ 0.0019,  0.0014,  0.0311,  ...,  0.0135, -0.0352,  0.0198],\n",
       "         [ 0.0109, -0.0005,  0.0287,  ...,  0.0007, -0.0010,  0.0004],\n",
       "         [ 0.0013,  0.0067,  0.0334,  ...,  0.0557, -0.0334,  0.0114]],\n",
       "        device='cuda:0', dtype=torch.bfloat16)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[1].self_attn.attn_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PREFILL_KV_NEW', 'PREFILL_KV_SHARED']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_metadata.prefill_metadata.shared_self_attention_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], device='cuda:0', size=(2, 0), dtype=torch.int32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_metadata.prefill_metadata.block_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
