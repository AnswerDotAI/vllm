{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-08 19:58:36,896\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.attention.backends.abstract import AttentionType, SharedSelfAttentionType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enforce xformers attention backend - changes are made here\n",
    "os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"XFORMERS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Inside LlamaAttention.forward\n",
    "# Change q to fixed input for testing.\n",
    "if self.cache_config.debug_kv_sharing: q = torch.ones_like(q)\n",
    "attn_output = self.attn(q, k, v, kv_cache, attn_metadata, compute_new_kv=compute_new_kv_map)\n",
    "if self.cache_config.debug_kv_sharing: self.attn_outputs.append(attn_output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-08 19:58:43 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='/workspace/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/workspace/models/dummy-llama, use_v2_block_manager=True, enable_prefix_caching=False)\n",
      "INFO 10-08 19:58:43 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-08 19:58:44 model_runner.py:879] Starting to load model /workspace/models/dummy-llama...\n",
      "INFO 10-08 19:58:44 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.26it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-08 19:58:45 model_runner.py:890] Loading model weights took 2.8008 GB\n",
      "INFO 10-08 19:58:46 gpu_executor.py:121] # GPU blocks: 5937, # CPU blocks: 65536\n"
     ]
    }
   ],
   "source": [
    "### test 1: No prefix caching, no quantization, use v1 block manager, no cuda graphs ###\n",
    "ENABLE_PREFIX_CACHING = False\n",
    "KV_CACHE_DTYPE = \"auto\"\n",
    "USE_V2_BLOCK_MANAGER = True\n",
    "ENFORCE_EAGER = True\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/workspace/models/dummy-llama\", tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "\t\t\t\t\t\t enforce_eager=ENFORCE_EAGER, kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "       \t\t\t\t\t enable_prefix_caching=ENABLE_PREFIX_CACHING, use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "\t\t\t\t\t\t tensor_parallel_size=1, dtype=\"bfloat16\", max_model_len=1024*1, kv_cache_map={0:0, 1:0}, gpu_memory_utilization=0.2,\n",
    "\t\t\t\t\t\t debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "\t# Reset attn outputs and attn metadatas from warmup.\n",
    "\tif len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "\t\tmodel.model.layers[0].self_attn.attn_outputs = []\n",
    "\tif len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "\t\tmodel.model.layers[1].self_attn.attn_outputs = []\n",
    "\tif len(model.model.attn_metadatas) > 0:\n",
    "\t\tmodel.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  7.73it/s, est. speed input: 23.20 toks/s, output: 7.73 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 17.83it/s, est. speed input: 53.53 toks/s, output: 53.52 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# 3 tokens decode.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "for attn_output_layer0, attn_output_layer1 in zip(model.model.layers[0].self_attn.attn_outputs, \n",
    "                                                  model.model.layers[1].self_attn.attn_outputs):\n",
    "    assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "\n",
    "# 3 attention metadata for a 3 model forward pass (1 prefill + 2 decode)\n",
    "assert len(model.model.attn_metadatas) == 3 \n",
    "# Prefill phase.\n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "assert attn_metadata.decode_metadata is None\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 1.\n",
    "attn_metadata = model.model.attn_metadatas[1]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 2.\n",
    "attn_metadata = model.model.attn_metadatas[2]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 2 (fp8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-08 19:59:59 config.py:576] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\n",
      "INFO 10-08 19:59:59 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='/workspace/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=fp8, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/workspace/models/dummy-llama, use_v2_block_manager=True, enable_prefix_caching=False)\n",
      "INFO 10-08 19:59:59 selector.py:116] Using XFormers backend.\n",
      "INFO 10-08 19:59:59 model_runner.py:879] Starting to load model /workspace/models/dummy-llama...\n",
      "INFO 10-08 19:59:59 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-08 20:00:00 model_runner.py:890] Loading model weights took 2.7696 GB\n",
      "INFO 10-08 20:00:01 gpu_executor.py:121] # GPU blocks: 16610, # CPU blocks: 131072\n"
     ]
    }
   ],
   "source": [
    "### test 1: No prefix caching, fp8 quantization, use v1 block manager, no cuda graphs ###\n",
    "ENABLE_PREFIX_CACHING = False\n",
    "KV_CACHE_DTYPE = \"fp8\"\n",
    "USE_V2_BLOCK_MANAGER = True\n",
    "ENFORCE_EAGER = True\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/workspace/models/dummy-llama\", tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "\t\t\t\t\t\t enforce_eager=ENFORCE_EAGER, kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "       \t\t\t\t\t enable_prefix_caching=ENABLE_PREFIX_CACHING, use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "\t\t\t\t\t\t tensor_parallel_size=1, dtype=\"bfloat16\", max_model_len=1024*1, kv_cache_map={0:0, 1:0}, gpu_memory_utilization=0.2,\n",
    "\t\t\t\t\t\t debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "\t# Reset attn outputs and attn metadatas from warmup.\n",
    "\tif len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "\t\tmodel.model.layers[0].self_attn.attn_outputs = []\n",
    "\tif len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "\t\tmodel.model.layers[1].self_attn.attn_outputs = []\n",
    "\tif len(model.model.attn_metadatas) > 0:\n",
    "\t\tmodel.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 70.06it/s, est. speed input: 211.72 toks/s, output: 70.53 toks/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m attn_output_layer0 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mattn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m attn_output_layer1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mattn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(attn_output_layer0, attn_output_layer1)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 1 attention metadata for a single model forward pass\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mattn_metadatas) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0109, -0.0005,  0.0287,  ...,  0.0007, -0.0010,  0.0004],\n",
       "         [ 0.0013,  0.0067,  0.0334,  ...,  0.0557, -0.0334,  0.0114],\n",
       "         [ 0.0029,  0.0021,  0.0306,  ...,  0.0140, -0.0356,  0.0200],\n",
       "         [ 0.0019,  0.0014,  0.0311,  ...,  0.0135, -0.0352,  0.0198],\n",
       "         [ 0.0109, -0.0005,  0.0287,  ...,  0.0007, -0.0010,  0.0004],\n",
       "         [ 0.0013,  0.0067,  0.0334,  ...,  0.0557, -0.0334,  0.0114]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([[ 0.0137, -0.0078,  0.0293,  ...,  0.0078, -0.0088,  0.0078],\n",
       "         [ 0.0031,  0.0021,  0.0339,  ...,  0.0564, -0.0339,  0.0134],\n",
       "         [ 0.0047, -0.0020,  0.0312,  ...,  0.0142, -0.0369,  0.0204],\n",
       "         [ 0.0035, -0.0027,  0.0317,  ...,  0.0135, -0.0366,  0.0203],\n",
       "         [ 0.0137, -0.0078,  0.0293,  ...,  0.0078, -0.0088,  0.0078],\n",
       "         [ 0.0031,  0.0021,  0.0339,  ...,  0.0564, -0.0339,  0.0134]],\n",
       "        device='cuda:0', dtype=torch.bfloat16))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_layer0, attn_output_layer1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 3 (cuda graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-08 20:01:59 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='/workspace/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/workspace/models/dummy-llama, use_v2_block_manager=True, enable_prefix_caching=False)\n",
      "INFO 10-08 20:01:59 model_runner.py:879] Starting to load model /workspace/models/dummy-llama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.26it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.26it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-08 20:02:01 model_runner.py:890] Loading model weights took 2.7696 GB\n",
      "INFO 10-08 20:02:02 gpu_executor.py:121] # GPU blocks: 53681, # CPU blocks: 65536\n",
      "INFO 10-08 20:02:02 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-08 20:02:02 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-08 20:02:34 model_runner.py:1300] Graph capturing finished in 32 secs.\n"
     ]
    }
   ],
   "source": [
    "### test 1: No prefix caching, bf16 quantization, use v1 block manager, cuda graph mode ###\n",
    "ENABLE_PREFIX_CACHING = False\n",
    "KV_CACHE_DTYPE = \"auto\"\n",
    "USE_V2_BLOCK_MANAGER = True\n",
    "ENFORCE_EAGER = False\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/workspace/models/dummy-llama\", tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "\t\t\t\t\t\t enforce_eager=ENFORCE_EAGER, kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "       \t\t\t\t\t enable_prefix_caching=ENABLE_PREFIX_CACHING, use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "\t\t\t\t\t\t tensor_parallel_size=1, dtype=\"bfloat16\", max_model_len=1024*1, kv_cache_map={0:0, 1:0}, gpu_memory_utilization=0.2,\n",
    "\t\t\t\t\t\t debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "\t# Reset attn outputs and attn metadatas from warmup.\n",
    "\tif len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "\t\tmodel.model.layers[0].self_attn.attn_outputs = []\n",
    "\tif len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "\t\tmodel.model.layers[1].self_attn.attn_outputs = []\n",
    "\tif len(model.model.attn_metadatas) > 0:\n",
    "\t\tmodel.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 182.40it/s, est. speed input: 555.64 toks/s, output: 184.84 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 103.80it/s, est. speed input: 314.16 toks/s, output: 313.80 toks/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(attn_output_layer0, attn_output_layer1)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 3 attention metadata for a 3 model forward pass (1 prefill + 2 decode)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mattn_metadatas) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Prefill phase.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m attn_metadata \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mattn_metadatas[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3 tokens decode.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "for attn_output_layer0, attn_output_layer1 in zip(model.model.layers[0].self_attn.attn_outputs, \n",
    "                                                  model.model.layers[1].self_attn.attn_outputs):\n",
    "    assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "\n",
    "# 3 attention metadata for a 3 model forward pass (1 prefill + 2 decode)\n",
    "assert len(model.model.attn_metadatas) == 3 \n",
    "# Prefill phase.\n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "assert attn_metadata.decode_metadata is None\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 1.\n",
    "attn_metadata = model.model.attn_metadatas[1]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 2.\n",
    "attn_metadata = model.model.attn_metadatas[2]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.model.attn_metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 4 (block manager v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-08 20:07:31 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='/workspace/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/workspace/models/dummy-llama, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 10-08 20:07:31 model_runner.py:879] Starting to load model /workspace/models/dummy-llama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-08 20:07:33 model_runner.py:890] Loading model weights took 2.7696 GB\n",
      "INFO 10-08 20:07:33 gpu_executor.py:121] # GPU blocks: 53425, # CPU blocks: 65536\n"
     ]
    }
   ],
   "source": [
    "### test 1: No prefix caching, bf16 quantization, use v1 block manager, cuda graph mode ###\n",
    "ENABLE_PREFIX_CACHING = False\n",
    "KV_CACHE_DTYPE = \"auto\"\n",
    "USE_V2_BLOCK_MANAGER = False\n",
    "ENFORCE_EAGER = True\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/workspace/models/dummy-llama\", tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "\t\t\t\t\t\t enforce_eager=ENFORCE_EAGER, kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "       \t\t\t\t\t enable_prefix_caching=ENABLE_PREFIX_CACHING, use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "\t\t\t\t\t\t tensor_parallel_size=1, dtype=\"bfloat16\", max_model_len=1024*1, kv_cache_map={0:0, 1:0}, gpu_memory_utilization=0.2,\n",
    "\t\t\t\t\t\t debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "\t# Reset attn outputs and attn metadatas from warmup.\n",
    "\tif len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "\t\tmodel.model.layers[0].self_attn.attn_outputs = []\n",
    "\tif len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "\t\tmodel.model.layers[1].self_attn.attn_outputs = []\n",
    "\tif len(model.model.attn_metadatas) > 0:\n",
    "\t\tmodel.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  5.40it/s, est. speed input: 16.22 toks/s, output: 5.41 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 94.08it/s, est. speed input: 284.54 toks/s, output: 284.25 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# 3 tokens decode.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "for attn_output_layer0, attn_output_layer1 in zip(model.model.layers[0].self_attn.attn_outputs, \n",
    "                                                  model.model.layers[1].self_attn.attn_outputs):\n",
    "    assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "\n",
    "# 3 attention metadata for a 3 model forward pass (1 prefill + 2 decode)\n",
    "assert len(model.model.attn_metadatas) == 3 \n",
    "# Prefill phase.\n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "assert attn_metadata.decode_metadata is None\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 1.\n",
    "attn_metadata = model.model.attn_metadatas[1]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 2.\n",
    "attn_metadata = model.model.attn_metadatas[2]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 5 (prefix caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-08 20:09:13 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='/workspace/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/workspace/models/dummy-llama, use_v2_block_manager=False, enable_prefix_caching=True)\n",
      "INFO 10-08 20:09:13 model_runner.py:879] Starting to load model /workspace/models/dummy-llama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.18it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.18it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-08 20:09:14 model_runner.py:890] Loading model weights took 2.7696 GB\n",
      "INFO 10-08 20:09:15 gpu_executor.py:121] # GPU blocks: 53425, # CPU blocks: 65536\n",
      "INFO 10-08 20:09:15 block_manager_v1.py:263] Automatic prefix caching is enabled.\n"
     ]
    }
   ],
   "source": [
    "### test 1: No prefix caching, bf16 quantization, use v1 block manager, cuda graph mode ###\n",
    "ENABLE_PREFIX_CACHING = True\n",
    "KV_CACHE_DTYPE = \"auto\"\n",
    "USE_V2_BLOCK_MANAGER = False\n",
    "ENFORCE_EAGER = True\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/workspace/models/dummy-llama\", tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "\t\t\t\t\t\t enforce_eager=ENFORCE_EAGER, kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "       \t\t\t\t\t enable_prefix_caching=ENABLE_PREFIX_CACHING, use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "\t\t\t\t\t\t tensor_parallel_size=1, dtype=\"bfloat16\", max_model_len=1024*1, kv_cache_map={0:0, 1:0}, gpu_memory_utilization=0.2,\n",
    "\t\t\t\t\t\t debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 95.46it/s, est. speed input: 288.79 toks/s, output: 96.16 toks/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m output \u001b[38;5;241m=\u001b[39m test_llm_generator\u001b[38;5;241m.\u001b[39mgenerate([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello world!\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m], SamplingParams(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, ignore_eos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m attn_output_layer0 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m attn_output_layer1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mattn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(attn_output_layer0, attn_output_layer1)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
