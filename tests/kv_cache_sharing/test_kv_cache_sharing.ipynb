{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/miniconda3/envs/llm_quant/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-10 15:14:49,423\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.attention.backends.abstract import AttentionType, SharedSelfAttentionType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enforce xformers attention backend - changes are made here\n",
    "os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"XFORMERS_CLA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Inside LlamaAttention.forward\n",
    "# Change q to fixed input for testing.\n",
    "if self.cache_config.debug_kv_sharing: q = torch.ones_like(q)\n",
    "attn_output = self.attn(q, k, v, kv_cache, attn_metadata, compute_new_kv=compute_new_kv_map)\n",
    "if self.cache_config.debug_kv_sharing: self.attn_outputs.append(attn_output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test 1: No prefix caching, no quantization, use v1 block manager, no cuda graphs ###\n",
    "ENABLE_PREFIX_CACHING = False\n",
    "KV_CACHE_DTYPE = \"auto\"\n",
    "USE_V2_BLOCK_MANAGER = True\n",
    "ENFORCE_EAGER = True\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/home/k/models/dummy-llama\", \n",
    "                         tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "                         enforce_eager=ENFORCE_EAGER, \n",
    "                         kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "                         enable_prefix_caching=ENABLE_PREFIX_CACHING, \n",
    "                         use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "                         tensor_parallel_size=1, \n",
    "                         dtype=\"bfloat16\", \n",
    "                         max_model_len=1024*1, \n",
    "                         kv_cache_map={0:0, 1:0}, \n",
    "                         gpu_memory_utilization=0.2,\n",
    "                         debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "    # Reset attn outputs and attn metadatas from warmup.\n",
    "    if len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[0].self_attn.attn_outputs = []\n",
    "    if len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[1].self_attn.attn_outputs = []\n",
    "    if len(model.model.attn_metadatas) > 0:\n",
    "        model.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████| 2/2 [00:00<00:00, 262.64it/s, est. speed input: 799.52 toks/s, output: 266.04 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████| 2/2 [00:00<00:00, 124.73it/s, est. speed input: 376.85 toks/s, output: 376.54 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# 3 tokens decode.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "for attn_output_layer0, attn_output_layer1 in zip(model.model.layers[0].self_attn.attn_outputs, \n",
    "                                                  model.model.layers[1].self_attn.attn_outputs):\n",
    "    assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "\n",
    "# 3 attention metadata for a 3 model forward pass (1 prefill + 2 decode)\n",
    "assert len(model.model.attn_metadatas) == 3 \n",
    "# Prefill phase.\n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "assert attn_metadata.decode_metadata is None\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 1.\n",
    "attn_metadata = model.model.attn_metadatas[1]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 2.\n",
    "attn_metadata = model.model.attn_metadatas[2]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 2 (fp8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if kv_cache_dtype in (\"fp8\", \"fp8_e4m3\"):\n",
    "    target_dtype = torch.float8_e4m3fn\n",
    "elif kv_cache_dtype == \"fp8_e5m2\":\n",
    "    target_dtype = torch.float8_e5m2\n",
    "```\n",
    "\n",
    "Quantiztion stack: \n",
    "- PagedAttention.write_to_paged_cache\n",
    "- _custom_ops.reshape_and_cache\n",
    "- torch.ops._C_cache_ops.reshape_and_cache\n",
    "- cache_kernels.cu/reshape_and_cache\n",
    "- CALL_RESHAPE_AND_CACHE (macro)\n",
    "- reshape_and_cache_kernel\n",
    "- fp8::scaled_convert\n",
    "- scaled_vec_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 15:15:14 config.py:629] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\n",
      "WARNING 10-10 15:15:14 config.py:365] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 10-10 15:15:14 llm_engine.py:237] Initializing an LLM engine (v0.1.dev2968+g2fa3c83) with config: model='/home/k/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=fp8, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/k/models/dummy-llama, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n",
      "ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 15:15:15 selector.py:122] Using XFormers CLA backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/miniconda3/envs/llm_quant/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/k/miniconda3/envs/llm_quant/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 15:15:16 model_runner.py:1049] Starting to load model /home/k/models/dummy-llama...\n",
      "INFO 10-10 15:15:16 selector.py:122] Using XFormers CLA backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.31it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.31it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-10 15:15:17 model_runner.py:1060] Loading model weights took 2.8008 GB\n",
      "INFO 10-10 15:15:18 gpu_executor.py:122] # GPU blocks: 11874, # CPU blocks: 131072\n"
     ]
    }
   ],
   "source": [
    "### test 1: No prefix caching, fp8 quantization, use v1 block manager, no cuda graphs ###\n",
    "ENABLE_PREFIX_CACHING = False\n",
    "KV_CACHE_DTYPE = \"fp8\"\n",
    "USE_V2_BLOCK_MANAGER = True\n",
    "ENFORCE_EAGER = True\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/home/k/models/dummy-llama\", \n",
    "                         tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "                         enforce_eager=ENFORCE_EAGER, \n",
    "                         kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "                         enable_prefix_caching=ENABLE_PREFIX_CACHING, \n",
    "                         use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "                         tensor_parallel_size=1, \n",
    "                         dtype=\"bfloat16\",\n",
    "                         max_model_len=1024*1, \n",
    "                         kv_cache_map={0:0, 1:0}, \n",
    "                         gpu_memory_utilization=0.2, \n",
    "                         debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "    # Reset attn outputs and attn metadatas from warmup.\n",
    "    if len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[0].self_attn.attn_outputs = []\n",
    "    if len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[1].self_attn.attn_outputs = []\n",
    "    if len(model.model.attn_metadatas) > 0:\n",
    "        model.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 128.44it/s, est. speed input: 388.09 toks/s, output: 129.25 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.allclose(attn_output_layer0, attn_output_layer1, atol=1e-2, rtol=1e-2)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 120.05it/s, est. speed input: 362.65 toks/s, output: 362.37 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# 3 tokens decode.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "for attn_output_layer0, attn_output_layer1 in zip(model.model.layers[0].self_attn.attn_outputs, \n",
    "                                                  model.model.layers[1].self_attn.attn_outputs):\n",
    "    assert torch.allclose(attn_output_layer0, attn_output_layer1, atol=1e-2, rtol=1e-2)\n",
    "\n",
    "# 3 attention metadata for a 3 model forward pass (1 prefill + 2 decode)\n",
    "assert len(model.model.attn_metadatas) == 3 \n",
    "# Prefill phase.\n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "assert attn_metadata.decode_metadata is None\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 1.\n",
    "attn_metadata = model.model.attn_metadatas[1]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 2.\n",
    "attn_metadata = model.model.attn_metadatas[2]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 3 (cuda graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:44:55 llm_engine.py:237] Initializing an LLM engine (v0.1.dev2968+g2fa3c83) with config: model='/home/k/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/k/models/dummy-llama, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n",
      "ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:44:57 model_runner.py:1049] Starting to load model /home/k/models/dummy-llama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.22it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:44:58 model_runner.py:1060] Loading model weights took 2.7696 GB\n",
      "INFO 10-09 16:44:58 gpu_executor.py:122] # GPU blocks: 8049, # CPU blocks: 65536\n",
      "INFO 10-09 16:44:58 model_runner.py:1383] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-09 16:44:58 model_runner.py:1387] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-09 16:45:31 model_runner.py:1511] Graph capturing finished in 33 secs.\n"
     ]
    }
   ],
   "source": [
    "### test 1: No prefix caching, bf16 quantization, use v1 block manager, cuda graph mode ###\n",
    "ENABLE_PREFIX_CACHING = False\n",
    "KV_CACHE_DTYPE = \"auto\"\n",
    "USE_V2_BLOCK_MANAGER = True\n",
    "ENFORCE_EAGER = False\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/home/k/models/dummy-llama\", \n",
    "                         tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "                         enforce_eager=ENFORCE_EAGER, \n",
    "                         kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "                         enable_prefix_caching=ENABLE_PREFIX_CACHING, \n",
    "                         use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "                         tensor_parallel_size=1, \n",
    "                         dtype=\"bfloat16\", \n",
    "                         max_model_len=1024*1, \n",
    "                         kv_cache_map={0:0, 1:0}, \n",
    "                         gpu_memory_utilization=0.2,\n",
    "                         debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "    # Reset attn outputs and attn metadatas from warmup.\n",
    "    if len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[0].self_attn.attn_outputs = []\n",
    "    if len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[1].self_attn.attn_outputs = []\n",
    "    if len(model.model.attn_metadatas) > 0:\n",
    "        model.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 163.54it/s, est. speed input: 495.73 toks/s, output: 165.06 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 79.58it/s, est. speed input: 239.93 toks/s, output: 239.80 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# 3 tokens decode.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "for attn_output_layer0, attn_output_layer1 in zip(model.model.layers[0].self_attn.attn_outputs, \n",
    "                                                  model.model.layers[1].self_attn.attn_outputs):\n",
    "    assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "\n",
    "\n",
    "    \n",
    "#     NOTE: Any python object stored here is not updated when it is\n",
    "#     cuda-graph replayed. If you have values that need to be changed\n",
    "#     dynamically, it should be stored in tensor. The tensor has to be\n",
    "#     updated from `CUDAGraphRunner.forward` API.\n",
    "    \n",
    "    \n",
    "# # 3 attention metadata for a 3 model forward pass (1 prefill + 2 decode)\n",
    "# assert len(model.model.attn_metadatas) == 3 \n",
    "# # Prefill phase.\n",
    "# attn_metadata = model.model.attn_metadatas[0]\n",
    "# assert attn_metadata.decode_metadata is None\n",
    "# expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "# assert attn_metadata.prefill_metadata.shared_self_attention_types == expected\n",
    "# # Decode phase 1.\n",
    "# attn_metadata = model.model.attn_metadatas[1]\n",
    "# assert attn_metadata.prefill_metadata is None\n",
    "# expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "# assert attn_metadata.decode_metadata.shared_self_attention_types == expected\n",
    "# # Decode phase 2.\n",
    "# attn_metadata = model.model.attn_metadatas[2]\n",
    "# assert attn_metadata.prefill_metadata is None\n",
    "# expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "# assert attn_metadata.decode_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 4 (block manager v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-09 16:47:27 config.py:365] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 10-09 16:47:27 llm_engine.py:237] Initializing an LLM engine (v0.1.dev2968+g2fa3c83) with config: model='/home/k/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/k/models/dummy-llama, use_v2_block_manager=False, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n",
      "ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:47:29 model_runner.py:1049] Starting to load model /home/k/models/dummy-llama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 16:47:29 model_runner.py:1060] Loading model weights took 2.7696 GB\n",
      "INFO 10-09 16:47:30 gpu_executor.py:122] # GPU blocks: 8049, # CPU blocks: 65536\n"
     ]
    }
   ],
   "source": [
    "ENABLE_PREFIX_CACHING = False\n",
    "KV_CACHE_DTYPE = \"auto\"\n",
    "USE_V2_BLOCK_MANAGER = False\n",
    "ENFORCE_EAGER = True\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/home/k/models/dummy-llama\", \n",
    "                         tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "                         enforce_eager=ENFORCE_EAGER, \n",
    "                         kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "                         enable_prefix_caching=ENABLE_PREFIX_CACHING, \n",
    "                         use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "                         tensor_parallel_size=1, \n",
    "                         dtype=\"bfloat16\", \n",
    "                         max_model_len=1024*1,\n",
    "                         kv_cache_map={0:0, 1:0}, \n",
    "                         gpu_memory_utilization=0.2,\n",
    "                         debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "    # Reset attn outputs and attn metadatas from warmup.\n",
    "    if len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[0].self_attn.attn_outputs = []\n",
    "    if len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[1].self_attn.attn_outputs = []\n",
    "    if len(model.model.attn_metadatas) > 0:\n",
    "        model.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 121.16it/s, est. speed input: 366.24 toks/s, output: 121.98 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 113.05it/s, est. speed input: 342.08 toks/s, output: 341.75 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# 3 tokens decode.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "for attn_output_layer0, attn_output_layer1 in zip(model.model.layers[0].self_attn.attn_outputs, \n",
    "                                                  model.model.layers[1].self_attn.attn_outputs):\n",
    "    assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "\n",
    "# 3 attention metadata for a 3 model forward pass (1 prefill + 2 decode)\n",
    "assert len(model.model.attn_metadatas) == 3 \n",
    "# Prefill phase.\n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "assert attn_metadata.decode_metadata is None\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 1.\n",
    "attn_metadata = model.model.attn_metadatas[1]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected\n",
    "# Decode phase 2.\n",
    "attn_metadata = model.model.attn_metadatas[2]\n",
    "assert attn_metadata.prefill_metadata is None\n",
    "expected = [SharedSelfAttentionType.DECODE_KV_NEW.name, SharedSelfAttentionType.DECODE_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.decode_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 5 (prefix caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.core.block.interfaces import Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-09 18:24:18 config.py:365] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 10-09 18:24:18 llm_engine.py:237] Initializing an LLM engine (v0.1.dev2968+g2fa3c83) with config: model='/home/k/models/dummy-llama', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/k/models/dummy-llama, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n",
      "ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/k/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 18:24:19 model_runner.py:1049] Starting to load model /home/k/models/dummy-llama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-09 18:24:20 model_runner.py:1060] Loading model weights took 2.7696 GB\n",
      "INFO 10-09 18:24:21 gpu_executor.py:122] # GPU blocks: 8305, # CPU blocks: 65536\n"
     ]
    }
   ],
   "source": [
    "### test 1: No prefix caching, bf16 quantization, use v1 block manager, cuda graph mode ###\n",
    "ENABLE_PREFIX_CACHING = True\n",
    "KV_CACHE_DTYPE = \"auto\"\n",
    "USE_V2_BLOCK_MANAGER = True\n",
    "ENFORCE_EAGER = True\n",
    "DEBUG_KV_SHARING = True\n",
    "\n",
    "\n",
    "# This the base llm generator with KV cache sharing, where layer 1 gets its KV cache from layer 0.\n",
    "test_llm_generator = LLM(model=\"/home/k/models/dummy-llama\", \n",
    "                         tokenizer=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "                         enforce_eager=ENFORCE_EAGER, \n",
    "                         kv_cache_dtype=KV_CACHE_DTYPE,\n",
    "                         enable_prefix_caching=ENABLE_PREFIX_CACHING, \n",
    "                         use_v2_block_manager=USE_V2_BLOCK_MANAGER,\n",
    "                         tensor_parallel_size=1,\n",
    "                         dtype=\"bfloat16\",\n",
    "                         max_model_len=1024*1,\n",
    "                         kv_cache_map={0:0, 1:0},\n",
    "                         gpu_memory_utilization=0.2,\n",
    "                         debug_kv_sharing=DEBUG_KV_SHARING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_llm_generator.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "worker = test_llm_generator.llm_engine.model_executor.driver_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler = test_llm_generator.llm_engine.scheduler[0]\n",
    "gpu_allocator = scheduler.block_manager.block_allocator._allocators[Device.GPU]\n",
    "cpu_allocator = scheduler.block_manager.block_allocator._allocators[Device.CPU]\n",
    "gpu_allocator._cached_blocks, cpu_allocator._cached_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging is on.\n",
    "assert model.model.cache_config.debug_kv_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_attn_outputs_and_metadatas(model):\n",
    "    # Reset attn outputs and attn metadatas from warmup.\n",
    "    if len(model.model.layers[0].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[0].self_attn.attn_outputs = []\n",
    "    if len(model.model.layers[1].self_attn.attn_outputs) > 0:\n",
    "        model.model.layers[1].self_attn.attn_outputs = []\n",
    "    if len(model.model.attn_metadatas) > 0:\n",
    "        model.model.attn_metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 123.85it/s, est. speed input: 374.75 toks/s, output: 124.81 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=1, ignore_eos=True))\n",
    "\n",
    "# Attention outputs of layer 0 and layer 1 should be equal, Q is fixed to 1s during debug mode.\n",
    "attn_output_layer0 = model.model.layers[0].self_attn.attn_outputs[0]\n",
    "attn_output_layer1 = model.model.layers[1].self_attn.attn_outputs[0]\n",
    "assert torch.equal(attn_output_layer0, attn_output_layer1)\n",
    "# 1 attention metadata for a single model forward pass\n",
    "assert len(model.model.attn_metadatas) == 1 \n",
    "attn_metadata = model.model.attn_metadatas[0]\n",
    "# No decode request with max_tokens=1.\n",
    "assert attn_metadata.decode_metadata is None\n",
    "# Layer 0 creates new KV and layer 1 reuses KV from cache during prefill.\n",
    "expected = [SharedSelfAttentionType.PREFILL_KV_NEW.name, SharedSelfAttentionType.PREFILL_KV_SHARED.name] # [layer 0, layer 1]\n",
    "assert attn_metadata.prefill_metadata.shared_self_attention_types == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler = test_llm_generator.llm_engine.scheduler[0]\n",
    "gpu_allocator = scheduler.block_manager.block_allocator._allocators[Device.GPU]\n",
    "cpu_allocator = scheduler.block_manager.block_allocator._allocators[Device.CPU]\n",
    "gpu_allocator._cached_blocks, cpu_allocator._cached_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 86.09it/s, est. speed input: 259.70 toks/s, output: 259.55 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Just prefill with prefix cached.\n",
    "reset_attn_outputs_and_metadatas(model)\n",
    "output = test_llm_generator.generate([\"hello world!\", \"hello\"], SamplingParams(temperature=0.0, max_tokens=3, ignore_eos=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler = test_llm_generator.llm_engine.scheduler[0]\n",
    "gpu_allocator = scheduler.block_manager.block_allocator._allocators[Device.GPU]\n",
    "cpu_allocator = scheduler.block_manager.block_allocator._allocators[Device.CPU]\n",
    "gpu_allocator._cached_blocks, cpu_allocator._cached_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<vllm.core.block.cpu_gpu_block_allocator.CpuGpuBlockAllocator>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.block_manager.block_allocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_allocator._block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33220"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpu_allocator._block_pool._pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 15339, 1917, 0, 220, 12451]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_allocator._block_pool._pool[0].token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 15339, 12451, 37364]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_allocator._block_pool._pool[1].token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_allocator._block_pool._pool[2].token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.block_manager.block_allocator.get_prefix_cache_hit_rate(Device.GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
