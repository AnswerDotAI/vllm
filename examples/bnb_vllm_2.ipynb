{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dcc0f0a-991d-4d3d-800a-353c41a4c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "import safetensors\n",
    "import safetensors.torch\n",
    "from pathlib import Path\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.functional import dequantize_4bit, QuantState\n",
    "from bitsandbytes.nn.modules import Params4bit, Linear4bit\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from accelerate import init_empty_weights\n",
    "from glob import glob\n",
    "import os\n",
    "from fastcore.parallel import parallel\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd20cb7-c709-4f1f-9319-0145ae7736e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.39.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3fbcf3e-d815-45ae-b6a3-1cb87083c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.sequence import SequenceGroupMetadata, SequenceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf36a8b-a0b5-4cd9-8ca0-6dfe6527053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e97d65d-9fcf-41a7-85ac-67c10851f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear(model:nn.Module, linear_replacement:nn.Module, quant_config:dict|None=None,\n",
    "                   skip_modules:List[str]=[\"lm_head\"], **kwargs):\n",
    "    \"\"\"\n",
    "    Replace linear modules with a new Linear module.\n",
    "    Parameters:\n",
    "        model (`torch.nn.Module`):\n",
    "            Input model or `torch.nn.Module` as the function is run recursively.\n",
    "        linear_replacement (`torch.nn.Module`):\n",
    "            The linear module that replaces the old one. Only expects standard arguments.\n",
    "            If other arguments need to be passed, use a lambda.\n",
    "        skip_modules (`List[str]`, *optional*, defaults to `lm_head`):\n",
    "            List of modules names not to convert. Defaults to `lm_head`.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if name in skip_modules:\n",
    "            print(f\"Skipping {name}\")\n",
    "            continue\n",
    "        \n",
    "        if len(list(module.children())) > 0:\n",
    "            replace_linear(module, linear_replacement, quant_config, skip_modules, **kwargs)\n",
    "\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            if issubclass(linear_replacement, Linear4bit):\n",
    "                model._modules[name] = linear_replacement(\n",
    "                    module.in_features,\n",
    "                    module.out_features,\n",
    "                    module.bias is not None,\n",
    "                    **kwargs\n",
    "                )\n",
    "            # elif issubclass(linear_replacement, HQQLinear):\n",
    "            #     model._modules[name] = linear_replacement(module, quant_config, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported linear replacement: {type(linear_replacement)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d876f664-14db-4788-859b-f6d15d810a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_quantize(module:nn.Module, name:str, value:torch.Tensor, device:torch.device=None, dtype:torch.dtype=None,\n",
    "                      skip_names:list[str]=[], is_meta_rank:bool=False, low_memory:bool=True, verbose:bool=False,\n",
    "                      quant_method:str='bnb', is_dora:bool=False):\n",
    "    \"\"\"\n",
    "    Loads `value` tensor into submodule of `module`, optionally skipping `skip_names` and converting to `dtype`.\n",
    "\n",
    "    Quantizes `Params4bit` on `device` then places on \"cpu\" if low_memory=True or \"meta\" if is_meta_rank=True.\n",
    "    \"\"\"\n",
    "    def place_on_device(value):\n",
    "        if is_meta_rank:\n",
    "            device = 'meta'\n",
    "        elif low_memory:\n",
    "            device = 'cpu'\n",
    "        return value.to(device=device, dtype=dtype)\n",
    "\n",
    "    if any([skip_name in name for skip_name in skip_names]):\n",
    "        if verbose:\n",
    "            print(f\"Skipping {name} because it is in skip_names\")\n",
    "        return\n",
    "\n",
    "    module_key, _, value_key = name.rpartition('.')\n",
    "    try:\n",
    "        submodule = module.get_submodule(module_key)\n",
    "    except AttributeError as e:\n",
    "        print(f\"Module {module_key} not found:\\n{e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        if quant_method=='bnb':\n",
    "            param = submodule.get_parameter(value_key)\n",
    "            if isinstance(param, Params4bit):\n",
    "                # With `sync_module_states=True`, a meta device Params4bit needs to be the same\n",
    "                # shape as the quantized Params4bit with an initialized quant_state. However,\n",
    "                # FSDP only syncs parameters and buffers, so the quant_state isn't copied. This\n",
    "                # workaround quantizes Params4bit to initialize quant_state on all ranks, then\n",
    "                # replaces Params4bit's data with a meta tensor to free memory on non-rank 0.\n",
    "                if is_dora:\n",
    "                    setattr(submodule, \"dora_scale\", value.norm(p=2, dim=1).to(dtype=dtype).to(\"cpu\"))                \n",
    "                    print(\"DORA scale initialized\")\n",
    "                value = type(param)(value.to(device=device, dtype=dtype).data, **param.__dict__).cuda(device)\n",
    "                if is_meta_rank:\n",
    "                    value = type(param)(value.data.to(\"meta\"), **value.__dict__)\n",
    "                elif low_memory:\n",
    "                    value = type(param)(value.data.to(\"cpu\"), **value.__dict__)\n",
    "                # print(\"Loaded quantized layer\")\n",
    "            else:\n",
    "                value = type(param)(place_on_device(value).data)\n",
    "                # print(\"Loaded regular layer\")\n",
    "    except AttributeError:\n",
    "        # it's a buffer\n",
    "        value = place_on_device(value)\n",
    "        pass\n",
    "    setattr(submodule, value_key, value)\n",
    "\n",
    "def load_and_quantize_parallel(name_param, model, **kwargs):\n",
    "    name, param = name_param\n",
    "    load_and_quantize(model, name, param, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cefeae3-254e-4fb2-9f48-6b82c5ef9571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636d87e-5114-4abd-a936-c09e8e4e1eda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Load HF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9a6799e-7115-49ed-b364-b17f4eed7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_math_model_dir = \"/home/ubuntu/models/llama-7b-orca-math-100k-full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "603c6abb-49d0-4927-a008-a0e3932c08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "cfg = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "cfg._attn_implementation = \"flash_attention_2\"\n",
    "cfg._attn_implementation_internal = \"flash_attention_2\"\n",
    "skip_modules = [\"lm_head\"]\n",
    "load_param_skip_names = ['inv_freq']\n",
    "compute_dtype = torch_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f84eb212-ee74-4532-8916-9c25e30fe5bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "192246e1-5b3f-4a26-bdc5-8ea9ebeb21ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import LlamaFlashAttention2\n",
    "# https://github.com/Dao-AILab/flash-attention/issues/742\n",
    "# !pip install transformers==4.33.1\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(cfg)\n",
    "    model.model = replace_linear(model.model, Linear4bit, compute_dtype=compute_dtype,\n",
    "                                 quant_type='nf4', compress_statistics=False,\n",
    "                                 quant_storage=torch.uint8, skip_modules=skip_modules)\n",
    "# For some reason slower.\n",
    "#     for layer in model.model.layers: \n",
    "#         m = getattr(layer, 'self_attn')\n",
    "#         setattr(layer, 'self_attn', LlamaFlashAttention2(m.config, m.layer_idx))\n",
    "# model.config._attn_implementation = \"flash_attention_2\"\n",
    "# model.config._attn_implementation_internal = \"flash_attention_2\"\n",
    "model.is_loaded_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03dbbd6d-64f9-4cd9-b90d-62366a01e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = safetensors.torch.load_file(glob(os.path.join(orca_math_model_dir, \"*.safetensors\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9ac7792-d5e2-4a1b-aa4d-9e0cfb557219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#291) [None,None,None,None,None,None,None,None,None,None...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel(load_and_quantize_parallel, \n",
    "         iter(weights.items()), \n",
    "         n_workers=8, \n",
    "         threadpool=True,\n",
    "         model=model, \n",
    "         dtype=torch_dtype, \n",
    "         device=torch.cuda.current_device(),\n",
    "         skip_names=load_param_skip_names,\n",
    "         is_meta_rank=False,\n",
    "         verbose=True,\n",
    "         quant_method=\"bnb\",\n",
    "         is_dora=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1fc6660-9e13-4fb7-aea8-f69791b82d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4655164b-b2c7-4053-a34b-b13a504cdc25",
   "metadata": {},
   "source": [
    "#### Load VLLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26a93991-c222-4c66-b8fe-d05cfb214173",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-09 11:54:03 config.py:744] Casting torch.float16 to torch.bfloat16.\n",
      "WARNING 04-09 11:54:03 config.py:208] bnb quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 11:54:05,526\tWARNING utils.py:580 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.\n",
      "2024-04-09 11:54:05,530\tWARNING utils.py:592 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 32.3 to 32.\n",
      "2024-04-09 11:54:06,744\tINFO worker.py:1752 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 11:54:07 llm_engine.py:70] Initializing an LLM engine (v0.3.3) with config: model='/workspace/models/llama-7b-orca-math-100k-full-quantized', tokenizer='meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=True, quantization=bnb, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 04-09 11:54:15 pynccl_utils.py:13] vLLM is using nccl==2.18.1\n",
      "INFO 04-09 11:54:15 selector.py:44] flash_attn is not found.\n",
      "INFO 04-09 11:54:15 selector.py:20] Using XFormers backend.\n",
      "\u001b[36m(RayWorkerVllm pid=214327)\u001b[0m INFO 04-09 11:54:16 pynccl_utils.py:13] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerVllm pid=214327)\u001b[0m INFO 04-09 11:54:17 selector.py:44] flash_attn is not found.\n",
      "\u001b[36m(RayWorkerVllm pid=214327)\u001b[0m INFO 04-09 11:54:17 selector.py:20] Using XFormers backend.\n",
      "INFO 04-09 11:54:20 model_runner.py:104] Loading model weights took 0.9786 GB\n",
      "\u001b[36m(RayWorkerVllm pid=214327)\u001b[0m INFO 04-09 11:54:20 model_runner.py:104] Loading model weights took 0.9786 GB\n",
      "INFO 04-09 11:54:23 ray_gpu_executor.py:242] # GPU blocks: 10041, # CPU blocks: 2048\n",
      "INFO 04-09 11:54:24 block_manager_v1.py:239] disable automatic prefix caching\n"
     ]
    }
   ],
   "source": [
    "orca_math_model_dir = \"/workspace/models/llama-7b-orca-math-100k-full-quantized\"\n",
    "# orca_math_model_dir = \"/workspace/models/llama-7b-orca-math-100k-bnb-qdora-vllm\"\n",
    "llm = LLM(model=orca_math_model_dir, tokenizer=\"meta-llama/Llama-2-7b-hf\", dtype=\"bfloat16\", \n",
    "          tensor_parallel_size=4, enforce_eager=True, quantization=\"bnb\", gpu_memory_utilization=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc614d14-a91b-41f7-a499-6e9aee05d297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# llm = LLM(model=\"robertgshaw2/llama-2-7b-chat-marlin\", tensor_parallel_size=2, \n",
    "#           enforce_eager=False, quantization=\"marlin\", gpu_memory_utilization=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca89adb-a60e-4428-8659-cf1a32c901ed",
   "metadata": {},
   "source": [
    "#### Benchmark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48a240b7-f3cd-440d-8b4b-590cb619d85d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e0a5b63-5471-454b-bd19-c5cedcccff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed(fn, *args, **kwargs):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    result = fn(*args, **kwargs)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return result, start.elapsed_time(end) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1493b478-de96-4923-b514-c31b01ccc8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"microsoft/orca-math-word-problems-200k\")['train'].shuffle(seed=42)\n",
    "# train with 10k for starters. Then 100k.\n",
    "# dataset = dataset.select(range(0,100000))\n",
    "\n",
    "# select last 5k as validation\n",
    "dataset = dataset.select(range(len(dataset)-5000,len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17184384-234c-48ed-ab09-b5bcede0ce8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last occurring number or ratio in \"The item costs $123.45, but with a discount of $10.00, the final price is $113.45.\" is: $113.45\n",
      "The last occurring number or ratio in \"The ratio of water to concentrate is 5.5:1 for the mixture.\" is: 5.5:1\n",
      "The last occurring number or ratio in \"The investment return was 10:1.\" is: 10:1\n",
      "The last occurring number or ratio in \"Answer is 42.3.\n",
      "Answer is 42\" is: 42\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_last_number_or_ratio(s):\n",
    "    # Find all sequences of digits, possibly with leading currency symbols, decimal points, and ratios\n",
    "    patterns = re.findall(r'[\\$€£]?\\d+(?:\\.\\d+)?(?:\\:\\d+(?:\\.\\d+)?)?', s)\n",
    "    \n",
    "    # Return the last pattern found, or None if there are no matches\n",
    "    if patterns:\n",
    "        return patterns[-1]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "examples = [\n",
    "    \"The item costs $123.45, but with a discount of $10.00, the final price is $113.45.\",\n",
    "    \"The ratio of water to concentrate is 5.5:1 for the mixture.\",\n",
    "    \"The investment return was 10:1.\",\n",
    "    \"Answer is 42.3.\\nAnswer is 42\"\n",
    "]\n",
    "\n",
    "for s in examples:\n",
    "    print(f\"The last occurring number or ratio in \\\"{s}\\\" is: {extract_last_number_or_ratio(s)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b85d3b42-4343-4b12-83f4-b88240d36342",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_answers_gt = parallel(extract_last_number_or_ratio, dataset['answer'], progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2fb3db0-fd1e-4c52-866c-e113de7786a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [f\"###Question:\\n{question}\\n###Answer:\\n\" for question in dataset[:500]['question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1827693-789f-41d0-bc8b-5bc4965220e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23f49c41-f323-4956-ae58-f81553d8ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "522fb1c2-e834-4a65-82d7-2f28267cb391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 7/7 [07:05<00:00, 60.72s/it]\n"
     ]
    }
   ],
   "source": [
    "answers_pred = []\n",
    "short_answers_pred = []\n",
    "bs = 8\n",
    "for i in tqdm(range(0,len(valid_dataset.select(range(50))),bs)):\n",
    "    \n",
    "    inputs = [f\"###Question:\\n{question}\\n###Answer:\\n\" for question in valid_dataset[i:i+bs]['question']]\n",
    "    input_ids = tokenizer(inputs)['input_ids']\n",
    "    \n",
    "    max_toks = max(len(toks) for toks in input_ids)\n",
    "    b = torch.stack([torch.tensor(((max_toks-len(toks))*[tokenizer.unk_token_id])+toks) for toks in input_ids])\n",
    "    input_lens = [len(toks) for toks in input_ids]\n",
    "    \n",
    "    output = model.generate(b.cuda(), \n",
    "                            do_sample=False, \n",
    "                            use_cache=True,\n",
    "                            pad_token_id=tokenizer.unk_token_id, \n",
    "                            eos_token_id=tokenizer.eos_token_id, \n",
    "                            max_new_tokens=1024).cpu()\n",
    "    \n",
    "    pred = [tokenizer.decode(o[o!=tokenizer.unk_token_id][n:]) for o,n in zip(output,input_lens)]\n",
    "    short_pred = [extract_last_number_or_ratio(p) for p in pred]\n",
    "    \n",
    "    answers_pred.extend(pred)\n",
    "    short_answers_pred.extend(short_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab504da8-9436-47df-bf99-4f9eb332f8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17857142857142858"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p==g for p,g in zip(short_answers_pred, short_answers_gt))/len(short_answers_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deea95c9-52c3-4e90-a535-a68e81ad8ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 50/50 [00:39<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(inputs[:50], SamplingParams(temperature=0.0, stop_token_ids=[tokenizer.eos_token_id], max_tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28aa855d-9897-4746-90cc-827ab0bf835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_answers_pred = [extract_last_number_or_ratio(o.outputs[0].text) for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4259ba1-176e-4661-b97b-142633890e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p==g for p,g in zip(short_answers_pred, short_answers_gt))/len(short_answers_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89c6219e-acce-474e-91f3-4ac6c2a1b149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15923"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(o.outputs[0].token_ids) for o in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72e4fc89-8f78-4c99-b55c-b01d6f46b52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.92307692307692"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50 /(39/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a0da8bc-216b-4584-84d5-c97c9c52dc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:26<00:00,  8.68s/it]\n"
     ]
    }
   ],
   "source": [
    "token_per_sec = []\n",
    "for inp in tqdm(inputs[:3]):\n",
    "    time_taken = timed(llm.generate,[inp], SamplingParams(temperature=0.0, stop_token_ids=[tokenizer.eos_token_id], max_tokens=1024), use_tqdm=False)\n",
    "    token_per_sec.append(len(time_taken[0][0].outputs[0].token_ids) / time_taken[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f391a2c9-0ff1-4c2a-9bf3-af7d3323fb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30.62790650031754, 30.732849847282548, 31.08896707737432]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2559ffce-8e47-45ff-a30c-7cffbbdcfa82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 22172, 3186, 445, 338], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hello world this is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd7f64d0-4c7e-42d7-b37f-a6236a26f351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_taken = timed(llm.generate,[\"hello world this is\"], SamplingParams(temperature=0.0, stop_token_ids=[tokenizer.eos_token_id], max_tokens=256), use_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e71f072e-12ff-4f44-917d-c24aaa1daf65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.96343040495391"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(time_taken[0][0].outputs[0].token_ids) / time_taken[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57080ea4-79fd-4336-b860-c0d962aa6ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448.4848484848485"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dollar / sec - sending 1 request without batching\n",
    "1e6 * (0.74 / 60 / 27.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ebe2db2-a7c2-4189-b540-2514ff2c5d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.956997359486987"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dollar / sec - sending requests with continuous batching\n",
    "1e6 * ((0.74 / 60) / (10604 / 12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abdc8b20-7a56-44fa-a2aa-1ab7fc647af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545.9558823529412"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dollar / sec - sending 1 request without batching\n",
    "1e6 * (1.782 / 2 / 60 / 27.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df2e8010-3efd-42a6-98ef-5f9b95882ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.46480231436837"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dollar / sec - sending requests with continuous batching\n",
    "1e6 * ((1.782 / 60) / (15555 / 39)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e049ce8b-f301-4a02-9c26-bdc07a387d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189.16155419222903"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dollar / sec - sending 1 request without batching (if non-eager mode fixed)\n",
    "1e6 * (0.74 / 60 / 65.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44c58c8c-2e8b-4bf8-aa92-251a6f8e4bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.827581647494381"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dollar / sec - sending requests with continuous batching (if non-eager mode fixed)\n",
    "1e6 * ((0.74 / 60) / (12605 / 8)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9d5cc99-4498-473a-b29c-f5c6a02b39f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_infer(question):\n",
    "    inputs = [f\"###Question:\\n{question}\\n###Answer:\\n\"]\n",
    "    input_ids = tokenizer(inputs)['input_ids']\n",
    "    \n",
    "    max_toks = max(len(toks) for toks in input_ids)\n",
    "    b = torch.stack([torch.tensor(((max_toks-len(toks))*[tokenizer.unk_token_id])+toks) for toks in input_ids])\n",
    "    input_lens = [len(toks) for toks in input_ids]\n",
    "    \n",
    "    output = model.generate(b.cuda(), \n",
    "                            do_sample=False, \n",
    "                            use_cache=True,\n",
    "                            pad_token_id=tokenizer.unk_token_id, \n",
    "                            eos_token_id=tokenizer.eos_token_id, \n",
    "                            max_new_tokens=1024).cpu()\n",
    "\n",
    "    tokens = [o[o!=tokenizer.unk_token_id][n:] for o,n in zip(output,input_lens)][0]\n",
    "    pred = [tokenizer.decode(tokens)]\n",
    "    return pred, tokens, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86b08814-2a5c-4d77-a4dc-bc7edcedff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 10/10 [01:47<00:00, 10.72s/it]\n"
     ]
    }
   ],
   "source": [
    "token_per_sec = []\n",
    "for question in tqdm(dataset[:10]['question']):\n",
    "    time_taken = timed(single_infer, question)\n",
    "    token_per_sec.append(time_taken[0][-1] / time_taken[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "451cbaba-50bf-40c8-b2b1-951f6928bcec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18.71630187320748,\n",
       " 19.143428841315547,\n",
       " 19.14635015085839,\n",
       " 19.184236823150655,\n",
       " 19.221320212171463,\n",
       " 19.072148874737856,\n",
       " 19.108014302928094,\n",
       " 19.104715232776528,\n",
       " 18.919215197450832,\n",
       " 19.106046480172743]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e00ce-8a2a-441c-a25d-787aa9278df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b7aaf0-ca87-48f1-9246-8c1262da2a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7450e2c8-85b2-4857-9a96-50b134d956a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fff7b9-9f52-4edf-a27b-0654fcb08912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f658616-6805-44b1-bb28-814aa4a26780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
