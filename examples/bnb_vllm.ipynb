{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22d8c79-3add-4d49-9cae-e212b493467e",
   "metadata": {},
   "source": [
    "### BNB with Tensor Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff26655-6d13-479a-9af8-9d64083ec9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from bitsandbytes.nn.modules import Params4bit, Linear4bit\n",
    "from bitsandbytes.functional import dequantize_4bit\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.functional import dequantize_4bit, QuantState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c906c609-2777-4148-acac-700d72842b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dea59055-c341-47c3-86b9-f48415d239d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocksize = 64\n",
    "quant_type = \"nf4\"\n",
    "quant_storage = torch.uint8\n",
    "\n",
    "data = torch.randn(128,256).to(torch.bfloat16)\n",
    "param = Params4bit(data, blocksize=blocksize, quant_type=quant_type, \n",
    "                   quant_storage=quant_storage, compress_statistics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b7929ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if quant_storage == torch.uint8:\n",
    "    pack_factor = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6890d631-c63f-4939-a5f4-00883a324aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Parameter(Params4bit([[ 1.3125,  1.9609, -0.0356,  ...,  0.2197,  0.0903,  0.2695],\n",
       "            [-2.0312, -0.8555,  0.9688,  ...,  0.4043,  0.6953,  0.2500],\n",
       "            [ 0.2275,  0.7500,  0.1885,  ..., -1.0391,  1.3516,  1.3516],\n",
       "            ...,\n",
       "            [ 0.5742,  0.3477, -0.2676,  ...,  0.8906,  0.3848,  0.6484],\n",
       "            [ 2.3281, -1.3125,  0.9180,  ..., -1.9609,  0.4219, -0.3652],\n",
       "            [ 1.2969,  0.2656, -0.5664,  ...,  0.8164,  0.6562, -1.2109]],\n",
       "           dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "222fae9b-6520-46de-bd43-21867026cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "920862c7-0c28-43cf-ba29-5a624eadaace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16384, 1]), 'nf4', torch.uint8, 64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.shape, param.quant_type, param.quant_storage, param.blocksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a273b007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.numel() / data.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a3b663b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_type': 'nf4',\n",
       " 'absmax': tensor([2.5156, 2.8906, 2.3906, 2.8438, 2.5625, 3.0625, 1.9219, 2.3906, 2.8594,\n",
       "         2.9531, 2.5469, 2.2812, 2.7812, 3.7969, 3.0625, 2.4062, 2.2500, 2.8125,\n",
       "         2.0781, 2.7656, 2.8281, 2.7500, 3.0625, 3.2969, 3.2656, 1.9922, 2.3750,\n",
       "         2.8594, 2.2344, 2.4844, 3.3125, 3.0625, 1.8828, 2.4688, 2.2031, 2.7344,\n",
       "         2.8594, 2.7656, 2.2344, 2.9844, 2.1719, 2.9062, 2.2188, 1.8281, 2.6406,\n",
       "         2.1094, 3.4688, 3.0000, 3.3750, 2.6250, 2.6719, 3.2969, 2.4688, 2.5312,\n",
       "         2.1562, 2.7969, 2.2031, 2.6719, 3.0781, 2.1875, 2.1406, 2.7969, 2.3594,\n",
       "         2.2969, 2.3750, 2.7188, 2.9844, 2.7656, 2.9688, 3.0781, 2.4219, 2.7969,\n",
       "         2.8594, 3.1875, 3.2344, 3.2031, 2.6250, 2.2344, 2.1250, 2.3750, 2.3750,\n",
       "         2.3594, 2.1875, 3.0000, 2.1562, 2.8594, 2.0156, 2.6250, 2.8906, 2.1562,\n",
       "         2.2812, 2.4375, 2.6250, 3.1562, 2.2031, 2.3750, 2.2344, 2.3594, 2.5000,\n",
       "         3.1875, 2.3281, 3.0625, 2.7344, 3.1719, 2.2031, 3.0781, 2.5156, 2.2344,\n",
       "         2.9844, 3.4062, 2.2031, 2.8281, 2.3125, 2.2812, 2.5781, 2.8125, 2.8906,\n",
       "         2.1875, 2.7031, 3.9375, 2.9375, 2.4531, 3.0625, 2.6562, 2.5781, 2.0312,\n",
       "         2.4531, 2.6250, 2.7500, 2.9375, 2.4375, 2.1875, 1.7422, 2.3750, 2.5781,\n",
       "         2.4375, 3.9531, 2.8906, 2.7969, 2.4219, 3.0156, 2.5938, 3.0156, 2.9375,\n",
       "         2.7500, 2.3594, 2.5469, 3.2812, 2.2812, 3.1562, 2.0156, 2.3906, 1.9766,\n",
       "         2.5938, 2.9688, 2.2812, 2.5469, 2.6094, 2.2500, 2.2812, 2.2344, 2.3750,\n",
       "         2.6562, 2.8125, 2.5156, 3.2812, 3.2031, 2.7969, 2.8750, 3.3906, 3.4844,\n",
       "         3.2500, 2.4688, 2.5938, 2.0156, 2.5469, 3.0781, 3.1562, 2.7188, 2.1719,\n",
       "         2.4531, 2.5469, 2.1250, 3.0312, 3.0781, 2.0156, 2.8125, 2.0469, 2.7344,\n",
       "         3.0469, 2.8750, 3.4062, 2.2656, 2.8125, 2.4531, 2.6250, 2.6562, 2.0781,\n",
       "         2.5625, 2.6250, 2.9375, 1.9531, 3.0781, 1.8516, 3.7656, 3.3906, 2.5469,\n",
       "         3.9219, 3.0312, 3.0938, 3.2188, 3.3906, 2.9531, 2.0156, 1.8047, 2.5156,\n",
       "         2.0938, 2.4688, 2.5156, 2.6094, 2.6094, 3.4375, 2.2812, 2.1562, 2.9844,\n",
       "         2.8594, 1.7891, 3.2031, 2.5156, 2.8906, 2.3750, 2.0469, 3.0312, 3.0312,\n",
       "         2.5625, 2.4844, 2.6094, 2.2031, 2.4688, 2.5156, 2.6562, 2.1719, 3.0938,\n",
       "         3.2812, 1.8594, 2.9375, 3.0938, 2.7656, 2.8906, 2.7812, 2.2188, 2.7500,\n",
       "         2.4844, 2.3281, 2.4844, 2.1562, 2.7969, 3.4531, 2.4531, 2.7031, 3.0625,\n",
       "         2.5000, 3.1094, 2.4531, 2.2031, 1.8828, 2.5625, 2.0469, 2.4531, 3.0156,\n",
       "         2.8125, 2.3906, 3.1094, 2.6094, 2.3281, 2.1562, 2.2969, 2.7188, 3.0469,\n",
       "         2.1250, 2.8125, 2.8125, 2.4219, 2.5625, 2.5625, 3.2344, 2.7031, 2.8750,\n",
       "         2.6719, 2.5625, 2.6406, 2.0000, 2.6562, 2.4375, 2.0156, 2.5625, 2.8438,\n",
       "         2.2500, 1.8281, 1.9766, 3.3281, 2.7500, 2.2500, 3.0938, 3.5312, 3.3125,\n",
       "         2.1562, 2.6562, 2.3750, 2.3281, 2.3750, 2.8906, 2.3906, 2.7812, 2.0781,\n",
       "         3.3281, 2.5938, 3.0781, 1.8281, 2.0000, 2.2188, 2.8750, 3.3750, 3.5156,\n",
       "         2.4375, 2.3750, 2.8750, 2.3438, 2.2188, 1.6719, 2.4531, 2.1562, 2.4219,\n",
       "         2.3281, 2.6250, 2.5156, 2.0781, 2.0781, 2.4531, 2.0938, 2.3750, 2.5469,\n",
       "         2.3594, 2.6406, 2.2500, 2.0938, 3.1875, 2.0625, 2.7500, 2.3125, 2.8438,\n",
       "         3.3125, 3.0469, 2.3906, 3.2812, 3.1250, 2.3750, 2.4219, 2.3438, 2.9688,\n",
       "         2.0625, 2.5000, 2.6094, 2.7344, 2.9688, 2.3125, 2.3438, 3.2344, 2.7656,\n",
       "         1.9766, 2.7344, 3.1406, 3.1094, 2.0938, 3.0625, 2.2344, 2.6875, 2.5156,\n",
       "         2.9375, 1.9219, 3.1406, 3.1406, 2.5781, 1.8828, 2.3281, 3.0625, 2.2500,\n",
       "         3.0625, 2.5469, 2.4062, 2.3438, 2.3438, 2.9688, 3.2969, 2.8281, 3.4531,\n",
       "         2.9219, 2.1875, 2.1562, 2.2031, 2.4375, 2.8281, 2.6875, 2.5469, 3.1875,\n",
       "         2.6406, 2.8281, 3.2031, 2.5625, 2.6719, 2.2969, 2.5938, 2.2344, 2.3594,\n",
       "         2.3281, 2.7656, 2.0625, 1.7969, 2.7656, 2.6406, 2.0781, 3.2344, 2.3906,\n",
       "         2.4375, 2.6562, 2.6406, 3.5312, 2.4688, 2.8438, 2.4062, 2.2812, 2.7500,\n",
       "         2.1875, 2.3750, 2.3750, 2.7969, 2.6250, 2.2344, 2.4062, 2.8594, 2.6094,\n",
       "         2.4531, 4.4062, 2.7344, 2.4531, 2.7031, 2.4219, 3.3906, 3.0312, 2.6719,\n",
       "         2.4531, 2.9219, 3.0000, 2.0469, 2.1719, 2.7656, 2.0781, 2.5156, 2.5781,\n",
       "         2.4531, 2.2031, 2.2656, 2.7188, 2.7500, 2.8594, 2.0625, 2.9844, 3.2188,\n",
       "         2.9219, 2.5312, 2.4688, 3.0156, 3.0781, 2.4375, 2.5469, 2.8906, 3.3281,\n",
       "         2.2656, 2.7812, 2.3750, 2.7969, 3.6719, 2.3125, 2.1094, 2.5469, 2.4844,\n",
       "         3.5938, 3.8906, 2.1562, 2.9062, 1.8984, 2.2500, 2.7188, 2.3281, 2.7188,\n",
       "         2.5000, 1.9844, 3.2344, 2.5938, 2.3438, 3.0000, 2.1719, 2.2812, 2.4844,\n",
       "         2.7969, 2.5625, 3.1875, 2.4062, 2.8281, 2.4375, 2.7969, 2.9688],\n",
       "        device='cuda:0'),\n",
       " 'blocksize': 64,\n",
       " 'quant_map': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
       "          0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
       "        device='cuda:0'),\n",
       " 'dtype': 'bfloat16',\n",
       " 'shape': (128, 256)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.quant_state.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b110f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size_per_partition = 64\n",
    "output_size_per_partition = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35c5479f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row-major quantization, reshape for vllm tensor parallelism\n",
    "qweight = param.data.reshape(data.size(0), data.size(1) // pack_factor); qweight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd34c2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[222, 115, 183,  ..., 106, 120, 120],\n",
       "        [ 20, 185, 205,  ..., 217,  25, 168],\n",
       "        [138, 132,  81,  ..., 160,  99, 221],\n",
       "        ...,\n",
       "        [152, 103, 182,  ..., 165,  43, 154],\n",
       "        [226, 181, 167,  ..., 133, 145, 149],\n",
       "        [200,  82, 216,  ..., 183, 154, 163]], device='cuda:0',\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b3a5601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[222],\n",
       "        [115],\n",
       "        [183],\n",
       "        ...,\n",
       "        [183],\n",
       "        [154],\n",
       "        [163]], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qweight.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11e42bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deqweight = dequantize_4bit(qweight.view(-1,1), param.quant_state, blocksize=blocksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e794c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(256., dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data - torch.randn_like(data)).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4c3eb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.7500, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data - deqweight.cpu()).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c66129c-c120-4b67-bf0c-fee3d8e94a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 128).cuda().to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed100e7d-5097-4682-9a6d-333f4434c877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_type': 'nf4',\n",
       " 'absmax': tensor([2.5156, 2.8906, 2.3906, 2.8438, 2.5625, 3.0625, 1.9219, 2.3906, 2.8594,\n",
       "         2.9531, 2.5469, 2.2812, 2.7812, 3.7969, 3.0625, 2.4062, 2.2500, 2.8125,\n",
       "         2.0781, 2.7656, 2.8281, 2.7500, 3.0625, 3.2969, 3.2656, 1.9922, 2.3750,\n",
       "         2.8594, 2.2344, 2.4844, 3.3125, 3.0625, 1.8828, 2.4688, 2.2031, 2.7344,\n",
       "         2.8594, 2.7656, 2.2344, 2.9844, 2.1719, 2.9062, 2.2188, 1.8281, 2.6406,\n",
       "         2.1094, 3.4688, 3.0000, 3.3750, 2.6250, 2.6719, 3.2969, 2.4688, 2.5312,\n",
       "         2.1562, 2.7969, 2.2031, 2.6719, 3.0781, 2.1875, 2.1406, 2.7969, 2.3594,\n",
       "         2.2969, 2.3750, 2.7188, 2.9844, 2.7656, 2.9688, 3.0781, 2.4219, 2.7969,\n",
       "         2.8594, 3.1875, 3.2344, 3.2031, 2.6250, 2.2344, 2.1250, 2.3750, 2.3750,\n",
       "         2.3594, 2.1875, 3.0000, 2.1562, 2.8594, 2.0156, 2.6250, 2.8906, 2.1562,\n",
       "         2.2812, 2.4375, 2.6250, 3.1562, 2.2031, 2.3750, 2.2344, 2.3594, 2.5000,\n",
       "         3.1875, 2.3281, 3.0625, 2.7344, 3.1719, 2.2031, 3.0781, 2.5156, 2.2344,\n",
       "         2.9844, 3.4062, 2.2031, 2.8281, 2.3125, 2.2812, 2.5781, 2.8125, 2.8906,\n",
       "         2.1875, 2.7031, 3.9375, 2.9375, 2.4531, 3.0625, 2.6562, 2.5781, 2.0312,\n",
       "         2.4531, 2.6250, 2.7500, 2.9375, 2.4375, 2.1875, 1.7422, 2.3750, 2.5781,\n",
       "         2.4375, 3.9531, 2.8906, 2.7969, 2.4219, 3.0156, 2.5938, 3.0156, 2.9375,\n",
       "         2.7500, 2.3594, 2.5469, 3.2812, 2.2812, 3.1562, 2.0156, 2.3906, 1.9766,\n",
       "         2.5938, 2.9688, 2.2812, 2.5469, 2.6094, 2.2500, 2.2812, 2.2344, 2.3750,\n",
       "         2.6562, 2.8125, 2.5156, 3.2812, 3.2031, 2.7969, 2.8750, 3.3906, 3.4844,\n",
       "         3.2500, 2.4688, 2.5938, 2.0156, 2.5469, 3.0781, 3.1562, 2.7188, 2.1719,\n",
       "         2.4531, 2.5469, 2.1250, 3.0312, 3.0781, 2.0156, 2.8125, 2.0469, 2.7344,\n",
       "         3.0469, 2.8750, 3.4062, 2.2656, 2.8125, 2.4531, 2.6250, 2.6562, 2.0781,\n",
       "         2.5625, 2.6250, 2.9375, 1.9531, 3.0781, 1.8516, 3.7656, 3.3906, 2.5469,\n",
       "         3.9219, 3.0312, 3.0938, 3.2188, 3.3906, 2.9531, 2.0156, 1.8047, 2.5156,\n",
       "         2.0938, 2.4688, 2.5156, 2.6094, 2.6094, 3.4375, 2.2812, 2.1562, 2.9844,\n",
       "         2.8594, 1.7891, 3.2031, 2.5156, 2.8906, 2.3750, 2.0469, 3.0312, 3.0312,\n",
       "         2.5625, 2.4844, 2.6094, 2.2031, 2.4688, 2.5156, 2.6562, 2.1719, 3.0938,\n",
       "         3.2812, 1.8594, 2.9375, 3.0938, 2.7656, 2.8906, 2.7812, 2.2188, 2.7500,\n",
       "         2.4844, 2.3281, 2.4844, 2.1562, 2.7969, 3.4531, 2.4531, 2.7031, 3.0625,\n",
       "         2.5000, 3.1094, 2.4531, 2.2031, 1.8828, 2.5625, 2.0469, 2.4531, 3.0156,\n",
       "         2.8125, 2.3906, 3.1094, 2.6094, 2.3281, 2.1562, 2.2969, 2.7188, 3.0469,\n",
       "         2.1250, 2.8125, 2.8125, 2.4219, 2.5625, 2.5625, 3.2344, 2.7031, 2.8750,\n",
       "         2.6719, 2.5625, 2.6406, 2.0000, 2.6562, 2.4375, 2.0156, 2.5625, 2.8438,\n",
       "         2.2500, 1.8281, 1.9766, 3.3281, 2.7500, 2.2500, 3.0938, 3.5312, 3.3125,\n",
       "         2.1562, 2.6562, 2.3750, 2.3281, 2.3750, 2.8906, 2.3906, 2.7812, 2.0781,\n",
       "         3.3281, 2.5938, 3.0781, 1.8281, 2.0000, 2.2188, 2.8750, 3.3750, 3.5156,\n",
       "         2.4375, 2.3750, 2.8750, 2.3438, 2.2188, 1.6719, 2.4531, 2.1562, 2.4219,\n",
       "         2.3281, 2.6250, 2.5156, 2.0781, 2.0781, 2.4531, 2.0938, 2.3750, 2.5469,\n",
       "         2.3594, 2.6406, 2.2500, 2.0938, 3.1875, 2.0625, 2.7500, 2.3125, 2.8438,\n",
       "         3.3125, 3.0469, 2.3906, 3.2812, 3.1250, 2.3750, 2.4219, 2.3438, 2.9688,\n",
       "         2.0625, 2.5000, 2.6094, 2.7344, 2.9688, 2.3125, 2.3438, 3.2344, 2.7656,\n",
       "         1.9766, 2.7344, 3.1406, 3.1094, 2.0938, 3.0625, 2.2344, 2.6875, 2.5156,\n",
       "         2.9375, 1.9219, 3.1406, 3.1406, 2.5781, 1.8828, 2.3281, 3.0625, 2.2500,\n",
       "         3.0625, 2.5469, 2.4062, 2.3438, 2.3438, 2.9688, 3.2969, 2.8281, 3.4531,\n",
       "         2.9219, 2.1875, 2.1562, 2.2031, 2.4375, 2.8281, 2.6875, 2.5469, 3.1875,\n",
       "         2.6406, 2.8281, 3.2031, 2.5625, 2.6719, 2.2969, 2.5938, 2.2344, 2.3594,\n",
       "         2.3281, 2.7656, 2.0625, 1.7969, 2.7656, 2.6406, 2.0781, 3.2344, 2.3906,\n",
       "         2.4375, 2.6562, 2.6406, 3.5312, 2.4688, 2.8438, 2.4062, 2.2812, 2.7500,\n",
       "         2.1875, 2.3750, 2.3750, 2.7969, 2.6250, 2.2344, 2.4062, 2.8594, 2.6094,\n",
       "         2.4531, 4.4062, 2.7344, 2.4531, 2.7031, 2.4219, 3.3906, 3.0312, 2.6719,\n",
       "         2.4531, 2.9219, 3.0000, 2.0469, 2.1719, 2.7656, 2.0781, 2.5156, 2.5781,\n",
       "         2.4531, 2.2031, 2.2656, 2.7188, 2.7500, 2.8594, 2.0625, 2.9844, 3.2188,\n",
       "         2.9219, 2.5312, 2.4688, 3.0156, 3.0781, 2.4375, 2.5469, 2.8906, 3.3281,\n",
       "         2.2656, 2.7812, 2.3750, 2.7969, 3.6719, 2.3125, 2.1094, 2.5469, 2.4844,\n",
       "         3.5938, 3.8906, 2.1562, 2.9062, 1.8984, 2.2500, 2.7188, 2.3281, 2.7188,\n",
       "         2.5000, 1.9844, 3.2344, 2.5938, 2.3438, 3.0000, 2.1719, 2.2812, 2.4844,\n",
       "         2.7969, 2.5625, 3.1875, 2.4062, 2.8281, 2.4375, 2.7969, 2.9688],\n",
       "        device='cuda:0'),\n",
       " 'blocksize': 64,\n",
       " 'quant_map': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
       "          0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
       "        device='cuda:0'),\n",
       " 'dtype': 'bfloat16',\n",
       " 'shape': (128, 256)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.quant_state.as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a33876",
   "metadata": {},
   "source": [
    "### Column Parallel\n",
    "\n",
    "The linear layer is defined as Y = XA + b. A is parallelized along its second dimension as A = [A_1, ..., A_p]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5883249c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qweight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ec7d2b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size_per_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b8e60aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "qweight_partitioned = qweight.split(output_size_per_partition, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e2f41ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qweight_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7746431f-166b-4a71-959c-83cead869e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n"
     ]
    }
   ],
   "source": [
    "for w in qweight_partitioned: print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "be00bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_absmax = param.quant_state.absmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a3d776ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_absmax_reshaped = orig_absmax.reshape(-1, data.size(1) // blocksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "567384ac-3657-4600-89db-737eb4a8ad73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_absmax_reshaped.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3ad52fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 4])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_absmax_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dfd94e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = len(qweight_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f63f1e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "absmax_partitioned = orig_absmax_reshaped.split(orig_absmax_reshaped.size(1) // num_partitions, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9d276291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2])\n",
      "torch.Size([128, 2])\n"
     ]
    }
   ],
   "source": [
    "for a in absmax_partitioned: print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9e9668e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qweight_partitioned), len(absmax_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3e7299d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state = copy.deepcopy(param.quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "80b8bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state.shape = torch.Size([quant_state.shape[0], quant_state.shape[1]//num_partitions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "de428e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "241a2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state.absmax = absmax_partitioned[0].contiguous().view(-1)\n",
    "deqweight_part1 = dequantize_4bit(qweight_partitioned[0].contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "quant_state.absmax = absmax_partitioned[1].contiguous().view(-1)\n",
    "deqweight_part2 = dequantize_4bit(qweight_partitioned[1].contiguous().view(-1,1), quant_state=quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e15c1671-03b7-45a4-b036-72bf3e82c43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 128]), torch.Size([128, 128]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deqweight_part1.shape, deqweight_part2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e4085cb3-2b80-492a-b89b-ca259ce950c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# quant_state.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b63ec5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 256])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deqweight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "978e7a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 256])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([deqweight_part1, deqweight_part2], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3d50215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(deqweight, torch.cat([deqweight_part1, deqweight_part2], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a43bafd4-7b83-4314-9019-a41499be5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = (x @ deqweight_part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4f2fe31f-1af2-4712-9356-b9894c2d7e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out2 = bnb.matmul_4bit(x, qweight_partitioned[1].contiguous().view(-1,1), quant_state=quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1bf8b879-052b-48f4-ba5c-d28e1672d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(out1, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "af60fb46-97b8-4b65-8f65-ddb11fd02c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc17f5f4",
   "metadata": {},
   "source": [
    "### Row Parallel\n",
    "\n",
    "The linear layer is defined as Y = XA + b. A is parallelized along\n",
    "its first dimension and X along its second dimension as:\n",
    "\n",
    "```\n",
    "    -   -\n",
    "    | A_1 |\n",
    "    | .   |\n",
    "A = | .   |        X = [X_1, ..., X_p]\n",
    "    | .   |\n",
    "    | A_p |\n",
    "    -   -\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c3dc1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qweight_partitioned = qweight.split(output_size_per_partition, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98d0add2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_partitions = len(qweight_partitioned); num_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77c5bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "for w in qweight_partitioned: print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed58848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_absmax = param.quant_state.absmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e368bab4-38d8-4ee5-9ca7-8a0943fd7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_absmax_reshaped = orig_absmax.reshape(-1, data.size(1) // blocksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b155bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "absmax_partitioned = orig_absmax.split(len(orig_absmax) // num_partitions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "130b5f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(absmax_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e64143be",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state = copy.deepcopy(param.quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47f91077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_state.shape = torch.Size([quant_state.shape[0]//num_partitions, quant_state.shape[1]]); quant_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "128e667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state.absmax = absmax_partitioned[0].contiguous().view(-1)\n",
    "deqweight_part1 = dequantize_4bit(qweight_partitioned[0].contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "quant_state.absmax = absmax_partitioned[1].contiguous().view(-1)\n",
    "deqweight_part2 = dequantize_4bit(qweight_partitioned[1].contiguous().view(-1,1), quant_state=quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d735c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(deqweight, torch.cat([deqweight_part1, deqweight_part2], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53f04e-f053-423b-b3f7-88283531858c",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39e0da2-6d9e-4a0e-bec9-7e4007c8af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-29 13:23:10 pynccl_utils.py:13] vLLM is using nccl==2.18.1\n"
     ]
    }
   ],
   "source": [
    "from vllm.model_executor.weight_utils import (default_weight_loader,\n",
    "                                              hf_model_weights_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b267b53-a7d4-43af-82e2-ac18b6b66a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_iterator = hf_model_weights_iterator(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdc64de-22cd-413c-9b63-e2351e018c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-29 08:48:45 weight_utils.py:177] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd674cc5dcf7487f923f00c680e6a73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93c344b17544540b2d5f59dac5af4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for name, loaded_weight in weights_iterator: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0853623c-1c05-4599-8b74-26e55d33a310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model.embed_tokens.weight',\n",
       " tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
       "          -6.5565e-06,  8.9407e-07],\n",
       "         [ 1.8616e-03, -3.3722e-03,  3.9864e-04,  ..., -8.3008e-03,\n",
       "           2.5787e-03, -3.9368e-03],\n",
       "         [ 1.0986e-02,  9.8877e-03, -5.0964e-03,  ...,  2.5177e-03,\n",
       "           7.7057e-04, -5.0049e-03],\n",
       "         ...,\n",
       "         [-1.3977e-02, -2.7313e-03, -1.9897e-02,  ..., -1.0437e-02,\n",
       "           9.5825e-03, -1.8005e-03],\n",
       "         [-1.0742e-02,  9.3384e-03,  1.2939e-02,  ..., -3.3203e-02,\n",
       "          -1.6357e-02,  3.3875e-03],\n",
       "         [-8.3008e-03, -4.0588e-03, -1.1063e-03,  ...,  3.4790e-03,\n",
       "          -1.2939e-02,  3.1948e-05]], dtype=torch.float16))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name, loaded_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b31e4388-af56-49ee-b81e-a4d2a333bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_iterator = hf_model_weights_iterator(\"TheBloke/CodeUp-Alpha-13B-HF-AWQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b23f97a-ad91-434b-aa63-2fa548c0193d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.mlp.down_proj.qzeros\n",
      "model.layers.0.mlp.down_proj.scales\n",
      "model.layers.0.mlp.gate_proj.qzeros\n",
      "model.layers.0.mlp.gate_proj.scales\n",
      "model.layers.0.mlp.up_proj.qzeros\n",
      "model.layers.0.mlp.up_proj.scales\n",
      "model.layers.0.self_attn.k_proj.qzeros\n",
      "model.layers.0.self_attn.k_proj.scales\n",
      "model.layers.0.self_attn.o_proj.qzeros\n",
      "model.layers.0.self_attn.o_proj.scales\n",
      "model.layers.0.self_attn.q_proj.qzeros\n",
      "model.layers.0.self_attn.q_proj.scales\n",
      "model.layers.0.self_attn.v_proj.qzeros\n",
      "model.layers.0.self_attn.v_proj.scales\n",
      "model.layers.1.mlp.down_proj.qzeros\n",
      "model.layers.1.mlp.down_proj.scales\n",
      "model.layers.1.mlp.gate_proj.qzeros\n",
      "model.layers.1.mlp.gate_proj.scales\n",
      "model.layers.1.mlp.up_proj.qzeros\n",
      "model.layers.1.mlp.up_proj.scales\n",
      "model.layers.1.self_attn.k_proj.qzeros\n",
      "model.layers.1.self_attn.k_proj.scales\n",
      "model.layers.1.self_attn.o_proj.qzeros\n",
      "model.layers.1.self_attn.o_proj.scales\n",
      "model.layers.1.self_attn.q_proj.qzeros\n",
      "model.layers.1.self_attn.q_proj.scales\n",
      "model.layers.1.self_attn.v_proj.qzeros\n",
      "model.layers.1.self_attn.v_proj.scales\n",
      "model.layers.10.mlp.down_proj.qzeros\n",
      "model.layers.10.mlp.down_proj.scales\n",
      "model.layers.10.mlp.gate_proj.qzeros\n",
      "model.layers.10.mlp.gate_proj.scales\n",
      "model.layers.10.mlp.up_proj.qzeros\n",
      "model.layers.10.mlp.up_proj.scales\n",
      "model.layers.10.self_attn.k_proj.qzeros\n",
      "model.layers.10.self_attn.k_proj.scales\n",
      "model.layers.10.self_attn.o_proj.qzeros\n",
      "model.layers.10.self_attn.o_proj.scales\n",
      "model.layers.10.self_attn.q_proj.qzeros\n",
      "model.layers.10.self_attn.q_proj.scales\n",
      "model.layers.10.self_attn.v_proj.qzeros\n",
      "model.layers.10.self_attn.v_proj.scales\n",
      "model.layers.11.mlp.down_proj.qzeros\n",
      "model.layers.11.mlp.down_proj.scales\n",
      "model.layers.11.mlp.gate_proj.qzeros\n",
      "model.layers.11.mlp.gate_proj.scales\n",
      "model.layers.11.mlp.up_proj.qzeros\n",
      "model.layers.11.mlp.up_proj.scales\n",
      "model.layers.11.self_attn.k_proj.qzeros\n",
      "model.layers.11.self_attn.k_proj.scales\n",
      "model.layers.11.self_attn.o_proj.qzeros\n",
      "model.layers.11.self_attn.o_proj.scales\n",
      "model.layers.11.self_attn.q_proj.qzeros\n",
      "model.layers.11.self_attn.q_proj.scales\n",
      "model.layers.11.self_attn.v_proj.qzeros\n",
      "model.layers.11.self_attn.v_proj.scales\n",
      "model.layers.12.mlp.down_proj.qzeros\n",
      "model.layers.12.mlp.down_proj.scales\n",
      "model.layers.12.mlp.gate_proj.qzeros\n",
      "model.layers.12.mlp.gate_proj.scales\n",
      "model.layers.12.mlp.up_proj.qzeros\n",
      "model.layers.12.mlp.up_proj.scales\n",
      "model.layers.12.self_attn.k_proj.qzeros\n",
      "model.layers.12.self_attn.k_proj.scales\n",
      "model.layers.12.self_attn.o_proj.qzeros\n",
      "model.layers.12.self_attn.o_proj.scales\n",
      "model.layers.12.self_attn.q_proj.qzeros\n",
      "model.layers.12.self_attn.q_proj.scales\n",
      "model.layers.12.self_attn.v_proj.qzeros\n",
      "model.layers.12.self_attn.v_proj.scales\n",
      "model.layers.13.mlp.down_proj.qzeros\n",
      "model.layers.13.mlp.down_proj.scales\n",
      "model.layers.13.mlp.gate_proj.qzeros\n",
      "model.layers.13.mlp.gate_proj.scales\n",
      "model.layers.13.mlp.up_proj.qzeros\n",
      "model.layers.13.mlp.up_proj.scales\n",
      "model.layers.13.self_attn.k_proj.qzeros\n",
      "model.layers.13.self_attn.k_proj.scales\n",
      "model.layers.13.self_attn.o_proj.qzeros\n",
      "model.layers.13.self_attn.o_proj.scales\n",
      "model.layers.13.self_attn.q_proj.qzeros\n",
      "model.layers.13.self_attn.q_proj.scales\n",
      "model.layers.13.self_attn.v_proj.qzeros\n",
      "model.layers.13.self_attn.v_proj.scales\n",
      "model.layers.14.mlp.down_proj.qzeros\n",
      "model.layers.14.mlp.down_proj.scales\n",
      "model.layers.14.mlp.gate_proj.qzeros\n",
      "model.layers.14.mlp.gate_proj.scales\n",
      "model.layers.14.mlp.up_proj.qzeros\n",
      "model.layers.14.mlp.up_proj.scales\n",
      "model.layers.14.self_attn.k_proj.qzeros\n",
      "model.layers.14.self_attn.k_proj.scales\n",
      "model.layers.14.self_attn.o_proj.qzeros\n",
      "model.layers.14.self_attn.o_proj.scales\n",
      "model.layers.14.self_attn.q_proj.qzeros\n",
      "model.layers.14.self_attn.q_proj.scales\n",
      "model.layers.14.self_attn.v_proj.qzeros\n",
      "model.layers.14.self_attn.v_proj.scales\n",
      "model.layers.15.mlp.down_proj.qzeros\n",
      "model.layers.15.mlp.down_proj.scales\n",
      "model.layers.15.mlp.gate_proj.qzeros\n",
      "model.layers.15.mlp.gate_proj.scales\n",
      "model.layers.15.mlp.up_proj.qzeros\n",
      "model.layers.15.mlp.up_proj.scales\n",
      "model.layers.15.self_attn.k_proj.qzeros\n",
      "model.layers.15.self_attn.k_proj.scales\n",
      "model.layers.15.self_attn.o_proj.qzeros\n",
      "model.layers.15.self_attn.o_proj.scales\n",
      "model.layers.15.self_attn.q_proj.qzeros\n",
      "model.layers.15.self_attn.q_proj.scales\n",
      "model.layers.15.self_attn.v_proj.qzeros\n",
      "model.layers.15.self_attn.v_proj.scales\n",
      "model.layers.16.mlp.down_proj.qzeros\n",
      "model.layers.16.mlp.down_proj.scales\n",
      "model.layers.16.mlp.gate_proj.qzeros\n",
      "model.layers.16.mlp.gate_proj.scales\n",
      "model.layers.16.mlp.up_proj.qzeros\n",
      "model.layers.16.mlp.up_proj.scales\n",
      "model.layers.16.self_attn.k_proj.qzeros\n",
      "model.layers.16.self_attn.k_proj.scales\n",
      "model.layers.16.self_attn.o_proj.qzeros\n",
      "model.layers.16.self_attn.o_proj.scales\n",
      "model.layers.16.self_attn.q_proj.qzeros\n",
      "model.layers.16.self_attn.q_proj.scales\n",
      "model.layers.16.self_attn.v_proj.qzeros\n",
      "model.layers.16.self_attn.v_proj.scales\n",
      "model.layers.17.mlp.down_proj.qzeros\n",
      "model.layers.17.mlp.down_proj.scales\n",
      "model.layers.17.mlp.gate_proj.qzeros\n",
      "model.layers.17.mlp.gate_proj.scales\n",
      "model.layers.17.mlp.up_proj.qzeros\n",
      "model.layers.17.mlp.up_proj.scales\n",
      "model.layers.17.self_attn.k_proj.qzeros\n",
      "model.layers.17.self_attn.k_proj.scales\n",
      "model.layers.17.self_attn.o_proj.qzeros\n",
      "model.layers.17.self_attn.o_proj.scales\n",
      "model.layers.17.self_attn.q_proj.qzeros\n",
      "model.layers.17.self_attn.q_proj.scales\n",
      "model.layers.17.self_attn.v_proj.qzeros\n",
      "model.layers.17.self_attn.v_proj.scales\n",
      "model.layers.18.mlp.down_proj.qzeros\n",
      "model.layers.18.mlp.down_proj.scales\n",
      "model.layers.18.mlp.gate_proj.qzeros\n",
      "model.layers.18.mlp.gate_proj.scales\n",
      "model.layers.18.mlp.up_proj.qzeros\n",
      "model.layers.18.mlp.up_proj.scales\n",
      "model.layers.18.self_attn.k_proj.qzeros\n",
      "model.layers.18.self_attn.k_proj.scales\n",
      "model.layers.18.self_attn.o_proj.qzeros\n",
      "model.layers.18.self_attn.o_proj.scales\n",
      "model.layers.18.self_attn.q_proj.qzeros\n",
      "model.layers.18.self_attn.q_proj.scales\n",
      "model.layers.18.self_attn.v_proj.qzeros\n",
      "model.layers.18.self_attn.v_proj.scales\n",
      "model.layers.19.mlp.down_proj.qzeros\n",
      "model.layers.19.mlp.down_proj.scales\n",
      "model.layers.19.mlp.gate_proj.qzeros\n",
      "model.layers.19.mlp.gate_proj.scales\n",
      "model.layers.19.mlp.up_proj.qzeros\n",
      "model.layers.19.mlp.up_proj.scales\n",
      "model.layers.19.self_attn.k_proj.qzeros\n",
      "model.layers.19.self_attn.k_proj.scales\n",
      "model.layers.19.self_attn.o_proj.qzeros\n",
      "model.layers.19.self_attn.o_proj.scales\n",
      "model.layers.19.self_attn.q_proj.qzeros\n",
      "model.layers.19.self_attn.q_proj.scales\n",
      "model.layers.19.self_attn.v_proj.qzeros\n",
      "model.layers.19.self_attn.v_proj.scales\n",
      "model.layers.2.mlp.down_proj.qzeros\n",
      "model.layers.2.mlp.down_proj.scales\n",
      "model.layers.2.mlp.gate_proj.qzeros\n",
      "model.layers.2.mlp.gate_proj.scales\n",
      "model.layers.2.mlp.up_proj.qzeros\n",
      "model.layers.2.mlp.up_proj.scales\n",
      "model.layers.2.self_attn.k_proj.qzeros\n",
      "model.layers.2.self_attn.k_proj.scales\n",
      "model.layers.2.self_attn.o_proj.qzeros\n",
      "model.layers.2.self_attn.o_proj.scales\n",
      "model.layers.2.self_attn.q_proj.qzeros\n",
      "model.layers.2.self_attn.q_proj.scales\n",
      "model.layers.2.self_attn.v_proj.qzeros\n",
      "model.layers.2.self_attn.v_proj.scales\n",
      "model.layers.20.mlp.down_proj.qzeros\n",
      "model.layers.20.mlp.down_proj.scales\n",
      "model.layers.20.mlp.gate_proj.qzeros\n",
      "model.layers.20.mlp.gate_proj.scales\n",
      "model.layers.20.mlp.up_proj.qzeros\n",
      "model.layers.20.mlp.up_proj.scales\n",
      "model.layers.20.self_attn.k_proj.qzeros\n",
      "model.layers.20.self_attn.k_proj.scales\n",
      "model.layers.20.self_attn.o_proj.qzeros\n",
      "model.layers.20.self_attn.o_proj.scales\n",
      "model.layers.20.self_attn.q_proj.qzeros\n",
      "model.layers.20.self_attn.q_proj.scales\n",
      "model.layers.20.self_attn.v_proj.qzeros\n",
      "model.layers.20.self_attn.v_proj.scales\n",
      "model.layers.21.mlp.down_proj.qzeros\n",
      "model.layers.21.mlp.down_proj.scales\n",
      "model.layers.21.mlp.gate_proj.qzeros\n",
      "model.layers.21.mlp.gate_proj.scales\n",
      "model.layers.21.mlp.up_proj.qzeros\n",
      "model.layers.21.mlp.up_proj.scales\n",
      "model.layers.21.self_attn.k_proj.qzeros\n",
      "model.layers.21.self_attn.k_proj.scales\n",
      "model.layers.21.self_attn.o_proj.qzeros\n",
      "model.layers.21.self_attn.o_proj.scales\n",
      "model.layers.21.self_attn.q_proj.qzeros\n",
      "model.layers.21.self_attn.q_proj.scales\n",
      "model.layers.21.self_attn.v_proj.qzeros\n",
      "model.layers.21.self_attn.v_proj.scales\n",
      "model.layers.22.mlp.down_proj.qzeros\n",
      "model.layers.22.mlp.down_proj.scales\n",
      "model.layers.22.mlp.gate_proj.qzeros\n",
      "model.layers.22.mlp.gate_proj.scales\n",
      "model.layers.22.mlp.up_proj.qzeros\n",
      "model.layers.22.mlp.up_proj.scales\n",
      "model.layers.22.self_attn.k_proj.qzeros\n",
      "model.layers.22.self_attn.k_proj.scales\n",
      "model.layers.22.self_attn.o_proj.qzeros\n",
      "model.layers.22.self_attn.o_proj.scales\n",
      "model.layers.22.self_attn.q_proj.qzeros\n",
      "model.layers.22.self_attn.q_proj.scales\n",
      "model.layers.22.self_attn.v_proj.qzeros\n",
      "model.layers.22.self_attn.v_proj.scales\n",
      "model.layers.23.mlp.down_proj.qzeros\n",
      "model.layers.23.mlp.down_proj.scales\n",
      "model.layers.23.mlp.gate_proj.qzeros\n",
      "model.layers.23.mlp.gate_proj.scales\n",
      "model.layers.23.mlp.up_proj.qzeros\n",
      "model.layers.23.mlp.up_proj.scales\n",
      "model.layers.23.self_attn.k_proj.qzeros\n",
      "model.layers.23.self_attn.k_proj.scales\n",
      "model.layers.23.self_attn.o_proj.qzeros\n",
      "model.layers.23.self_attn.o_proj.scales\n",
      "model.layers.23.self_attn.q_proj.qzeros\n",
      "model.layers.23.self_attn.q_proj.scales\n",
      "model.layers.23.self_attn.v_proj.qzeros\n",
      "model.layers.23.self_attn.v_proj.scales\n",
      "model.layers.24.mlp.down_proj.qzeros\n",
      "model.layers.24.mlp.down_proj.scales\n",
      "model.layers.24.mlp.gate_proj.qzeros\n",
      "model.layers.24.mlp.gate_proj.scales\n",
      "model.layers.24.mlp.up_proj.qzeros\n",
      "model.layers.24.mlp.up_proj.scales\n",
      "model.layers.24.self_attn.k_proj.qzeros\n",
      "model.layers.24.self_attn.k_proj.scales\n",
      "model.layers.24.self_attn.o_proj.qzeros\n",
      "model.layers.24.self_attn.o_proj.scales\n",
      "model.layers.24.self_attn.q_proj.qzeros\n",
      "model.layers.24.self_attn.q_proj.scales\n",
      "model.layers.24.self_attn.v_proj.qzeros\n",
      "model.layers.24.self_attn.v_proj.scales\n",
      "model.layers.25.mlp.down_proj.qzeros\n",
      "model.layers.25.mlp.down_proj.scales\n",
      "model.layers.25.mlp.gate_proj.qzeros\n",
      "model.layers.25.mlp.gate_proj.scales\n",
      "model.layers.25.mlp.up_proj.qzeros\n",
      "model.layers.25.mlp.up_proj.scales\n",
      "model.layers.25.self_attn.k_proj.qzeros\n",
      "model.layers.25.self_attn.k_proj.scales\n",
      "model.layers.25.self_attn.o_proj.qzeros\n",
      "model.layers.25.self_attn.o_proj.scales\n",
      "model.layers.25.self_attn.q_proj.qzeros\n",
      "model.layers.25.self_attn.q_proj.scales\n",
      "model.layers.25.self_attn.v_proj.qzeros\n",
      "model.layers.25.self_attn.v_proj.scales\n",
      "model.layers.26.mlp.down_proj.qzeros\n",
      "model.layers.26.mlp.down_proj.scales\n",
      "model.layers.26.mlp.gate_proj.qzeros\n",
      "model.layers.26.mlp.gate_proj.scales\n",
      "model.layers.26.mlp.up_proj.qzeros\n",
      "model.layers.26.mlp.up_proj.scales\n",
      "model.layers.26.self_attn.k_proj.qzeros\n",
      "model.layers.26.self_attn.k_proj.scales\n",
      "model.layers.26.self_attn.o_proj.qzeros\n",
      "model.layers.26.self_attn.o_proj.scales\n",
      "model.layers.26.self_attn.q_proj.qzeros\n",
      "model.layers.26.self_attn.q_proj.scales\n",
      "model.layers.26.self_attn.v_proj.qzeros\n",
      "model.layers.26.self_attn.v_proj.scales\n",
      "model.layers.27.mlp.down_proj.qzeros\n",
      "model.layers.27.mlp.down_proj.scales\n",
      "model.layers.27.mlp.gate_proj.qzeros\n",
      "model.layers.27.mlp.gate_proj.scales\n",
      "model.layers.27.mlp.up_proj.qzeros\n",
      "model.layers.27.mlp.up_proj.scales\n",
      "model.layers.27.self_attn.k_proj.qzeros\n",
      "model.layers.27.self_attn.k_proj.scales\n",
      "model.layers.27.self_attn.o_proj.qzeros\n",
      "model.layers.27.self_attn.o_proj.scales\n",
      "model.layers.27.self_attn.q_proj.qzeros\n",
      "model.layers.27.self_attn.q_proj.scales\n",
      "model.layers.27.self_attn.v_proj.qzeros\n",
      "model.layers.27.self_attn.v_proj.scales\n",
      "model.layers.28.mlp.down_proj.qzeros\n",
      "model.layers.28.mlp.down_proj.scales\n",
      "model.layers.28.mlp.gate_proj.qzeros\n",
      "model.layers.28.mlp.gate_proj.scales\n",
      "model.layers.28.mlp.up_proj.qzeros\n",
      "model.layers.28.mlp.up_proj.scales\n",
      "model.layers.28.self_attn.k_proj.qzeros\n",
      "model.layers.28.self_attn.k_proj.scales\n",
      "model.layers.28.self_attn.o_proj.qzeros\n",
      "model.layers.28.self_attn.o_proj.scales\n",
      "model.layers.28.self_attn.q_proj.qzeros\n",
      "model.layers.28.self_attn.q_proj.scales\n",
      "model.layers.28.self_attn.v_proj.qzeros\n",
      "model.layers.28.self_attn.v_proj.scales\n",
      "model.layers.29.mlp.down_proj.qzeros\n",
      "model.layers.29.mlp.down_proj.scales\n",
      "model.layers.29.mlp.gate_proj.qzeros\n",
      "model.layers.29.mlp.gate_proj.scales\n",
      "model.layers.29.mlp.up_proj.qzeros\n",
      "model.layers.29.mlp.up_proj.scales\n",
      "model.layers.29.self_attn.k_proj.qzeros\n",
      "model.layers.29.self_attn.k_proj.scales\n",
      "model.layers.29.self_attn.o_proj.qzeros\n",
      "model.layers.29.self_attn.o_proj.scales\n",
      "model.layers.29.self_attn.q_proj.qzeros\n",
      "model.layers.29.self_attn.q_proj.scales\n",
      "model.layers.29.self_attn.v_proj.qzeros\n",
      "model.layers.29.self_attn.v_proj.scales\n",
      "model.layers.3.mlp.down_proj.qzeros\n",
      "model.layers.3.mlp.down_proj.scales\n",
      "model.layers.3.mlp.gate_proj.qzeros\n",
      "model.layers.3.mlp.gate_proj.scales\n",
      "model.layers.3.mlp.up_proj.qzeros\n",
      "model.layers.3.mlp.up_proj.scales\n",
      "model.layers.3.self_attn.k_proj.qzeros\n",
      "model.layers.3.self_attn.k_proj.scales\n",
      "model.layers.3.self_attn.o_proj.qzeros\n",
      "model.layers.3.self_attn.o_proj.scales\n",
      "model.layers.3.self_attn.q_proj.qzeros\n",
      "model.layers.3.self_attn.q_proj.scales\n",
      "model.layers.3.self_attn.v_proj.qzeros\n",
      "model.layers.3.self_attn.v_proj.scales\n",
      "model.layers.30.mlp.down_proj.qzeros\n",
      "model.layers.30.mlp.down_proj.scales\n",
      "model.layers.30.mlp.gate_proj.qzeros\n",
      "model.layers.30.mlp.gate_proj.scales\n",
      "model.layers.30.mlp.up_proj.qzeros\n",
      "model.layers.30.mlp.up_proj.scales\n",
      "model.layers.30.self_attn.k_proj.qzeros\n",
      "model.layers.30.self_attn.k_proj.scales\n",
      "model.layers.30.self_attn.o_proj.qzeros\n",
      "model.layers.30.self_attn.o_proj.scales\n",
      "model.layers.30.self_attn.q_proj.qzeros\n",
      "model.layers.30.self_attn.q_proj.scales\n",
      "model.layers.30.self_attn.v_proj.qzeros\n",
      "model.layers.30.self_attn.v_proj.scales\n",
      "model.layers.31.mlp.down_proj.qzeros\n",
      "model.layers.31.mlp.down_proj.scales\n",
      "model.layers.31.mlp.gate_proj.qzeros\n",
      "model.layers.31.mlp.gate_proj.scales\n",
      "model.layers.31.mlp.up_proj.qzeros\n",
      "model.layers.31.mlp.up_proj.scales\n",
      "model.layers.31.self_attn.k_proj.qzeros\n",
      "model.layers.31.self_attn.k_proj.scales\n",
      "model.layers.31.self_attn.o_proj.qzeros\n",
      "model.layers.31.self_attn.o_proj.scales\n",
      "model.layers.31.self_attn.q_proj.qzeros\n",
      "model.layers.31.self_attn.q_proj.scales\n",
      "model.layers.31.self_attn.v_proj.qzeros\n",
      "model.layers.31.self_attn.v_proj.scales\n",
      "model.layers.32.mlp.down_proj.qzeros\n",
      "model.layers.32.mlp.down_proj.scales\n",
      "model.layers.32.mlp.gate_proj.qzeros\n",
      "model.layers.32.mlp.gate_proj.scales\n",
      "model.layers.32.mlp.up_proj.qzeros\n",
      "model.layers.32.mlp.up_proj.scales\n",
      "model.layers.32.self_attn.k_proj.qzeros\n",
      "model.layers.32.self_attn.k_proj.scales\n",
      "model.layers.32.self_attn.o_proj.qzeros\n",
      "model.layers.32.self_attn.o_proj.scales\n",
      "model.layers.32.self_attn.q_proj.qzeros\n",
      "model.layers.32.self_attn.q_proj.scales\n",
      "model.layers.32.self_attn.v_proj.qzeros\n",
      "model.layers.32.self_attn.v_proj.scales\n",
      "model.layers.33.mlp.down_proj.qzeros\n",
      "model.layers.33.mlp.down_proj.scales\n",
      "model.layers.33.mlp.gate_proj.qzeros\n",
      "model.layers.33.mlp.gate_proj.scales\n",
      "model.layers.33.mlp.up_proj.qzeros\n",
      "model.layers.33.mlp.up_proj.scales\n",
      "model.layers.33.self_attn.k_proj.qzeros\n",
      "model.layers.33.self_attn.k_proj.scales\n",
      "model.layers.33.self_attn.o_proj.qzeros\n",
      "model.layers.33.self_attn.o_proj.scales\n",
      "model.layers.33.self_attn.q_proj.qzeros\n",
      "model.layers.33.self_attn.q_proj.scales\n",
      "model.layers.33.self_attn.v_proj.qzeros\n",
      "model.layers.33.self_attn.v_proj.scales\n",
      "model.layers.34.mlp.down_proj.qzeros\n",
      "model.layers.34.mlp.down_proj.scales\n",
      "model.layers.34.mlp.gate_proj.qzeros\n",
      "model.layers.34.mlp.gate_proj.scales\n",
      "model.layers.34.mlp.up_proj.qzeros\n",
      "model.layers.34.mlp.up_proj.scales\n",
      "model.layers.34.self_attn.k_proj.qzeros\n",
      "model.layers.34.self_attn.k_proj.scales\n",
      "model.layers.34.self_attn.o_proj.qzeros\n",
      "model.layers.34.self_attn.o_proj.scales\n",
      "model.layers.34.self_attn.q_proj.qzeros\n",
      "model.layers.34.self_attn.q_proj.scales\n",
      "model.layers.34.self_attn.v_proj.qzeros\n",
      "model.layers.34.self_attn.v_proj.scales\n",
      "model.layers.35.mlp.down_proj.qzeros\n",
      "model.layers.35.mlp.down_proj.scales\n",
      "model.layers.35.mlp.gate_proj.qzeros\n",
      "model.layers.35.mlp.gate_proj.scales\n",
      "model.layers.35.mlp.up_proj.qzeros\n",
      "model.layers.35.mlp.up_proj.scales\n",
      "model.layers.35.self_attn.k_proj.qzeros\n",
      "model.layers.35.self_attn.k_proj.scales\n",
      "model.layers.35.self_attn.o_proj.qzeros\n",
      "model.layers.35.self_attn.o_proj.scales\n",
      "model.layers.35.self_attn.q_proj.qzeros\n",
      "model.layers.35.self_attn.q_proj.scales\n",
      "model.layers.35.self_attn.v_proj.qzeros\n",
      "model.layers.35.self_attn.v_proj.scales\n",
      "model.layers.36.mlp.down_proj.qzeros\n",
      "model.layers.36.mlp.down_proj.scales\n",
      "model.layers.36.mlp.gate_proj.qzeros\n",
      "model.layers.36.mlp.gate_proj.scales\n",
      "model.layers.36.mlp.up_proj.qzeros\n",
      "model.layers.36.mlp.up_proj.scales\n",
      "model.layers.36.self_attn.k_proj.qzeros\n",
      "model.layers.36.self_attn.k_proj.scales\n",
      "model.layers.36.self_attn.o_proj.qzeros\n",
      "model.layers.36.self_attn.o_proj.scales\n",
      "model.layers.36.self_attn.q_proj.qzeros\n",
      "model.layers.36.self_attn.q_proj.scales\n",
      "model.layers.36.self_attn.v_proj.qzeros\n",
      "model.layers.36.self_attn.v_proj.scales\n",
      "model.layers.37.mlp.down_proj.qzeros\n",
      "model.layers.37.mlp.down_proj.scales\n",
      "model.layers.37.mlp.gate_proj.qzeros\n",
      "model.layers.37.mlp.gate_proj.scales\n",
      "model.layers.37.mlp.up_proj.qzeros\n",
      "model.layers.37.mlp.up_proj.scales\n",
      "model.layers.37.self_attn.k_proj.qzeros\n",
      "model.layers.37.self_attn.k_proj.scales\n",
      "model.layers.37.self_attn.o_proj.qzeros\n",
      "model.layers.37.self_attn.o_proj.scales\n",
      "model.layers.37.self_attn.q_proj.qzeros\n",
      "model.layers.37.self_attn.q_proj.scales\n",
      "model.layers.37.self_attn.v_proj.qzeros\n",
      "model.layers.37.self_attn.v_proj.scales\n",
      "model.layers.38.mlp.down_proj.qzeros\n",
      "model.layers.38.mlp.down_proj.scales\n",
      "model.layers.38.mlp.gate_proj.qzeros\n",
      "model.layers.38.mlp.gate_proj.scales\n",
      "model.layers.38.mlp.up_proj.qzeros\n",
      "model.layers.38.mlp.up_proj.scales\n",
      "model.layers.38.self_attn.k_proj.qzeros\n",
      "model.layers.38.self_attn.k_proj.scales\n",
      "model.layers.38.self_attn.o_proj.qzeros\n",
      "model.layers.38.self_attn.o_proj.scales\n",
      "model.layers.38.self_attn.q_proj.qzeros\n",
      "model.layers.38.self_attn.q_proj.scales\n",
      "model.layers.38.self_attn.v_proj.qzeros\n",
      "model.layers.38.self_attn.v_proj.scales\n",
      "model.layers.39.mlp.down_proj.qzeros\n",
      "model.layers.39.mlp.down_proj.scales\n",
      "model.layers.39.mlp.gate_proj.qzeros\n",
      "model.layers.39.mlp.gate_proj.scales\n",
      "model.layers.39.mlp.up_proj.qzeros\n",
      "model.layers.39.mlp.up_proj.scales\n",
      "model.layers.39.self_attn.k_proj.qzeros\n",
      "model.layers.39.self_attn.k_proj.scales\n",
      "model.layers.39.self_attn.o_proj.qzeros\n",
      "model.layers.39.self_attn.o_proj.scales\n",
      "model.layers.39.self_attn.q_proj.qzeros\n",
      "model.layers.39.self_attn.q_proj.scales\n",
      "model.layers.39.self_attn.v_proj.qzeros\n",
      "model.layers.39.self_attn.v_proj.scales\n",
      "model.layers.4.mlp.down_proj.qzeros\n",
      "model.layers.4.mlp.down_proj.scales\n",
      "model.layers.4.mlp.gate_proj.qzeros\n",
      "model.layers.4.mlp.gate_proj.scales\n",
      "model.layers.4.mlp.up_proj.qzeros\n",
      "model.layers.4.mlp.up_proj.scales\n",
      "model.layers.4.self_attn.k_proj.qzeros\n",
      "model.layers.4.self_attn.k_proj.scales\n",
      "model.layers.4.self_attn.o_proj.qzeros\n",
      "model.layers.4.self_attn.o_proj.scales\n",
      "model.layers.4.self_attn.q_proj.qzeros\n",
      "model.layers.4.self_attn.q_proj.scales\n",
      "model.layers.4.self_attn.v_proj.qzeros\n",
      "model.layers.4.self_attn.v_proj.scales\n",
      "model.layers.5.mlp.down_proj.qzeros\n",
      "model.layers.5.mlp.down_proj.scales\n",
      "model.layers.5.mlp.gate_proj.qzeros\n",
      "model.layers.5.mlp.gate_proj.scales\n",
      "model.layers.5.mlp.up_proj.qzeros\n",
      "model.layers.5.mlp.up_proj.scales\n",
      "model.layers.5.self_attn.k_proj.qzeros\n",
      "model.layers.5.self_attn.k_proj.scales\n",
      "model.layers.5.self_attn.o_proj.qzeros\n",
      "model.layers.5.self_attn.o_proj.scales\n",
      "model.layers.5.self_attn.q_proj.qzeros\n",
      "model.layers.5.self_attn.q_proj.scales\n",
      "model.layers.5.self_attn.v_proj.qzeros\n",
      "model.layers.5.self_attn.v_proj.scales\n",
      "model.layers.6.mlp.down_proj.qzeros\n",
      "model.layers.6.mlp.down_proj.scales\n",
      "model.layers.6.mlp.gate_proj.qzeros\n",
      "model.layers.6.mlp.gate_proj.scales\n",
      "model.layers.6.mlp.up_proj.qzeros\n",
      "model.layers.6.mlp.up_proj.scales\n",
      "model.layers.6.self_attn.k_proj.qzeros\n",
      "model.layers.6.self_attn.k_proj.scales\n",
      "model.layers.6.self_attn.o_proj.qzeros\n",
      "model.layers.6.self_attn.o_proj.scales\n",
      "model.layers.6.self_attn.q_proj.qzeros\n",
      "model.layers.6.self_attn.q_proj.scales\n",
      "model.layers.6.self_attn.v_proj.qzeros\n",
      "model.layers.6.self_attn.v_proj.scales\n",
      "model.layers.7.mlp.down_proj.qzeros\n",
      "model.layers.7.mlp.down_proj.scales\n",
      "model.layers.7.mlp.gate_proj.qzeros\n",
      "model.layers.7.mlp.gate_proj.scales\n",
      "model.layers.7.mlp.up_proj.qzeros\n",
      "model.layers.7.mlp.up_proj.scales\n",
      "model.layers.7.self_attn.k_proj.qzeros\n",
      "model.layers.7.self_attn.k_proj.scales\n",
      "model.layers.7.self_attn.o_proj.qzeros\n",
      "model.layers.7.self_attn.o_proj.scales\n",
      "model.layers.7.self_attn.q_proj.qzeros\n",
      "model.layers.7.self_attn.q_proj.scales\n",
      "model.layers.7.self_attn.v_proj.qzeros\n",
      "model.layers.7.self_attn.v_proj.scales\n",
      "model.layers.8.mlp.down_proj.qzeros\n",
      "model.layers.8.mlp.down_proj.scales\n",
      "model.layers.8.mlp.gate_proj.qzeros\n",
      "model.layers.8.mlp.gate_proj.scales\n",
      "model.layers.8.mlp.up_proj.qzeros\n",
      "model.layers.8.mlp.up_proj.scales\n",
      "model.layers.8.self_attn.k_proj.qzeros\n",
      "model.layers.8.self_attn.k_proj.scales\n",
      "model.layers.8.self_attn.o_proj.qzeros\n",
      "model.layers.8.self_attn.o_proj.scales\n",
      "model.layers.8.self_attn.q_proj.qzeros\n",
      "model.layers.8.self_attn.q_proj.scales\n",
      "model.layers.8.self_attn.v_proj.qzeros\n",
      "model.layers.8.self_attn.v_proj.scales\n",
      "model.layers.9.mlp.down_proj.qzeros\n",
      "model.layers.9.mlp.down_proj.scales\n",
      "model.layers.9.mlp.gate_proj.qzeros\n",
      "model.layers.9.mlp.gate_proj.scales\n",
      "model.layers.9.mlp.up_proj.qzeros\n",
      "model.layers.9.mlp.up_proj.scales\n",
      "model.layers.9.self_attn.k_proj.qzeros\n",
      "model.layers.9.self_attn.k_proj.scales\n",
      "model.layers.9.self_attn.o_proj.qzeros\n",
      "model.layers.9.self_attn.o_proj.scales\n",
      "model.layers.9.self_attn.q_proj.qzeros\n",
      "model.layers.9.self_attn.q_proj.scales\n",
      "model.layers.9.self_attn.v_proj.qzeros\n",
      "model.layers.9.self_attn.v_proj.scales\n"
     ]
    }
   ],
   "source": [
    "for name, loaded_weight in weights_iterator: \n",
    "    if 'scales' in name or 'zeros' in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9098d1-2374-4060-b8fe-60052c04110e",
   "metadata": {},
   "source": [
    "### Create Quantized Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7cfc1da-2147-4722-a766-ee216a29ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, json\n",
    "from safetensors.torch import save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7d1dd78-d696-4477-b878-a13a763c270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path(\"/home/ubuntu/models/llama-7b-hf-nf4-quantized\")\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c935e5-e5a6-4c64-8bfd-168e07352d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14571164abf44dda94ba3e361c0968b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a8cb2-6eff-4645-bcff-c4bf66238802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original quantized layers from fsdp_qlora/train.py\n",
    "# [\"k_proj\", \"q_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afaa582c-95b9-4e22-a209-485cfb73f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_layers = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eaef503-678f-4084-b3f7-faa0af7f6d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_state_dict = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3686d0a-497a-4486-bf84-169dc6924ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pack_factor = 2\n",
    "blocksize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "776a5270-682e-441b-aa58-6ec8e92cfb0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    }
   ],
   "source": [
    "for n,p in model.state_dict().items():\n",
    "    if any(l in n for l in quantized_layers) and \"weight\" in n:\n",
    "        # output_size x input_size\n",
    "        print(n, p.shape, p.t().shape)\n",
    "        param = Params4bit(p.t(), quant_type=\"nf4\", blocksize=blocksize, compress_statistics=False, quant_storage=torch.uint8)\n",
    "        input_size, output_size = p.t().shape\n",
    "        param.cuda();\n",
    "\n",
    "        # reshape for tensor parallelism\n",
    "        qweight, absmax = param.data.cpu(), param.quant_state.absmax.cpu()        \n",
    "        qweight = qweight.reshape(input_size, output_size // pack_factor)\n",
    "        absmax = absmax.reshape(-1, output_size // blocksize)\n",
    "                \n",
    "        quantized_state_dict[n] = qweight\n",
    "        quantized_state_dict[n.replace(\".weight\", \".absmax\")] = absmax\n",
    "\n",
    "        param = None\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07d917df-7614-41b5-bac7-c85fb73c9ef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save quantized weights\n",
    "save_file(quantized_state_dict, model_dir/\"model_state_dict.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4857052b-6de3-44ca-be4a-b36e77a71817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save quantization config\n",
    "quant_config_filename = model_dir/\"quantize_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6130b09-c496-40b5-b022-fbd6237c994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config_dict = {\n",
    "    \"weight_bits\" : 4,\n",
    "    \"blocksize\" : 64,\n",
    "    \"quant_type\" : \"nf4\",\n",
    "    \"quant_storage\" : \"uint8\",\n",
    "    \"compress_statistics\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "772b7e27-53ec-4a22-8970-d2a455e580f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(quant_config_filename, \"w+\") as f: json.dump(quant_config_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6324272-3832-45bb-95da-f98a992937fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10e9d27d-c2d9-48e5-8e75-3f1ddb1b2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model config\n",
    "model_config_filename = model_dir/\"config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "863d3eb1-d461-4c6c-9c5b-df04a7addb8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(model_config_filename, \"w+\") as f: json.dump(model_config.to_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a28ac0-9d96-46a6-a0e8-a28211728cb4",
   "metadata": {},
   "source": [
    "### BNB Quantized VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5951483a-d5d6-4c82-bab0-c07d868f8c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c509ba59-3540-4947-b9ea-5783fd6c0358",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = model_dir = \"/home/ubuntu/models/llama-7b-hf-nf4-quantized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a0b76b-96c4-404e-ade5-29002536bb6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-29 14:30:46 config.py:744] Casting torch.float16 to torch.bfloat16.\n",
      "WARNING 03-29 14:30:46 config.py:208] bnb quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 03-29 14:30:46 llm_engine.py:70] Initializing an LLM engine (v0.3.3) with config: model='/home/ubuntu/models/llama-7b-hf-nf4-quantized', tokenizer='meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=bnb, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 03-29 14:30:47 pynccl_utils.py:13] vLLM is using nccl==2.18.1\n",
      "INFO 03-29 14:30:48 selector.py:44] flash_attn is not found.\n",
      "INFO 03-29 14:30:48 selector.py:20] Using XFormers backend.\n",
      "INFO 03-29 14:30:51 model_runner.py:104] Loading model weights took 3.9585 GB\n",
      "INFO 03-29 14:30:53 gpu_executor.py:94] # GPU blocks: 2087, # CPU blocks: 512\n",
      "INFO 03-29 14:30:55 model_runner.py:770] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-29 14:30:55 model_runner.py:774] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "split_with_sizes expects split_sizes to sum exactly to 4096 (input tensor's size at dimension -1), but got split_sizes=[4096, 4096, 4096]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-2-7b-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbfloat16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbnb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/vllm-fork/vllm/entrypoints/llm.py:111\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     93\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m     94\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     95\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    110\u001b[0m )\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/git/vllm-fork/vllm/engine/llm_engine.py:150\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args)\u001b[0m\n\u001b[1;32m    147\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m GPUExecutor\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m             \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m             \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/git/vllm-fork/vllm/engine/llm_engine.py:106\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, lora_config, vision_language_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenizer \u001b[38;5;241m=\u001b[39m Detokenizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mvision_language_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Ping the tokenizer to ensure liveness if it runs in a\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# different process.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mping()\n",
      "File \u001b[0;32m~/git/vllm-fork/vllm/executor/gpu_executor.py:40\u001b[0m, in \u001b[0;36mGPUExecutor.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, lora_config, vision_language_config)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_worker()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Profile the memory usage and initialize the cache.\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/vllm-fork/vllm/executor/gpu_executor.py:107\u001b[0m, in \u001b[0;36mGPUExecutor._init_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker\u001b[38;5;241m.\u001b[39minit_cache_engine(cache_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Warm up the model. This includes capturing the model into CUDA graph\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# if enforce_eager is False.\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarm_up_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/vllm-fork/vllm/worker/worker.py:166\u001b[0m, in \u001b[0;36mWorker.warm_up_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwarm_up_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39menforce_eager:\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapture_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Reset the seed to ensure that the random state is not affected by\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# the model initialization and profiling.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     set_random_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mseed)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/vllm-fork/vllm/worker/model_runner.py:833\u001b[0m, in \u001b[0;36mModelRunner.capture_model\u001b[0;34m(self, kv_caches)\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_active_loras(\u001b[38;5;28mset\u001b[39m(), lora_mapping)\n\u001b[1;32m    832\u001b[0m graph_runner \u001b[38;5;241m=\u001b[39m CUDAGraphRunner(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 833\u001b[0m \u001b[43mgraph_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapture\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_positions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_memory_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_memory_pool \u001b[38;5;241m=\u001b[39m graph_runner\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mpool()\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_runners[batch_size] \u001b[38;5;241m=\u001b[39m graph_runner\n",
      "File \u001b[0;32m~/git/vllm-fork/vllm/worker/model_runner.py:885\u001b[0m, in \u001b[0;36mCUDAGraphRunner.capture\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, memory_pool, **kwargs)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;66;03m# Run the model once without capturing the graph.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;66;03m# This is to make sure that the captured graph does not include the\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;66;03m# kernel launches for initial benchmarking (e.g., Triton autotune).\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _maybe_pynccl():\n\u001b[0;32m--> 885\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# Capture the graph.\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;66;03m# NOTE(woosuk): Python 3.8 does not support multi-line with statements.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;66;03m# https://stackoverflow.com/questions/31039022/python-multi-line-with-statement\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/vllm-fork/vllm/model_executor/models/llama.py:345\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    340\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m     attn_metadata: AttentionMetadata,\n\u001b[1;32m    344\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 345\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/vllm-fork/vllm/model_executor/models/llama.py:271\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, inputs_embeds)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)):\n\u001b[1;32m    270\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\n\u001b[0;32m--> 271\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states, residual)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/vllm-fork/vllm/model_executor/models/llama.py:213\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata, residual)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(\n\u001b[1;32m    212\u001b[0m         hidden_states, residual)\n\u001b[0;32m--> 213\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    221\u001b[0m hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(\n\u001b[1;32m    222\u001b[0m     hidden_states, residual)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/vllm-fork/vllm/model_executor/models/llama.py:154\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    148\u001b[0m     positions: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m     attn_metadata: AttentionMetadata,\n\u001b[1;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    153\u001b[0m     qkv, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv_proj(hidden_states)\n\u001b[0;32m--> 154\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43mqkv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(positions, q, k)\n\u001b[1;32m    156\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(q, k, v, kv_cache, attn_metadata)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor.py:864\u001b[0m, in \u001b[0;36mTensor.split\u001b[0;34m(self, split_size, dim)\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_VF\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m, split_size, dim)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 864\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_with_sizes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: split_with_sizes expects split_sizes to sum exactly to 4096 (input tensor's size at dimension -1), but got split_sizes=[4096, 4096, 4096]"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_dir, tokenizer=\"meta-llama/Llama-2-7b-hf\", dtype=\"bfloat16\", quantization=\"bnb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2bc83-f56a-49fc-9e7e-524212ad5aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2dd0ae-ef59-48df-96e1-57e23eb67bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a32c36-08e2-4135-9f1d-d77b71f041c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccffeb4c-f474-41b1-86a1-e4e9a6e67fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258bca3-acae-4213-b12b-d53b6a201751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5694bb7-06bf-4b63-9ddf-f633e3a3f690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279b99cc-49cf-4b7e-a5c7-95c7f98bcba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
