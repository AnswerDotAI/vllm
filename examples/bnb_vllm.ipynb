{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22d8c79-3add-4d49-9cae-e212b493467e",
   "metadata": {},
   "source": [
    "### BNB with Tensor Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff26655-6d13-479a-9af8-9d64083ec9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from bitsandbytes.nn.modules import Params4bit, Linear4bit\n",
    "from bitsandbytes.functional import dequantize_4bit\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.functional import dequantize_4bit, QuantState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c906c609-2777-4148-acac-700d72842b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea59055-c341-47c3-86b9-f48415d239d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocksize = 64\n",
    "quant_type = \"nf4\"\n",
    "quant_storage = torch.uint8\n",
    "\n",
    "data = torch.randn(128,256).to(torch.bfloat16)\n",
    "param = Params4bit(data, blocksize=blocksize, quant_type=quant_type, \n",
    "                   quant_storage=quant_storage, compress_statistics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b7929ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if quant_storage == torch.uint8:\n",
    "    pack_factor = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6890d631-c63f-4939-a5f4-00883a324aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Parameter(Params4bit([[-0.1260, -1.6250,  0.0508,  ...,  0.3379, -1.7500,  1.1484],\n",
       "            [-0.2305, -1.3047, -2.0312,  ..., -0.1328, -0.3125,  1.8594],\n",
       "            [-1.1875,  0.9766,  0.0840,  ..., -2.2812,  0.6016,  1.1328],\n",
       "            ...,\n",
       "            [ 1.1875,  1.0234,  1.3281,  ...,  1.5781, -0.7227,  0.5156],\n",
       "            [ 0.0610, -0.3477,  1.2578,  ..., -1.7109,  1.0781,  0.6914],\n",
       "            [ 0.2002, -0.7422,  0.0527,  ...,  1.1953, -0.0952,  0.0854]],\n",
       "           dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "222fae9b-6520-46de-bd43-21867026cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "920862c7-0c28-43cf-ba29-5a624eadaace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16384, 1]), 'nf4', torch.uint8, 64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.shape, param.quant_type, param.quant_storage, param.blocksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78e79d3e-ca1c-49e8-8da1-8578b346d2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a273b007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.numel() / data.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a3b663b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_type': 'nf4',\n",
       " 'absmax': tensor([2.6406, 2.4062, 3.7812, 2.2344, 2.7500, 3.2500, 2.2500, 2.8594, 2.7656,\n",
       "         2.4688, 3.2969, 3.4844, 2.7812, 2.2188, 2.9375, 3.2812, 2.1094, 2.8125,\n",
       "         2.2344, 3.2344, 3.2188, 2.8125, 2.3906, 2.2812, 2.7031, 3.5938, 2.5156,\n",
       "         2.9219, 2.1875, 3.0469, 2.5781, 2.5781, 2.2188, 2.8125, 2.3281, 2.6094,\n",
       "         2.9219, 2.7344, 2.1719, 3.4375, 2.5000, 3.0625, 2.4688, 2.5625, 2.9531,\n",
       "         2.8125, 2.7969, 2.2500, 3.6094, 2.4062, 2.0781, 2.4531, 3.3281, 2.6250,\n",
       "         3.1250, 2.7969, 2.8594, 2.4844, 2.8906, 2.5000, 2.1719, 3.1406, 2.6094,\n",
       "         2.4375, 3.0000, 2.2188, 3.4062, 3.9688, 2.6250, 2.5938, 2.2500, 3.0000,\n",
       "         2.7188, 2.5625, 2.5156, 3.3750, 2.5469, 2.8125, 3.0781, 2.4688, 2.5781,\n",
       "         3.3281, 2.5781, 1.7109, 2.9375, 2.0781, 2.2031, 2.3750, 2.0312, 2.3750,\n",
       "         2.4688, 2.6406, 2.2812, 2.2344, 3.2031, 2.5312, 1.8203, 2.3438, 2.0625,\n",
       "         2.7500, 2.5312, 2.6094, 2.4844, 2.7188, 2.6562, 2.1094, 1.8594, 2.4688,\n",
       "         2.2188, 3.0312, 3.4062, 2.2031, 2.2188, 2.0000, 2.8438, 2.5156, 2.3594,\n",
       "         1.9844, 2.9531, 2.7344, 2.8750, 2.0156, 2.1406, 2.9844, 3.0312, 2.7344,\n",
       "         2.2656, 2.5938, 3.5625, 2.8750, 1.8672, 2.4531, 1.8672, 2.8281, 2.2500,\n",
       "         2.0312, 2.9375, 2.1719, 1.9531, 2.0938, 2.5625, 3.2812, 2.4375, 2.8750,\n",
       "         2.9062, 2.0469, 2.7188, 2.5000, 2.7500, 2.1094, 1.7344, 2.3750, 2.8281,\n",
       "         3.1250, 3.2344, 2.8906, 2.2344, 1.9844, 4.2812, 2.6562, 2.5938, 2.3750,\n",
       "         2.8750, 2.1719, 3.1250, 2.9219, 3.5469, 2.4844, 2.6250, 1.9922, 2.3125,\n",
       "         2.4844, 2.7656, 2.3281, 2.7969, 3.2656, 2.2188, 2.1875, 2.2500, 2.3906,\n",
       "         2.2344, 2.5156, 3.3125, 1.9922, 1.7344, 2.6406, 2.1406, 2.5000, 2.1562,\n",
       "         2.5469, 2.1875, 1.9219, 1.9609, 2.4219, 2.8438, 2.0938, 2.9062, 1.8750,\n",
       "         2.6719, 2.5312, 3.2969, 2.6250, 2.2812, 2.4219, 2.1875, 2.6719, 3.2656,\n",
       "         2.3906, 1.8281, 2.4844, 2.8438, 2.5156, 2.6562, 2.4219, 2.0000, 2.8906,\n",
       "         2.5312, 2.7188, 2.7500, 3.4375, 2.6094, 2.7500, 1.7734, 2.1250, 2.4531,\n",
       "         2.4688, 2.7812, 2.5469, 2.0312, 1.8906, 3.6406, 2.3281, 2.6094, 2.5938,\n",
       "         2.1562, 2.4531, 2.1719, 2.1406, 2.2969, 2.1406, 3.3906, 2.3594, 2.2812,\n",
       "         2.6250, 2.7812, 2.5938, 2.6562, 2.6250, 2.3750, 3.3750, 2.2500, 2.1719,\n",
       "         2.8750, 2.1719, 2.4375, 2.4062, 2.5469, 2.9688, 2.1719, 1.8906, 2.2812,\n",
       "         2.2969, 2.6875, 2.7656, 3.5312, 2.4844, 2.2812, 3.7656, 3.4375, 2.9531,\n",
       "         2.2031, 2.7812, 3.0781, 2.5625, 2.1875, 2.1562, 3.2344, 2.3438, 2.2969,\n",
       "         2.1875, 2.4844, 2.6719, 2.1562, 2.2969, 2.7812, 3.2969, 1.9688, 2.1719,\n",
       "         2.9219, 1.9609, 3.2969, 3.0312, 2.6562, 2.8438, 2.7656, 2.5156, 2.5312,\n",
       "         2.4375, 2.8750, 3.1250, 2.0938, 2.5312, 2.4062, 2.5938, 3.1406, 2.2812,\n",
       "         2.7188, 3.2812, 3.0781, 2.8750, 2.3906, 2.2344, 2.4062, 1.9922, 3.5312,\n",
       "         2.6250, 2.0625, 2.9375, 2.1875, 2.4062, 2.7500, 2.7188, 2.1250, 2.3125,\n",
       "         3.2656, 2.3281, 2.9375, 2.6719, 2.2188, 2.4844, 2.5781, 2.4375, 3.7031,\n",
       "         3.4844, 2.7188, 2.4688, 2.2969, 2.9219, 3.0000, 1.9219, 2.0312, 2.8125,\n",
       "         1.9766, 3.5938, 2.9844, 2.9219, 2.0938, 2.2188, 2.7188, 2.6094, 2.6406,\n",
       "         2.6562, 2.4688, 2.0156, 3.0156, 3.0625, 2.1250, 2.7188, 2.5000, 2.9531,\n",
       "         2.7188, 2.4375, 2.7031, 2.7188, 2.8594, 2.6719, 2.0156, 2.3281, 2.8750,\n",
       "         2.1562, 2.5312, 2.9062, 2.1250, 2.5938, 2.4688, 2.7812, 2.9219, 2.2188,\n",
       "         1.8516, 2.5938, 3.1406, 2.6719, 1.7656, 2.6250, 2.0156, 2.6875, 2.2500,\n",
       "         3.1719, 2.5000, 3.1250, 1.6875, 3.0000, 2.0625, 2.7500, 2.9844, 2.4844,\n",
       "         2.9375, 3.5000, 1.8906, 2.4219, 2.1406, 3.0312, 2.8125, 1.9922, 2.1562,\n",
       "         1.7656, 2.3281, 2.6094, 2.4531, 2.7500, 2.5156, 2.3281, 3.1875, 1.6953,\n",
       "         2.4219, 3.2812, 2.3438, 2.3281, 3.0781, 2.5000, 2.6719, 2.2812, 1.8281,\n",
       "         2.0781, 2.7656, 2.8438, 2.3594, 2.1875, 1.9297, 2.6562, 2.7344, 2.3594,\n",
       "         2.9062, 2.2969, 3.0469, 2.9531, 2.8281, 3.0625, 2.3438, 2.0000, 2.9219,\n",
       "         3.1875, 2.3906, 2.7344, 2.1719, 2.9219, 2.3906, 3.0469, 2.0938, 2.2031,\n",
       "         2.8906, 2.5156, 2.3125, 1.7734, 3.5156, 2.4688, 3.5000, 2.2969, 2.3750,\n",
       "         2.2188, 2.5625, 2.4375, 2.1250, 2.8750, 2.7031, 2.8594, 2.3125, 3.4531,\n",
       "         3.0938, 2.6094, 2.7656, 2.9062, 2.4375, 2.7031, 2.5781, 2.6094, 2.2812,\n",
       "         2.9375, 2.2031, 3.0156, 2.2812, 2.5625, 3.1406, 2.4062, 1.9062, 2.8438,\n",
       "         2.4375, 2.5156, 2.7031, 2.1406, 3.0469, 2.5469, 2.0469, 2.4062, 2.2344,\n",
       "         2.2188, 2.3594, 2.3438, 1.7656, 1.8984, 2.7656, 2.0938, 2.8125, 2.5625,\n",
       "         2.0781, 2.5312, 3.1562, 2.5000, 2.3438, 2.4062, 2.9375, 2.5312],\n",
       "        device='cuda:0'),\n",
       " 'blocksize': 64,\n",
       " 'quant_map': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
       "          0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
       "        device='cuda:0'),\n",
       " 'dtype': 'bfloat16',\n",
       " 'shape': (128, 256)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.quant_state.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b110f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size_per_partition = 64\n",
    "output_size_per_partition = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c5479f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row-major quantization, reshape for vllm tensor parallelism\n",
    "qweight = param.data.reshape(data.size(0), data.size(1) // pack_factor); qweight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd34c2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 97, 119,  55,  ...,  65, 201,  29],\n",
       "        [ 98,  24,  97,  ..., 156, 134, 110],\n",
       "        [ 59, 112,  71,  ...,  58,  81, 155],\n",
       "        ...,\n",
       "        [203, 201, 227,  ...,  20, 173,  73],\n",
       "        [117, 217, 181,  ...,  65,  97, 202],\n",
       "        [132, 122, 134,  ...,  38,  44, 119]], device='cuda:0',\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b3a5601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 97],\n",
       "        [119],\n",
       "        [ 55],\n",
       "        ...,\n",
       "        [ 38],\n",
       "        [ 44],\n",
       "        [119]], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qweight.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11e42bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deqweight = dequantize_4bit(qweight.view(-1,1), param.quant_state, blocksize=blocksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e794c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(254., dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data - torch.randn_like(data)).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4c3eb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.5000, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data - deqweight.cpu()).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c66129c-c120-4b67-bf0c-fee3d8e94a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 128).cuda().to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed100e7d-5097-4682-9a6d-333f4434c877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_type': 'nf4',\n",
       " 'absmax': tensor([2.6406, 2.4062, 3.7812, 2.2344, 2.7500, 3.2500, 2.2500, 2.8594, 2.7656,\n",
       "         2.4688, 3.2969, 3.4844, 2.7812, 2.2188, 2.9375, 3.2812, 2.1094, 2.8125,\n",
       "         2.2344, 3.2344, 3.2188, 2.8125, 2.3906, 2.2812, 2.7031, 3.5938, 2.5156,\n",
       "         2.9219, 2.1875, 3.0469, 2.5781, 2.5781, 2.2188, 2.8125, 2.3281, 2.6094,\n",
       "         2.9219, 2.7344, 2.1719, 3.4375, 2.5000, 3.0625, 2.4688, 2.5625, 2.9531,\n",
       "         2.8125, 2.7969, 2.2500, 3.6094, 2.4062, 2.0781, 2.4531, 3.3281, 2.6250,\n",
       "         3.1250, 2.7969, 2.8594, 2.4844, 2.8906, 2.5000, 2.1719, 3.1406, 2.6094,\n",
       "         2.4375, 3.0000, 2.2188, 3.4062, 3.9688, 2.6250, 2.5938, 2.2500, 3.0000,\n",
       "         2.7188, 2.5625, 2.5156, 3.3750, 2.5469, 2.8125, 3.0781, 2.4688, 2.5781,\n",
       "         3.3281, 2.5781, 1.7109, 2.9375, 2.0781, 2.2031, 2.3750, 2.0312, 2.3750,\n",
       "         2.4688, 2.6406, 2.2812, 2.2344, 3.2031, 2.5312, 1.8203, 2.3438, 2.0625,\n",
       "         2.7500, 2.5312, 2.6094, 2.4844, 2.7188, 2.6562, 2.1094, 1.8594, 2.4688,\n",
       "         2.2188, 3.0312, 3.4062, 2.2031, 2.2188, 2.0000, 2.8438, 2.5156, 2.3594,\n",
       "         1.9844, 2.9531, 2.7344, 2.8750, 2.0156, 2.1406, 2.9844, 3.0312, 2.7344,\n",
       "         2.2656, 2.5938, 3.5625, 2.8750, 1.8672, 2.4531, 1.8672, 2.8281, 2.2500,\n",
       "         2.0312, 2.9375, 2.1719, 1.9531, 2.0938, 2.5625, 3.2812, 2.4375, 2.8750,\n",
       "         2.9062, 2.0469, 2.7188, 2.5000, 2.7500, 2.1094, 1.7344, 2.3750, 2.8281,\n",
       "         3.1250, 3.2344, 2.8906, 2.2344, 1.9844, 4.2812, 2.6562, 2.5938, 2.3750,\n",
       "         2.8750, 2.1719, 3.1250, 2.9219, 3.5469, 2.4844, 2.6250, 1.9922, 2.3125,\n",
       "         2.4844, 2.7656, 2.3281, 2.7969, 3.2656, 2.2188, 2.1875, 2.2500, 2.3906,\n",
       "         2.2344, 2.5156, 3.3125, 1.9922, 1.7344, 2.6406, 2.1406, 2.5000, 2.1562,\n",
       "         2.5469, 2.1875, 1.9219, 1.9609, 2.4219, 2.8438, 2.0938, 2.9062, 1.8750,\n",
       "         2.6719, 2.5312, 3.2969, 2.6250, 2.2812, 2.4219, 2.1875, 2.6719, 3.2656,\n",
       "         2.3906, 1.8281, 2.4844, 2.8438, 2.5156, 2.6562, 2.4219, 2.0000, 2.8906,\n",
       "         2.5312, 2.7188, 2.7500, 3.4375, 2.6094, 2.7500, 1.7734, 2.1250, 2.4531,\n",
       "         2.4688, 2.7812, 2.5469, 2.0312, 1.8906, 3.6406, 2.3281, 2.6094, 2.5938,\n",
       "         2.1562, 2.4531, 2.1719, 2.1406, 2.2969, 2.1406, 3.3906, 2.3594, 2.2812,\n",
       "         2.6250, 2.7812, 2.5938, 2.6562, 2.6250, 2.3750, 3.3750, 2.2500, 2.1719,\n",
       "         2.8750, 2.1719, 2.4375, 2.4062, 2.5469, 2.9688, 2.1719, 1.8906, 2.2812,\n",
       "         2.2969, 2.6875, 2.7656, 3.5312, 2.4844, 2.2812, 3.7656, 3.4375, 2.9531,\n",
       "         2.2031, 2.7812, 3.0781, 2.5625, 2.1875, 2.1562, 3.2344, 2.3438, 2.2969,\n",
       "         2.1875, 2.4844, 2.6719, 2.1562, 2.2969, 2.7812, 3.2969, 1.9688, 2.1719,\n",
       "         2.9219, 1.9609, 3.2969, 3.0312, 2.6562, 2.8438, 2.7656, 2.5156, 2.5312,\n",
       "         2.4375, 2.8750, 3.1250, 2.0938, 2.5312, 2.4062, 2.5938, 3.1406, 2.2812,\n",
       "         2.7188, 3.2812, 3.0781, 2.8750, 2.3906, 2.2344, 2.4062, 1.9922, 3.5312,\n",
       "         2.6250, 2.0625, 2.9375, 2.1875, 2.4062, 2.7500, 2.7188, 2.1250, 2.3125,\n",
       "         3.2656, 2.3281, 2.9375, 2.6719, 2.2188, 2.4844, 2.5781, 2.4375, 3.7031,\n",
       "         3.4844, 2.7188, 2.4688, 2.2969, 2.9219, 3.0000, 1.9219, 2.0312, 2.8125,\n",
       "         1.9766, 3.5938, 2.9844, 2.9219, 2.0938, 2.2188, 2.7188, 2.6094, 2.6406,\n",
       "         2.6562, 2.4688, 2.0156, 3.0156, 3.0625, 2.1250, 2.7188, 2.5000, 2.9531,\n",
       "         2.7188, 2.4375, 2.7031, 2.7188, 2.8594, 2.6719, 2.0156, 2.3281, 2.8750,\n",
       "         2.1562, 2.5312, 2.9062, 2.1250, 2.5938, 2.4688, 2.7812, 2.9219, 2.2188,\n",
       "         1.8516, 2.5938, 3.1406, 2.6719, 1.7656, 2.6250, 2.0156, 2.6875, 2.2500,\n",
       "         3.1719, 2.5000, 3.1250, 1.6875, 3.0000, 2.0625, 2.7500, 2.9844, 2.4844,\n",
       "         2.9375, 3.5000, 1.8906, 2.4219, 2.1406, 3.0312, 2.8125, 1.9922, 2.1562,\n",
       "         1.7656, 2.3281, 2.6094, 2.4531, 2.7500, 2.5156, 2.3281, 3.1875, 1.6953,\n",
       "         2.4219, 3.2812, 2.3438, 2.3281, 3.0781, 2.5000, 2.6719, 2.2812, 1.8281,\n",
       "         2.0781, 2.7656, 2.8438, 2.3594, 2.1875, 1.9297, 2.6562, 2.7344, 2.3594,\n",
       "         2.9062, 2.2969, 3.0469, 2.9531, 2.8281, 3.0625, 2.3438, 2.0000, 2.9219,\n",
       "         3.1875, 2.3906, 2.7344, 2.1719, 2.9219, 2.3906, 3.0469, 2.0938, 2.2031,\n",
       "         2.8906, 2.5156, 2.3125, 1.7734, 3.5156, 2.4688, 3.5000, 2.2969, 2.3750,\n",
       "         2.2188, 2.5625, 2.4375, 2.1250, 2.8750, 2.7031, 2.8594, 2.3125, 3.4531,\n",
       "         3.0938, 2.6094, 2.7656, 2.9062, 2.4375, 2.7031, 2.5781, 2.6094, 2.2812,\n",
       "         2.9375, 2.2031, 3.0156, 2.2812, 2.5625, 3.1406, 2.4062, 1.9062, 2.8438,\n",
       "         2.4375, 2.5156, 2.7031, 2.1406, 3.0469, 2.5469, 2.0469, 2.4062, 2.2344,\n",
       "         2.2188, 2.3594, 2.3438, 1.7656, 1.8984, 2.7656, 2.0938, 2.8125, 2.5625,\n",
       "         2.0781, 2.5312, 3.1562, 2.5000, 2.3438, 2.4062, 2.9375, 2.5312],\n",
       "        device='cuda:0'),\n",
       " 'blocksize': 64,\n",
       " 'quant_map': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
       "          0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
       "        device='cuda:0'),\n",
       " 'dtype': 'bfloat16',\n",
       " 'shape': (128, 256)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.quant_state.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d82d89f-235e-4a2a-b0b3-62a3e5d2adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size, output_size = data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea5b382e-e078-4a23-8a95-c5802eca1cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 256)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size, output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a33876",
   "metadata": {},
   "source": [
    "### Column Parallel\n",
    "\n",
    "The linear layer is defined as Y = XA + b. A is parallelized along its second dimension as A = [A_1, ..., A_p]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54220b7c-a24b-4052-8ccc-d0cbe04827ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5883249c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qweight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec7d2b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size_per_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8e60aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "qweight_partitioned = qweight.split(output_size_per_partition, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2f41ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qweight_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7746431f-166b-4a71-959c-83cead869e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n"
     ]
    }
   ],
   "source": [
    "for w in qweight_partitioned: print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be00bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_absmax = param.quant_state.absmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5c02e37-8815-474e-8b2e-da1414790c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_absmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3d776ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_absmax_reshaped = orig_absmax.reshape(input_size, data.size(1) // blocksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7e1670e-a677-47cc-9e67-1ea10d399e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.Size([128, 4]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_absmax_reshaped.dtype, orig_absmax_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dfd94e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = len(qweight_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f63f1e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "absmax_partitioned = orig_absmax_reshaped.split(orig_absmax_reshaped.size(1) // num_partitions, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d276291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2])\n",
      "torch.Size([128, 2])\n"
     ]
    }
   ],
   "source": [
    "for a in absmax_partitioned: print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e9668e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qweight_partitioned), len(absmax_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e7299d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state = copy.deepcopy(param.quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80b8bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state.shape = torch.Size([quant_state.shape[0], quant_state.shape[1]//num_partitions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de428e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "241a2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state.absmax = absmax_partitioned[0].contiguous().view(-1)\n",
    "deqweight_part1 = dequantize_4bit(qweight_partitioned[0].contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "quant_state.absmax = absmax_partitioned[1].contiguous().view(-1)\n",
    "deqweight_part2 = dequantize_4bit(qweight_partitioned[1].contiguous().view(-1,1), quant_state=quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e15c1671-03b7-45a4-b036-72bf3e82c43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 128]), torch.Size([128, 128]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deqweight_part1.shape, deqweight_part2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4085cb3-2b80-492a-b89b-ca259ce950c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# quant_state.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b63ec5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 256])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deqweight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "978e7a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 256])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([deqweight_part1, deqweight_part2], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d50215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(deqweight, torch.cat([deqweight_part1, deqweight_part2], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a43bafd4-7b83-4314-9019-a41499be5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = (x @ deqweight_part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f2fe31f-1af2-4712-9356-b9894c2d7e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out2 = bnb.matmul_4bit(x, qweight_partitioned[1].contiguous().view(-1,1), quant_state=quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bf8b879-052b-48f4-ba5c-d28e1672d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(out1, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af60fb46-97b8-4b65-8f65-ddb11fd02c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc17f5f4",
   "metadata": {},
   "source": [
    "### Row Parallel\n",
    "\n",
    "The linear layer is defined as Y = XA + b. A is parallelized along\n",
    "its first dimension and X along its second dimension as:\n",
    "\n",
    "```\n",
    "    -   -\n",
    "    | A_1 |\n",
    "    | .   |\n",
    "A = | .   |        X = [X_1, ..., X_p]\n",
    "    | .   |\n",
    "    | A_p |\n",
    "    -   -\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c3dc1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qweight_partitioned = qweight.split(output_size_per_partition, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98d0add2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_partitions = len(qweight_partitioned); num_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77c5bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "for w in qweight_partitioned: print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed58848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_absmax = param.quant_state.absmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e368bab4-38d8-4ee5-9ca7-8a0943fd7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_absmax_reshaped = orig_absmax.reshape(input_size, data.size(1) // blocksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b155bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "absmax_partitioned = orig_absmax.split(len(orig_absmax) // num_partitions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "130b5f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(absmax_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e64143be",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state = copy.deepcopy(param.quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "47f91077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_state.shape = torch.Size([quant_state.shape[0]//num_partitions, quant_state.shape[1]]); quant_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "128e667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state.absmax = absmax_partitioned[0].contiguous().view(-1)\n",
    "deqweight_part1 = dequantize_4bit(qweight_partitioned[0].contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "quant_state.absmax = absmax_partitioned[1].contiguous().view(-1)\n",
    "deqweight_part2 = dequantize_4bit(qweight_partitioned[1].contiguous().view(-1,1), quant_state=quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d735c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(deqweight, torch.cat([deqweight_part1, deqweight_part2], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53f04e-f053-423b-b3f7-88283531858c",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39e0da2-6d9e-4a0e-bec9-7e4007c8af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-29 13:23:10 pynccl_utils.py:13] vLLM is using nccl==2.18.1\n"
     ]
    }
   ],
   "source": [
    "from vllm.model_executor.weight_utils import default_weight_loader, hf_model_weights_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b267b53-a7d4-43af-82e2-ac18b6b66a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_iterator = hf_model_weights_iterator(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdc64de-22cd-413c-9b63-e2351e018c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-29 08:48:45 weight_utils.py:177] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd674cc5dcf7487f923f00c680e6a73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93c344b17544540b2d5f59dac5af4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for name, loaded_weight in weights_iterator: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0853623c-1c05-4599-8b74-26e55d33a310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model.embed_tokens.weight',\n",
       " tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
       "          -6.5565e-06,  8.9407e-07],\n",
       "         [ 1.8616e-03, -3.3722e-03,  3.9864e-04,  ..., -8.3008e-03,\n",
       "           2.5787e-03, -3.9368e-03],\n",
       "         [ 1.0986e-02,  9.8877e-03, -5.0964e-03,  ...,  2.5177e-03,\n",
       "           7.7057e-04, -5.0049e-03],\n",
       "         ...,\n",
       "         [-1.3977e-02, -2.7313e-03, -1.9897e-02,  ..., -1.0437e-02,\n",
       "           9.5825e-03, -1.8005e-03],\n",
       "         [-1.0742e-02,  9.3384e-03,  1.2939e-02,  ..., -3.3203e-02,\n",
       "          -1.6357e-02,  3.3875e-03],\n",
       "         [-8.3008e-03, -4.0588e-03, -1.1063e-03,  ...,  3.4790e-03,\n",
       "          -1.2939e-02,  3.1948e-05]], dtype=torch.float16))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name, loaded_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b31e4388-af56-49ee-b81e-a4d2a333bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_iterator = hf_model_weights_iterator(\"TheBloke/CodeUp-Alpha-13B-HF-AWQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b23f97a-ad91-434b-aa63-2fa548c0193d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.mlp.down_proj.qzeros\n",
      "model.layers.0.mlp.down_proj.scales\n",
      "model.layers.0.mlp.gate_proj.qzeros\n",
      "model.layers.0.mlp.gate_proj.scales\n",
      "model.layers.0.mlp.up_proj.qzeros\n",
      "model.layers.0.mlp.up_proj.scales\n",
      "model.layers.0.self_attn.k_proj.qzeros\n",
      "model.layers.0.self_attn.k_proj.scales\n",
      "model.layers.0.self_attn.o_proj.qzeros\n",
      "model.layers.0.self_attn.o_proj.scales\n",
      "model.layers.0.self_attn.q_proj.qzeros\n",
      "model.layers.0.self_attn.q_proj.scales\n",
      "model.layers.0.self_attn.v_proj.qzeros\n",
      "model.layers.0.self_attn.v_proj.scales\n",
      "model.layers.1.mlp.down_proj.qzeros\n",
      "model.layers.1.mlp.down_proj.scales\n",
      "model.layers.1.mlp.gate_proj.qzeros\n",
      "model.layers.1.mlp.gate_proj.scales\n",
      "model.layers.1.mlp.up_proj.qzeros\n",
      "model.layers.1.mlp.up_proj.scales\n",
      "model.layers.1.self_attn.k_proj.qzeros\n",
      "model.layers.1.self_attn.k_proj.scales\n",
      "model.layers.1.self_attn.o_proj.qzeros\n",
      "model.layers.1.self_attn.o_proj.scales\n",
      "model.layers.1.self_attn.q_proj.qzeros\n",
      "model.layers.1.self_attn.q_proj.scales\n",
      "model.layers.1.self_attn.v_proj.qzeros\n",
      "model.layers.1.self_attn.v_proj.scales\n",
      "model.layers.10.mlp.down_proj.qzeros\n",
      "model.layers.10.mlp.down_proj.scales\n",
      "model.layers.10.mlp.gate_proj.qzeros\n",
      "model.layers.10.mlp.gate_proj.scales\n",
      "model.layers.10.mlp.up_proj.qzeros\n",
      "model.layers.10.mlp.up_proj.scales\n",
      "model.layers.10.self_attn.k_proj.qzeros\n",
      "model.layers.10.self_attn.k_proj.scales\n",
      "model.layers.10.self_attn.o_proj.qzeros\n",
      "model.layers.10.self_attn.o_proj.scales\n",
      "model.layers.10.self_attn.q_proj.qzeros\n",
      "model.layers.10.self_attn.q_proj.scales\n",
      "model.layers.10.self_attn.v_proj.qzeros\n",
      "model.layers.10.self_attn.v_proj.scales\n",
      "model.layers.11.mlp.down_proj.qzeros\n",
      "model.layers.11.mlp.down_proj.scales\n",
      "model.layers.11.mlp.gate_proj.qzeros\n",
      "model.layers.11.mlp.gate_proj.scales\n",
      "model.layers.11.mlp.up_proj.qzeros\n",
      "model.layers.11.mlp.up_proj.scales\n",
      "model.layers.11.self_attn.k_proj.qzeros\n",
      "model.layers.11.self_attn.k_proj.scales\n",
      "model.layers.11.self_attn.o_proj.qzeros\n",
      "model.layers.11.self_attn.o_proj.scales\n",
      "model.layers.11.self_attn.q_proj.qzeros\n",
      "model.layers.11.self_attn.q_proj.scales\n",
      "model.layers.11.self_attn.v_proj.qzeros\n",
      "model.layers.11.self_attn.v_proj.scales\n",
      "model.layers.12.mlp.down_proj.qzeros\n",
      "model.layers.12.mlp.down_proj.scales\n",
      "model.layers.12.mlp.gate_proj.qzeros\n",
      "model.layers.12.mlp.gate_proj.scales\n",
      "model.layers.12.mlp.up_proj.qzeros\n",
      "model.layers.12.mlp.up_proj.scales\n",
      "model.layers.12.self_attn.k_proj.qzeros\n",
      "model.layers.12.self_attn.k_proj.scales\n",
      "model.layers.12.self_attn.o_proj.qzeros\n",
      "model.layers.12.self_attn.o_proj.scales\n",
      "model.layers.12.self_attn.q_proj.qzeros\n",
      "model.layers.12.self_attn.q_proj.scales\n",
      "model.layers.12.self_attn.v_proj.qzeros\n",
      "model.layers.12.self_attn.v_proj.scales\n",
      "model.layers.13.mlp.down_proj.qzeros\n",
      "model.layers.13.mlp.down_proj.scales\n",
      "model.layers.13.mlp.gate_proj.qzeros\n",
      "model.layers.13.mlp.gate_proj.scales\n",
      "model.layers.13.mlp.up_proj.qzeros\n",
      "model.layers.13.mlp.up_proj.scales\n",
      "model.layers.13.self_attn.k_proj.qzeros\n",
      "model.layers.13.self_attn.k_proj.scales\n",
      "model.layers.13.self_attn.o_proj.qzeros\n",
      "model.layers.13.self_attn.o_proj.scales\n",
      "model.layers.13.self_attn.q_proj.qzeros\n",
      "model.layers.13.self_attn.q_proj.scales\n",
      "model.layers.13.self_attn.v_proj.qzeros\n",
      "model.layers.13.self_attn.v_proj.scales\n",
      "model.layers.14.mlp.down_proj.qzeros\n",
      "model.layers.14.mlp.down_proj.scales\n",
      "model.layers.14.mlp.gate_proj.qzeros\n",
      "model.layers.14.mlp.gate_proj.scales\n",
      "model.layers.14.mlp.up_proj.qzeros\n",
      "model.layers.14.mlp.up_proj.scales\n",
      "model.layers.14.self_attn.k_proj.qzeros\n",
      "model.layers.14.self_attn.k_proj.scales\n",
      "model.layers.14.self_attn.o_proj.qzeros\n",
      "model.layers.14.self_attn.o_proj.scales\n",
      "model.layers.14.self_attn.q_proj.qzeros\n",
      "model.layers.14.self_attn.q_proj.scales\n",
      "model.layers.14.self_attn.v_proj.qzeros\n",
      "model.layers.14.self_attn.v_proj.scales\n",
      "model.layers.15.mlp.down_proj.qzeros\n",
      "model.layers.15.mlp.down_proj.scales\n",
      "model.layers.15.mlp.gate_proj.qzeros\n",
      "model.layers.15.mlp.gate_proj.scales\n",
      "model.layers.15.mlp.up_proj.qzeros\n",
      "model.layers.15.mlp.up_proj.scales\n",
      "model.layers.15.self_attn.k_proj.qzeros\n",
      "model.layers.15.self_attn.k_proj.scales\n",
      "model.layers.15.self_attn.o_proj.qzeros\n",
      "model.layers.15.self_attn.o_proj.scales\n",
      "model.layers.15.self_attn.q_proj.qzeros\n",
      "model.layers.15.self_attn.q_proj.scales\n",
      "model.layers.15.self_attn.v_proj.qzeros\n",
      "model.layers.15.self_attn.v_proj.scales\n",
      "model.layers.16.mlp.down_proj.qzeros\n",
      "model.layers.16.mlp.down_proj.scales\n",
      "model.layers.16.mlp.gate_proj.qzeros\n",
      "model.layers.16.mlp.gate_proj.scales\n",
      "model.layers.16.mlp.up_proj.qzeros\n",
      "model.layers.16.mlp.up_proj.scales\n",
      "model.layers.16.self_attn.k_proj.qzeros\n",
      "model.layers.16.self_attn.k_proj.scales\n",
      "model.layers.16.self_attn.o_proj.qzeros\n",
      "model.layers.16.self_attn.o_proj.scales\n",
      "model.layers.16.self_attn.q_proj.qzeros\n",
      "model.layers.16.self_attn.q_proj.scales\n",
      "model.layers.16.self_attn.v_proj.qzeros\n",
      "model.layers.16.self_attn.v_proj.scales\n",
      "model.layers.17.mlp.down_proj.qzeros\n",
      "model.layers.17.mlp.down_proj.scales\n",
      "model.layers.17.mlp.gate_proj.qzeros\n",
      "model.layers.17.mlp.gate_proj.scales\n",
      "model.layers.17.mlp.up_proj.qzeros\n",
      "model.layers.17.mlp.up_proj.scales\n",
      "model.layers.17.self_attn.k_proj.qzeros\n",
      "model.layers.17.self_attn.k_proj.scales\n",
      "model.layers.17.self_attn.o_proj.qzeros\n",
      "model.layers.17.self_attn.o_proj.scales\n",
      "model.layers.17.self_attn.q_proj.qzeros\n",
      "model.layers.17.self_attn.q_proj.scales\n",
      "model.layers.17.self_attn.v_proj.qzeros\n",
      "model.layers.17.self_attn.v_proj.scales\n",
      "model.layers.18.mlp.down_proj.qzeros\n",
      "model.layers.18.mlp.down_proj.scales\n",
      "model.layers.18.mlp.gate_proj.qzeros\n",
      "model.layers.18.mlp.gate_proj.scales\n",
      "model.layers.18.mlp.up_proj.qzeros\n",
      "model.layers.18.mlp.up_proj.scales\n",
      "model.layers.18.self_attn.k_proj.qzeros\n",
      "model.layers.18.self_attn.k_proj.scales\n",
      "model.layers.18.self_attn.o_proj.qzeros\n",
      "model.layers.18.self_attn.o_proj.scales\n",
      "model.layers.18.self_attn.q_proj.qzeros\n",
      "model.layers.18.self_attn.q_proj.scales\n",
      "model.layers.18.self_attn.v_proj.qzeros\n",
      "model.layers.18.self_attn.v_proj.scales\n",
      "model.layers.19.mlp.down_proj.qzeros\n",
      "model.layers.19.mlp.down_proj.scales\n",
      "model.layers.19.mlp.gate_proj.qzeros\n",
      "model.layers.19.mlp.gate_proj.scales\n",
      "model.layers.19.mlp.up_proj.qzeros\n",
      "model.layers.19.mlp.up_proj.scales\n",
      "model.layers.19.self_attn.k_proj.qzeros\n",
      "model.layers.19.self_attn.k_proj.scales\n",
      "model.layers.19.self_attn.o_proj.qzeros\n",
      "model.layers.19.self_attn.o_proj.scales\n",
      "model.layers.19.self_attn.q_proj.qzeros\n",
      "model.layers.19.self_attn.q_proj.scales\n",
      "model.layers.19.self_attn.v_proj.qzeros\n",
      "model.layers.19.self_attn.v_proj.scales\n",
      "model.layers.2.mlp.down_proj.qzeros\n",
      "model.layers.2.mlp.down_proj.scales\n",
      "model.layers.2.mlp.gate_proj.qzeros\n",
      "model.layers.2.mlp.gate_proj.scales\n",
      "model.layers.2.mlp.up_proj.qzeros\n",
      "model.layers.2.mlp.up_proj.scales\n",
      "model.layers.2.self_attn.k_proj.qzeros\n",
      "model.layers.2.self_attn.k_proj.scales\n",
      "model.layers.2.self_attn.o_proj.qzeros\n",
      "model.layers.2.self_attn.o_proj.scales\n",
      "model.layers.2.self_attn.q_proj.qzeros\n",
      "model.layers.2.self_attn.q_proj.scales\n",
      "model.layers.2.self_attn.v_proj.qzeros\n",
      "model.layers.2.self_attn.v_proj.scales\n",
      "model.layers.20.mlp.down_proj.qzeros\n",
      "model.layers.20.mlp.down_proj.scales\n",
      "model.layers.20.mlp.gate_proj.qzeros\n",
      "model.layers.20.mlp.gate_proj.scales\n",
      "model.layers.20.mlp.up_proj.qzeros\n",
      "model.layers.20.mlp.up_proj.scales\n",
      "model.layers.20.self_attn.k_proj.qzeros\n",
      "model.layers.20.self_attn.k_proj.scales\n",
      "model.layers.20.self_attn.o_proj.qzeros\n",
      "model.layers.20.self_attn.o_proj.scales\n",
      "model.layers.20.self_attn.q_proj.qzeros\n",
      "model.layers.20.self_attn.q_proj.scales\n",
      "model.layers.20.self_attn.v_proj.qzeros\n",
      "model.layers.20.self_attn.v_proj.scales\n",
      "model.layers.21.mlp.down_proj.qzeros\n",
      "model.layers.21.mlp.down_proj.scales\n",
      "model.layers.21.mlp.gate_proj.qzeros\n",
      "model.layers.21.mlp.gate_proj.scales\n",
      "model.layers.21.mlp.up_proj.qzeros\n",
      "model.layers.21.mlp.up_proj.scales\n",
      "model.layers.21.self_attn.k_proj.qzeros\n",
      "model.layers.21.self_attn.k_proj.scales\n",
      "model.layers.21.self_attn.o_proj.qzeros\n",
      "model.layers.21.self_attn.o_proj.scales\n",
      "model.layers.21.self_attn.q_proj.qzeros\n",
      "model.layers.21.self_attn.q_proj.scales\n",
      "model.layers.21.self_attn.v_proj.qzeros\n",
      "model.layers.21.self_attn.v_proj.scales\n",
      "model.layers.22.mlp.down_proj.qzeros\n",
      "model.layers.22.mlp.down_proj.scales\n",
      "model.layers.22.mlp.gate_proj.qzeros\n",
      "model.layers.22.mlp.gate_proj.scales\n",
      "model.layers.22.mlp.up_proj.qzeros\n",
      "model.layers.22.mlp.up_proj.scales\n",
      "model.layers.22.self_attn.k_proj.qzeros\n",
      "model.layers.22.self_attn.k_proj.scales\n",
      "model.layers.22.self_attn.o_proj.qzeros\n",
      "model.layers.22.self_attn.o_proj.scales\n",
      "model.layers.22.self_attn.q_proj.qzeros\n",
      "model.layers.22.self_attn.q_proj.scales\n",
      "model.layers.22.self_attn.v_proj.qzeros\n",
      "model.layers.22.self_attn.v_proj.scales\n",
      "model.layers.23.mlp.down_proj.qzeros\n",
      "model.layers.23.mlp.down_proj.scales\n",
      "model.layers.23.mlp.gate_proj.qzeros\n",
      "model.layers.23.mlp.gate_proj.scales\n",
      "model.layers.23.mlp.up_proj.qzeros\n",
      "model.layers.23.mlp.up_proj.scales\n",
      "model.layers.23.self_attn.k_proj.qzeros\n",
      "model.layers.23.self_attn.k_proj.scales\n",
      "model.layers.23.self_attn.o_proj.qzeros\n",
      "model.layers.23.self_attn.o_proj.scales\n",
      "model.layers.23.self_attn.q_proj.qzeros\n",
      "model.layers.23.self_attn.q_proj.scales\n",
      "model.layers.23.self_attn.v_proj.qzeros\n",
      "model.layers.23.self_attn.v_proj.scales\n",
      "model.layers.24.mlp.down_proj.qzeros\n",
      "model.layers.24.mlp.down_proj.scales\n",
      "model.layers.24.mlp.gate_proj.qzeros\n",
      "model.layers.24.mlp.gate_proj.scales\n",
      "model.layers.24.mlp.up_proj.qzeros\n",
      "model.layers.24.mlp.up_proj.scales\n",
      "model.layers.24.self_attn.k_proj.qzeros\n",
      "model.layers.24.self_attn.k_proj.scales\n",
      "model.layers.24.self_attn.o_proj.qzeros\n",
      "model.layers.24.self_attn.o_proj.scales\n",
      "model.layers.24.self_attn.q_proj.qzeros\n",
      "model.layers.24.self_attn.q_proj.scales\n",
      "model.layers.24.self_attn.v_proj.qzeros\n",
      "model.layers.24.self_attn.v_proj.scales\n",
      "model.layers.25.mlp.down_proj.qzeros\n",
      "model.layers.25.mlp.down_proj.scales\n",
      "model.layers.25.mlp.gate_proj.qzeros\n",
      "model.layers.25.mlp.gate_proj.scales\n",
      "model.layers.25.mlp.up_proj.qzeros\n",
      "model.layers.25.mlp.up_proj.scales\n",
      "model.layers.25.self_attn.k_proj.qzeros\n",
      "model.layers.25.self_attn.k_proj.scales\n",
      "model.layers.25.self_attn.o_proj.qzeros\n",
      "model.layers.25.self_attn.o_proj.scales\n",
      "model.layers.25.self_attn.q_proj.qzeros\n",
      "model.layers.25.self_attn.q_proj.scales\n",
      "model.layers.25.self_attn.v_proj.qzeros\n",
      "model.layers.25.self_attn.v_proj.scales\n",
      "model.layers.26.mlp.down_proj.qzeros\n",
      "model.layers.26.mlp.down_proj.scales\n",
      "model.layers.26.mlp.gate_proj.qzeros\n",
      "model.layers.26.mlp.gate_proj.scales\n",
      "model.layers.26.mlp.up_proj.qzeros\n",
      "model.layers.26.mlp.up_proj.scales\n",
      "model.layers.26.self_attn.k_proj.qzeros\n",
      "model.layers.26.self_attn.k_proj.scales\n",
      "model.layers.26.self_attn.o_proj.qzeros\n",
      "model.layers.26.self_attn.o_proj.scales\n",
      "model.layers.26.self_attn.q_proj.qzeros\n",
      "model.layers.26.self_attn.q_proj.scales\n",
      "model.layers.26.self_attn.v_proj.qzeros\n",
      "model.layers.26.self_attn.v_proj.scales\n",
      "model.layers.27.mlp.down_proj.qzeros\n",
      "model.layers.27.mlp.down_proj.scales\n",
      "model.layers.27.mlp.gate_proj.qzeros\n",
      "model.layers.27.mlp.gate_proj.scales\n",
      "model.layers.27.mlp.up_proj.qzeros\n",
      "model.layers.27.mlp.up_proj.scales\n",
      "model.layers.27.self_attn.k_proj.qzeros\n",
      "model.layers.27.self_attn.k_proj.scales\n",
      "model.layers.27.self_attn.o_proj.qzeros\n",
      "model.layers.27.self_attn.o_proj.scales\n",
      "model.layers.27.self_attn.q_proj.qzeros\n",
      "model.layers.27.self_attn.q_proj.scales\n",
      "model.layers.27.self_attn.v_proj.qzeros\n",
      "model.layers.27.self_attn.v_proj.scales\n",
      "model.layers.28.mlp.down_proj.qzeros\n",
      "model.layers.28.mlp.down_proj.scales\n",
      "model.layers.28.mlp.gate_proj.qzeros\n",
      "model.layers.28.mlp.gate_proj.scales\n",
      "model.layers.28.mlp.up_proj.qzeros\n",
      "model.layers.28.mlp.up_proj.scales\n",
      "model.layers.28.self_attn.k_proj.qzeros\n",
      "model.layers.28.self_attn.k_proj.scales\n",
      "model.layers.28.self_attn.o_proj.qzeros\n",
      "model.layers.28.self_attn.o_proj.scales\n",
      "model.layers.28.self_attn.q_proj.qzeros\n",
      "model.layers.28.self_attn.q_proj.scales\n",
      "model.layers.28.self_attn.v_proj.qzeros\n",
      "model.layers.28.self_attn.v_proj.scales\n",
      "model.layers.29.mlp.down_proj.qzeros\n",
      "model.layers.29.mlp.down_proj.scales\n",
      "model.layers.29.mlp.gate_proj.qzeros\n",
      "model.layers.29.mlp.gate_proj.scales\n",
      "model.layers.29.mlp.up_proj.qzeros\n",
      "model.layers.29.mlp.up_proj.scales\n",
      "model.layers.29.self_attn.k_proj.qzeros\n",
      "model.layers.29.self_attn.k_proj.scales\n",
      "model.layers.29.self_attn.o_proj.qzeros\n",
      "model.layers.29.self_attn.o_proj.scales\n",
      "model.layers.29.self_attn.q_proj.qzeros\n",
      "model.layers.29.self_attn.q_proj.scales\n",
      "model.layers.29.self_attn.v_proj.qzeros\n",
      "model.layers.29.self_attn.v_proj.scales\n",
      "model.layers.3.mlp.down_proj.qzeros\n",
      "model.layers.3.mlp.down_proj.scales\n",
      "model.layers.3.mlp.gate_proj.qzeros\n",
      "model.layers.3.mlp.gate_proj.scales\n",
      "model.layers.3.mlp.up_proj.qzeros\n",
      "model.layers.3.mlp.up_proj.scales\n",
      "model.layers.3.self_attn.k_proj.qzeros\n",
      "model.layers.3.self_attn.k_proj.scales\n",
      "model.layers.3.self_attn.o_proj.qzeros\n",
      "model.layers.3.self_attn.o_proj.scales\n",
      "model.layers.3.self_attn.q_proj.qzeros\n",
      "model.layers.3.self_attn.q_proj.scales\n",
      "model.layers.3.self_attn.v_proj.qzeros\n",
      "model.layers.3.self_attn.v_proj.scales\n",
      "model.layers.30.mlp.down_proj.qzeros\n",
      "model.layers.30.mlp.down_proj.scales\n",
      "model.layers.30.mlp.gate_proj.qzeros\n",
      "model.layers.30.mlp.gate_proj.scales\n",
      "model.layers.30.mlp.up_proj.qzeros\n",
      "model.layers.30.mlp.up_proj.scales\n",
      "model.layers.30.self_attn.k_proj.qzeros\n",
      "model.layers.30.self_attn.k_proj.scales\n",
      "model.layers.30.self_attn.o_proj.qzeros\n",
      "model.layers.30.self_attn.o_proj.scales\n",
      "model.layers.30.self_attn.q_proj.qzeros\n",
      "model.layers.30.self_attn.q_proj.scales\n",
      "model.layers.30.self_attn.v_proj.qzeros\n",
      "model.layers.30.self_attn.v_proj.scales\n",
      "model.layers.31.mlp.down_proj.qzeros\n",
      "model.layers.31.mlp.down_proj.scales\n",
      "model.layers.31.mlp.gate_proj.qzeros\n",
      "model.layers.31.mlp.gate_proj.scales\n",
      "model.layers.31.mlp.up_proj.qzeros\n",
      "model.layers.31.mlp.up_proj.scales\n",
      "model.layers.31.self_attn.k_proj.qzeros\n",
      "model.layers.31.self_attn.k_proj.scales\n",
      "model.layers.31.self_attn.o_proj.qzeros\n",
      "model.layers.31.self_attn.o_proj.scales\n",
      "model.layers.31.self_attn.q_proj.qzeros\n",
      "model.layers.31.self_attn.q_proj.scales\n",
      "model.layers.31.self_attn.v_proj.qzeros\n",
      "model.layers.31.self_attn.v_proj.scales\n",
      "model.layers.32.mlp.down_proj.qzeros\n",
      "model.layers.32.mlp.down_proj.scales\n",
      "model.layers.32.mlp.gate_proj.qzeros\n",
      "model.layers.32.mlp.gate_proj.scales\n",
      "model.layers.32.mlp.up_proj.qzeros\n",
      "model.layers.32.mlp.up_proj.scales\n",
      "model.layers.32.self_attn.k_proj.qzeros\n",
      "model.layers.32.self_attn.k_proj.scales\n",
      "model.layers.32.self_attn.o_proj.qzeros\n",
      "model.layers.32.self_attn.o_proj.scales\n",
      "model.layers.32.self_attn.q_proj.qzeros\n",
      "model.layers.32.self_attn.q_proj.scales\n",
      "model.layers.32.self_attn.v_proj.qzeros\n",
      "model.layers.32.self_attn.v_proj.scales\n",
      "model.layers.33.mlp.down_proj.qzeros\n",
      "model.layers.33.mlp.down_proj.scales\n",
      "model.layers.33.mlp.gate_proj.qzeros\n",
      "model.layers.33.mlp.gate_proj.scales\n",
      "model.layers.33.mlp.up_proj.qzeros\n",
      "model.layers.33.mlp.up_proj.scales\n",
      "model.layers.33.self_attn.k_proj.qzeros\n",
      "model.layers.33.self_attn.k_proj.scales\n",
      "model.layers.33.self_attn.o_proj.qzeros\n",
      "model.layers.33.self_attn.o_proj.scales\n",
      "model.layers.33.self_attn.q_proj.qzeros\n",
      "model.layers.33.self_attn.q_proj.scales\n",
      "model.layers.33.self_attn.v_proj.qzeros\n",
      "model.layers.33.self_attn.v_proj.scales\n",
      "model.layers.34.mlp.down_proj.qzeros\n",
      "model.layers.34.mlp.down_proj.scales\n",
      "model.layers.34.mlp.gate_proj.qzeros\n",
      "model.layers.34.mlp.gate_proj.scales\n",
      "model.layers.34.mlp.up_proj.qzeros\n",
      "model.layers.34.mlp.up_proj.scales\n",
      "model.layers.34.self_attn.k_proj.qzeros\n",
      "model.layers.34.self_attn.k_proj.scales\n",
      "model.layers.34.self_attn.o_proj.qzeros\n",
      "model.layers.34.self_attn.o_proj.scales\n",
      "model.layers.34.self_attn.q_proj.qzeros\n",
      "model.layers.34.self_attn.q_proj.scales\n",
      "model.layers.34.self_attn.v_proj.qzeros\n",
      "model.layers.34.self_attn.v_proj.scales\n",
      "model.layers.35.mlp.down_proj.qzeros\n",
      "model.layers.35.mlp.down_proj.scales\n",
      "model.layers.35.mlp.gate_proj.qzeros\n",
      "model.layers.35.mlp.gate_proj.scales\n",
      "model.layers.35.mlp.up_proj.qzeros\n",
      "model.layers.35.mlp.up_proj.scales\n",
      "model.layers.35.self_attn.k_proj.qzeros\n",
      "model.layers.35.self_attn.k_proj.scales\n",
      "model.layers.35.self_attn.o_proj.qzeros\n",
      "model.layers.35.self_attn.o_proj.scales\n",
      "model.layers.35.self_attn.q_proj.qzeros\n",
      "model.layers.35.self_attn.q_proj.scales\n",
      "model.layers.35.self_attn.v_proj.qzeros\n",
      "model.layers.35.self_attn.v_proj.scales\n",
      "model.layers.36.mlp.down_proj.qzeros\n",
      "model.layers.36.mlp.down_proj.scales\n",
      "model.layers.36.mlp.gate_proj.qzeros\n",
      "model.layers.36.mlp.gate_proj.scales\n",
      "model.layers.36.mlp.up_proj.qzeros\n",
      "model.layers.36.mlp.up_proj.scales\n",
      "model.layers.36.self_attn.k_proj.qzeros\n",
      "model.layers.36.self_attn.k_proj.scales\n",
      "model.layers.36.self_attn.o_proj.qzeros\n",
      "model.layers.36.self_attn.o_proj.scales\n",
      "model.layers.36.self_attn.q_proj.qzeros\n",
      "model.layers.36.self_attn.q_proj.scales\n",
      "model.layers.36.self_attn.v_proj.qzeros\n",
      "model.layers.36.self_attn.v_proj.scales\n",
      "model.layers.37.mlp.down_proj.qzeros\n",
      "model.layers.37.mlp.down_proj.scales\n",
      "model.layers.37.mlp.gate_proj.qzeros\n",
      "model.layers.37.mlp.gate_proj.scales\n",
      "model.layers.37.mlp.up_proj.qzeros\n",
      "model.layers.37.mlp.up_proj.scales\n",
      "model.layers.37.self_attn.k_proj.qzeros\n",
      "model.layers.37.self_attn.k_proj.scales\n",
      "model.layers.37.self_attn.o_proj.qzeros\n",
      "model.layers.37.self_attn.o_proj.scales\n",
      "model.layers.37.self_attn.q_proj.qzeros\n",
      "model.layers.37.self_attn.q_proj.scales\n",
      "model.layers.37.self_attn.v_proj.qzeros\n",
      "model.layers.37.self_attn.v_proj.scales\n",
      "model.layers.38.mlp.down_proj.qzeros\n",
      "model.layers.38.mlp.down_proj.scales\n",
      "model.layers.38.mlp.gate_proj.qzeros\n",
      "model.layers.38.mlp.gate_proj.scales\n",
      "model.layers.38.mlp.up_proj.qzeros\n",
      "model.layers.38.mlp.up_proj.scales\n",
      "model.layers.38.self_attn.k_proj.qzeros\n",
      "model.layers.38.self_attn.k_proj.scales\n",
      "model.layers.38.self_attn.o_proj.qzeros\n",
      "model.layers.38.self_attn.o_proj.scales\n",
      "model.layers.38.self_attn.q_proj.qzeros\n",
      "model.layers.38.self_attn.q_proj.scales\n",
      "model.layers.38.self_attn.v_proj.qzeros\n",
      "model.layers.38.self_attn.v_proj.scales\n",
      "model.layers.39.mlp.down_proj.qzeros\n",
      "model.layers.39.mlp.down_proj.scales\n",
      "model.layers.39.mlp.gate_proj.qzeros\n",
      "model.layers.39.mlp.gate_proj.scales\n",
      "model.layers.39.mlp.up_proj.qzeros\n",
      "model.layers.39.mlp.up_proj.scales\n",
      "model.layers.39.self_attn.k_proj.qzeros\n",
      "model.layers.39.self_attn.k_proj.scales\n",
      "model.layers.39.self_attn.o_proj.qzeros\n",
      "model.layers.39.self_attn.o_proj.scales\n",
      "model.layers.39.self_attn.q_proj.qzeros\n",
      "model.layers.39.self_attn.q_proj.scales\n",
      "model.layers.39.self_attn.v_proj.qzeros\n",
      "model.layers.39.self_attn.v_proj.scales\n",
      "model.layers.4.mlp.down_proj.qzeros\n",
      "model.layers.4.mlp.down_proj.scales\n",
      "model.layers.4.mlp.gate_proj.qzeros\n",
      "model.layers.4.mlp.gate_proj.scales\n",
      "model.layers.4.mlp.up_proj.qzeros\n",
      "model.layers.4.mlp.up_proj.scales\n",
      "model.layers.4.self_attn.k_proj.qzeros\n",
      "model.layers.4.self_attn.k_proj.scales\n",
      "model.layers.4.self_attn.o_proj.qzeros\n",
      "model.layers.4.self_attn.o_proj.scales\n",
      "model.layers.4.self_attn.q_proj.qzeros\n",
      "model.layers.4.self_attn.q_proj.scales\n",
      "model.layers.4.self_attn.v_proj.qzeros\n",
      "model.layers.4.self_attn.v_proj.scales\n",
      "model.layers.5.mlp.down_proj.qzeros\n",
      "model.layers.5.mlp.down_proj.scales\n",
      "model.layers.5.mlp.gate_proj.qzeros\n",
      "model.layers.5.mlp.gate_proj.scales\n",
      "model.layers.5.mlp.up_proj.qzeros\n",
      "model.layers.5.mlp.up_proj.scales\n",
      "model.layers.5.self_attn.k_proj.qzeros\n",
      "model.layers.5.self_attn.k_proj.scales\n",
      "model.layers.5.self_attn.o_proj.qzeros\n",
      "model.layers.5.self_attn.o_proj.scales\n",
      "model.layers.5.self_attn.q_proj.qzeros\n",
      "model.layers.5.self_attn.q_proj.scales\n",
      "model.layers.5.self_attn.v_proj.qzeros\n",
      "model.layers.5.self_attn.v_proj.scales\n",
      "model.layers.6.mlp.down_proj.qzeros\n",
      "model.layers.6.mlp.down_proj.scales\n",
      "model.layers.6.mlp.gate_proj.qzeros\n",
      "model.layers.6.mlp.gate_proj.scales\n",
      "model.layers.6.mlp.up_proj.qzeros\n",
      "model.layers.6.mlp.up_proj.scales\n",
      "model.layers.6.self_attn.k_proj.qzeros\n",
      "model.layers.6.self_attn.k_proj.scales\n",
      "model.layers.6.self_attn.o_proj.qzeros\n",
      "model.layers.6.self_attn.o_proj.scales\n",
      "model.layers.6.self_attn.q_proj.qzeros\n",
      "model.layers.6.self_attn.q_proj.scales\n",
      "model.layers.6.self_attn.v_proj.qzeros\n",
      "model.layers.6.self_attn.v_proj.scales\n",
      "model.layers.7.mlp.down_proj.qzeros\n",
      "model.layers.7.mlp.down_proj.scales\n",
      "model.layers.7.mlp.gate_proj.qzeros\n",
      "model.layers.7.mlp.gate_proj.scales\n",
      "model.layers.7.mlp.up_proj.qzeros\n",
      "model.layers.7.mlp.up_proj.scales\n",
      "model.layers.7.self_attn.k_proj.qzeros\n",
      "model.layers.7.self_attn.k_proj.scales\n",
      "model.layers.7.self_attn.o_proj.qzeros\n",
      "model.layers.7.self_attn.o_proj.scales\n",
      "model.layers.7.self_attn.q_proj.qzeros\n",
      "model.layers.7.self_attn.q_proj.scales\n",
      "model.layers.7.self_attn.v_proj.qzeros\n",
      "model.layers.7.self_attn.v_proj.scales\n",
      "model.layers.8.mlp.down_proj.qzeros\n",
      "model.layers.8.mlp.down_proj.scales\n",
      "model.layers.8.mlp.gate_proj.qzeros\n",
      "model.layers.8.mlp.gate_proj.scales\n",
      "model.layers.8.mlp.up_proj.qzeros\n",
      "model.layers.8.mlp.up_proj.scales\n",
      "model.layers.8.self_attn.k_proj.qzeros\n",
      "model.layers.8.self_attn.k_proj.scales\n",
      "model.layers.8.self_attn.o_proj.qzeros\n",
      "model.layers.8.self_attn.o_proj.scales\n",
      "model.layers.8.self_attn.q_proj.qzeros\n",
      "model.layers.8.self_attn.q_proj.scales\n",
      "model.layers.8.self_attn.v_proj.qzeros\n",
      "model.layers.8.self_attn.v_proj.scales\n",
      "model.layers.9.mlp.down_proj.qzeros\n",
      "model.layers.9.mlp.down_proj.scales\n",
      "model.layers.9.mlp.gate_proj.qzeros\n",
      "model.layers.9.mlp.gate_proj.scales\n",
      "model.layers.9.mlp.up_proj.qzeros\n",
      "model.layers.9.mlp.up_proj.scales\n",
      "model.layers.9.self_attn.k_proj.qzeros\n",
      "model.layers.9.self_attn.k_proj.scales\n",
      "model.layers.9.self_attn.o_proj.qzeros\n",
      "model.layers.9.self_attn.o_proj.scales\n",
      "model.layers.9.self_attn.q_proj.qzeros\n",
      "model.layers.9.self_attn.q_proj.scales\n",
      "model.layers.9.self_attn.v_proj.qzeros\n",
      "model.layers.9.self_attn.v_proj.scales\n"
     ]
    }
   ],
   "source": [
    "for name, loaded_weight in weights_iterator: \n",
    "    if 'scales' in name or 'zeros' in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9098d1-2374-4060-b8fe-60052c04110e",
   "metadata": {},
   "source": [
    "### Create Quantized Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7cfc1da-2147-4722-a766-ee216a29ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, json\n",
    "from safetensors.torch import save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7d1dd78-d696-4477-b878-a13a763c270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path(\"/home/ubuntu/models/llama-7b-hf-nf4-quantized\")\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c935e5-e5a6-4c64-8bfd-168e07352d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14571164abf44dda94ba3e361c0968b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a8cb2-6eff-4645-bcff-c4bf66238802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original quantized layers from fsdp_qlora/train.py\n",
    "# [\"k_proj\", \"q_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afaa582c-95b9-4e22-a209-485cfb73f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to AWQ for now\n",
    "quantized_layers = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eaef503-678f-4084-b3f7-faa0af7f6d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_state_dict = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3686d0a-497a-4486-bf84-169dc6924ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pack_factor = 2\n",
    "blocksize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "776a5270-682e-441b-aa58-6ec8e92cfb0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    }
   ],
   "source": [
    "for n,p in model.state_dict().items():\n",
    "    if any(l in n for l in quantized_layers) and \"weight\" in n:\n",
    "        # output_size x input_size\n",
    "        print(n, p.shape, p.t().shape)\n",
    "        param = Params4bit(p.t(), quant_type=\"nf4\", blocksize=blocksize, compress_statistics=False, quant_storage=torch.uint8)\n",
    "        input_size, output_size = p.t().shape\n",
    "        param.cuda();\n",
    "\n",
    "        # reshape for tensor parallelism\n",
    "        qweight, absmax = param.data.cpu(), param.quant_state.absmax.cpu()        \n",
    "        qweight = qweight.reshape(input_size, output_size // pack_factor)\n",
    "        absmax = absmax.reshape(input_size, output_size // blocksize)\n",
    "                \n",
    "        quantized_state_dict[n] = qweight\n",
    "        quantized_state_dict[n.replace(\".weight\", \".absmax\")] = absmax\n",
    "\n",
    "        param = None\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07d917df-7614-41b5-bac7-c85fb73c9ef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save quantized weights\n",
    "save_file(quantized_state_dict, model_dir/\"model_state_dict.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4857052b-6de3-44ca-be4a-b36e77a71817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save quantization config\n",
    "quant_config_filename = model_dir/\"quantize_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6130b09-c496-40b5-b022-fbd6237c994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config_dict = {\n",
    "    \"weight_bits\" : 4,\n",
    "    \"blocksize\" : 64,\n",
    "    \"quant_type\" : \"nf4\",\n",
    "    \"quant_storage\" : \"uint8\",\n",
    "    \"compress_statistics\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "772b7e27-53ec-4a22-8970-d2a455e580f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(quant_config_filename, \"w+\") as f: json.dump(quant_config_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6324272-3832-45bb-95da-f98a992937fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10e9d27d-c2d9-48e5-8e75-3f1ddb1b2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model config\n",
    "model_config_filename = model_dir/\"config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "863d3eb1-d461-4c6c-9c5b-df04a7addb8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(model_config_filename, \"w+\") as f: json.dump(model_config.to_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a28ac0-9d96-46a6-a0e8-a28211728cb4",
   "metadata": {},
   "source": [
    "### BNB Quantized VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4eca361-0895-4858-ad46-c705ff64451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import safetensors\n",
    "import safetensors.torch\n",
    "from pathlib import Path\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.functional import dequantize_4bit, QuantState\n",
    "from bitsandbytes.nn.modules import Params4bit\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5951483a-d5d6-4c82-bab0-c07d868f8c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29255580-2318-4525-91c3-db40b45ca86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0268c03d2fb04644a622af82db40bd3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c509ba59-3540-4947-b9ea-5783fd6c0358",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/home/ubuntu/models/llama-7b-hf-nf4-quantized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a0b76b-96c4-404e-ade5-29002536bb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-02 17:52:17 config.py:208] bnb quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 04-02 17:52:17 llm_engine.py:70] Initializing an LLM engine (v0.3.3) with config: model='/home/ubuntu/models/llama-7b-hf-nf4-quantized', tokenizer='meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float32, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=bnb, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 04-02 17:52:18 pynccl_utils.py:13] vLLM is using nccl==2.18.1\n",
      "INFO 04-02 17:52:18 selector.py:37] Cannot use FlashAttention backend for dtype other than torch.float16 or torch.bfloat16.\n",
      "INFO 04-02 17:52:18 selector.py:20] Using XFormers backend.\n",
      "INFO 04-02 17:52:22 model_runner.py:104] Loading model weights took 4.4473 GB\n",
      "INFO 04-02 17:52:25 gpu_executor.py:94] # GPU blocks: 990, # CPU blocks: 256\n",
      "INFO 04-02 17:52:27 model_runner.py:770] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-02 17:52:27 model_runner.py:774] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-02 17:52:38 model_runner.py:846] Graph capturing finished in 11 secs.\n",
      "INFO 04-02 17:52:38 block_manager_v1.py:239] disable automatic prefix caching\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_dir, tokenizer=\"meta-llama/Llama-2-7b-hf\", dtype=\"float32\", quantization=\"bnb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "803928ae-f941-469e-bf0f-4d777c56e8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(['I'], sampling_params=SamplingParams(max_tokens=16, temperature=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de878d0f-63c8-4c9d-a4d6-fc2e56d3c162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'ayay Vic Vic Vic Vic Vic Vic Vic South South South South South South\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e07c132-e999-4d19-89f5-9dafa94b52dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bnb'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.llm_engine.model_executor.model_config.quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16e91aa8-821a-44dd-98c2-d3671ee9e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llm.llm_engine.model_executor.driver_worker.model_runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc4e514b-cbee-43be-8ff6-cf5f63fc33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_state_dict = safetensors.torch.load_file(Path(model_dir)/\"model_state_dict.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1719f27a-ae03-492b-a3cd-0b734c4c4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_state_dict = hf_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b1dc898-a330-4ea0-93c4-f1b119e90b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_map = torch.tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
    "                           0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000]).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "652b0aae-febc-4189-ac62-803967d4af05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): VocabParallelEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (qkv_proj): QKVParallelLinear()\n",
       "          (o_proj): RowParallelLinear()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (attn): Attention()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_up_proj): MergedColumnParallelLinear()\n",
       "          (down_proj): RowParallelLinear()\n",
       "          (act_fn): SiluAndMul()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): ParallelLMHead()\n",
       "  (logits_processor): LogitsProcessor()\n",
       "  (sampler): Sampler()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cbda28c-cab0-476e-924b-6a6cced74626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quant_state(absmax, shape):\n",
    "    quant_state = QuantState(absmax, dtype=torch.bfloat16)\n",
    "    quant_state.shape = torch.Size(shape)\n",
    "    quant_state.blocksize = 64\n",
    "    quant_state.quant_type = \"nf4\"\n",
    "    quant_state.code = quant_map\n",
    "    return quant_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3ecb93c-cce9-44a7-91a8-a5f3e7aa1dff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cheking: model.embed_tokens.weight\n",
      "Cheking: model.layers.0.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.0.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.0.self_attn.o_proj.weight\n",
      "Cheking: model.layers.0.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.0.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.0.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.0.mlp.down_proj.weight\n",
      "Cheking: model.layers.0.mlp.down_proj.absmax\n",
      "Cheking: model.layers.0.input_layernorm.weight\n",
      "Cheking: model.layers.0.post_attention_layernorm.weight\n",
      "Cheking: model.layers.1.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.1.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.1.self_attn.o_proj.weight\n",
      "Cheking: model.layers.1.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.1.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.1.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.1.mlp.down_proj.weight\n",
      "Cheking: model.layers.1.mlp.down_proj.absmax\n",
      "Cheking: model.layers.1.input_layernorm.weight\n",
      "Cheking: model.layers.1.post_attention_layernorm.weight\n",
      "Cheking: model.layers.2.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.2.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.2.self_attn.o_proj.weight\n",
      "Cheking: model.layers.2.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.2.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.2.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.2.mlp.down_proj.weight\n",
      "Cheking: model.layers.2.mlp.down_proj.absmax\n",
      "Cheking: model.layers.2.input_layernorm.weight\n",
      "Cheking: model.layers.2.post_attention_layernorm.weight\n",
      "Cheking: model.layers.3.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.3.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.3.self_attn.o_proj.weight\n",
      "Cheking: model.layers.3.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.3.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.3.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.3.mlp.down_proj.weight\n",
      "Cheking: model.layers.3.mlp.down_proj.absmax\n",
      "Cheking: model.layers.3.input_layernorm.weight\n",
      "Cheking: model.layers.3.post_attention_layernorm.weight\n",
      "Cheking: model.layers.4.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.4.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.4.self_attn.o_proj.weight\n",
      "Cheking: model.layers.4.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.4.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.4.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.4.mlp.down_proj.weight\n",
      "Cheking: model.layers.4.mlp.down_proj.absmax\n",
      "Cheking: model.layers.4.input_layernorm.weight\n",
      "Cheking: model.layers.4.post_attention_layernorm.weight\n",
      "Cheking: model.layers.5.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.5.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.5.self_attn.o_proj.weight\n",
      "Cheking: model.layers.5.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.5.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.5.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.5.mlp.down_proj.weight\n",
      "Cheking: model.layers.5.mlp.down_proj.absmax\n",
      "Cheking: model.layers.5.input_layernorm.weight\n",
      "Cheking: model.layers.5.post_attention_layernorm.weight\n",
      "Cheking: model.layers.6.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.6.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.6.self_attn.o_proj.weight\n",
      "Cheking: model.layers.6.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.6.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.6.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.6.mlp.down_proj.weight\n",
      "Cheking: model.layers.6.mlp.down_proj.absmax\n",
      "Cheking: model.layers.6.input_layernorm.weight\n",
      "Cheking: model.layers.6.post_attention_layernorm.weight\n",
      "Cheking: model.layers.7.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.7.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.7.self_attn.o_proj.weight\n",
      "Cheking: model.layers.7.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.7.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.7.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.7.mlp.down_proj.weight\n",
      "Cheking: model.layers.7.mlp.down_proj.absmax\n",
      "Cheking: model.layers.7.input_layernorm.weight\n",
      "Cheking: model.layers.7.post_attention_layernorm.weight\n",
      "Cheking: model.layers.8.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.8.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.8.self_attn.o_proj.weight\n",
      "Cheking: model.layers.8.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.8.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.8.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.8.mlp.down_proj.weight\n",
      "Cheking: model.layers.8.mlp.down_proj.absmax\n",
      "Cheking: model.layers.8.input_layernorm.weight\n",
      "Cheking: model.layers.8.post_attention_layernorm.weight\n",
      "Cheking: model.layers.9.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.9.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.9.self_attn.o_proj.weight\n",
      "Cheking: model.layers.9.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.9.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.9.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.9.mlp.down_proj.weight\n",
      "Cheking: model.layers.9.mlp.down_proj.absmax\n",
      "Cheking: model.layers.9.input_layernorm.weight\n",
      "Cheking: model.layers.9.post_attention_layernorm.weight\n",
      "Cheking: model.layers.10.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.10.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.10.self_attn.o_proj.weight\n",
      "Cheking: model.layers.10.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.10.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.10.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.10.mlp.down_proj.weight\n",
      "Cheking: model.layers.10.mlp.down_proj.absmax\n",
      "Cheking: model.layers.10.input_layernorm.weight\n",
      "Cheking: model.layers.10.post_attention_layernorm.weight\n",
      "Cheking: model.layers.11.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.11.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.11.self_attn.o_proj.weight\n",
      "Cheking: model.layers.11.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.11.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.11.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.11.mlp.down_proj.weight\n",
      "Cheking: model.layers.11.mlp.down_proj.absmax\n",
      "Cheking: model.layers.11.input_layernorm.weight\n",
      "Cheking: model.layers.11.post_attention_layernorm.weight\n",
      "Cheking: model.layers.12.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.12.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.12.self_attn.o_proj.weight\n",
      "Cheking: model.layers.12.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.12.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.12.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.12.mlp.down_proj.weight\n",
      "Cheking: model.layers.12.mlp.down_proj.absmax\n",
      "Cheking: model.layers.12.input_layernorm.weight\n",
      "Cheking: model.layers.12.post_attention_layernorm.weight\n",
      "Cheking: model.layers.13.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.13.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.13.self_attn.o_proj.weight\n",
      "Cheking: model.layers.13.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.13.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.13.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.13.mlp.down_proj.weight\n",
      "Cheking: model.layers.13.mlp.down_proj.absmax\n",
      "Cheking: model.layers.13.input_layernorm.weight\n",
      "Cheking: model.layers.13.post_attention_layernorm.weight\n",
      "Cheking: model.layers.14.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.14.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.14.self_attn.o_proj.weight\n",
      "Cheking: model.layers.14.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.14.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.14.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.14.mlp.down_proj.weight\n",
      "Cheking: model.layers.14.mlp.down_proj.absmax\n",
      "Cheking: model.layers.14.input_layernorm.weight\n",
      "Cheking: model.layers.14.post_attention_layernorm.weight\n",
      "Cheking: model.layers.15.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.15.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.15.self_attn.o_proj.weight\n",
      "Cheking: model.layers.15.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.15.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.15.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.15.mlp.down_proj.weight\n",
      "Cheking: model.layers.15.mlp.down_proj.absmax\n",
      "Cheking: model.layers.15.input_layernorm.weight\n",
      "Cheking: model.layers.15.post_attention_layernorm.weight\n",
      "Cheking: model.layers.16.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.16.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.16.self_attn.o_proj.weight\n",
      "Cheking: model.layers.16.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.16.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.16.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.16.mlp.down_proj.weight\n",
      "Cheking: model.layers.16.mlp.down_proj.absmax\n",
      "Cheking: model.layers.16.input_layernorm.weight\n",
      "Cheking: model.layers.16.post_attention_layernorm.weight\n",
      "Cheking: model.layers.17.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.17.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.17.self_attn.o_proj.weight\n",
      "Cheking: model.layers.17.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.17.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.17.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.17.mlp.down_proj.weight\n",
      "Cheking: model.layers.17.mlp.down_proj.absmax\n",
      "Cheking: model.layers.17.input_layernorm.weight\n",
      "Cheking: model.layers.17.post_attention_layernorm.weight\n",
      "Cheking: model.layers.18.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.18.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.18.self_attn.o_proj.weight\n",
      "Cheking: model.layers.18.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.18.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.18.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.18.mlp.down_proj.weight\n",
      "Cheking: model.layers.18.mlp.down_proj.absmax\n",
      "Cheking: model.layers.18.input_layernorm.weight\n",
      "Cheking: model.layers.18.post_attention_layernorm.weight\n",
      "Cheking: model.layers.19.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.19.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.19.self_attn.o_proj.weight\n",
      "Cheking: model.layers.19.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.19.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.19.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.19.mlp.down_proj.weight\n",
      "Cheking: model.layers.19.mlp.down_proj.absmax\n",
      "Cheking: model.layers.19.input_layernorm.weight\n",
      "Cheking: model.layers.19.post_attention_layernorm.weight\n",
      "Cheking: model.layers.20.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.20.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.20.self_attn.o_proj.weight\n",
      "Cheking: model.layers.20.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.20.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.20.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.20.mlp.down_proj.weight\n",
      "Cheking: model.layers.20.mlp.down_proj.absmax\n",
      "Cheking: model.layers.20.input_layernorm.weight\n",
      "Cheking: model.layers.20.post_attention_layernorm.weight\n",
      "Cheking: model.layers.21.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.21.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.21.self_attn.o_proj.weight\n",
      "Cheking: model.layers.21.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.21.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.21.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.21.mlp.down_proj.weight\n",
      "Cheking: model.layers.21.mlp.down_proj.absmax\n",
      "Cheking: model.layers.21.input_layernorm.weight\n",
      "Cheking: model.layers.21.post_attention_layernorm.weight\n",
      "Cheking: model.layers.22.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.22.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.22.self_attn.o_proj.weight\n",
      "Cheking: model.layers.22.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.22.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.22.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.22.mlp.down_proj.weight\n",
      "Cheking: model.layers.22.mlp.down_proj.absmax\n",
      "Cheking: model.layers.22.input_layernorm.weight\n",
      "Cheking: model.layers.22.post_attention_layernorm.weight\n",
      "Cheking: model.layers.23.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.23.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.23.self_attn.o_proj.weight\n",
      "Cheking: model.layers.23.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.23.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.23.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.23.mlp.down_proj.weight\n",
      "Cheking: model.layers.23.mlp.down_proj.absmax\n",
      "Cheking: model.layers.23.input_layernorm.weight\n",
      "Cheking: model.layers.23.post_attention_layernorm.weight\n",
      "Cheking: model.layers.24.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.24.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.24.self_attn.o_proj.weight\n",
      "Cheking: model.layers.24.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.24.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.24.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.24.mlp.down_proj.weight\n",
      "Cheking: model.layers.24.mlp.down_proj.absmax\n",
      "Cheking: model.layers.24.input_layernorm.weight\n",
      "Cheking: model.layers.24.post_attention_layernorm.weight\n",
      "Cheking: model.layers.25.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.25.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.25.self_attn.o_proj.weight\n",
      "Cheking: model.layers.25.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.25.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.25.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.25.mlp.down_proj.weight\n",
      "Cheking: model.layers.25.mlp.down_proj.absmax\n",
      "Cheking: model.layers.25.input_layernorm.weight\n",
      "Cheking: model.layers.25.post_attention_layernorm.weight\n",
      "Cheking: model.layers.26.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.26.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.26.self_attn.o_proj.weight\n",
      "Cheking: model.layers.26.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.26.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.26.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.26.mlp.down_proj.weight\n",
      "Cheking: model.layers.26.mlp.down_proj.absmax\n",
      "Cheking: model.layers.26.input_layernorm.weight\n",
      "Cheking: model.layers.26.post_attention_layernorm.weight\n",
      "Cheking: model.layers.27.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.27.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.27.self_attn.o_proj.weight\n",
      "Cheking: model.layers.27.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.27.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.27.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.27.mlp.down_proj.weight\n",
      "Cheking: model.layers.27.mlp.down_proj.absmax\n",
      "Cheking: model.layers.27.input_layernorm.weight\n",
      "Cheking: model.layers.27.post_attention_layernorm.weight\n",
      "Cheking: model.layers.28.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.28.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.28.self_attn.o_proj.weight\n",
      "Cheking: model.layers.28.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.28.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.28.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.28.mlp.down_proj.weight\n",
      "Cheking: model.layers.28.mlp.down_proj.absmax\n",
      "Cheking: model.layers.28.input_layernorm.weight\n",
      "Cheking: model.layers.28.post_attention_layernorm.weight\n",
      "Cheking: model.layers.29.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.29.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.29.self_attn.o_proj.weight\n",
      "Cheking: model.layers.29.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.29.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.29.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.29.mlp.down_proj.weight\n",
      "Cheking: model.layers.29.mlp.down_proj.absmax\n",
      "Cheking: model.layers.29.input_layernorm.weight\n",
      "Cheking: model.layers.29.post_attention_layernorm.weight\n",
      "Cheking: model.layers.30.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.30.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.30.self_attn.o_proj.weight\n",
      "Cheking: model.layers.30.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.30.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.30.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.30.mlp.down_proj.weight\n",
      "Cheking: model.layers.30.mlp.down_proj.absmax\n",
      "Cheking: model.layers.30.input_layernorm.weight\n",
      "Cheking: model.layers.30.post_attention_layernorm.weight\n",
      "Cheking: model.layers.31.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.31.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.31.self_attn.o_proj.weight\n",
      "Cheking: model.layers.31.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.31.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.31.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.31.mlp.down_proj.weight\n",
      "Cheking: model.layers.31.mlp.down_proj.absmax\n",
      "Cheking: model.layers.31.input_layernorm.weight\n",
      "Cheking: model.layers.31.post_attention_layernorm.weight\n",
      "Cheking: model.norm.weight\n",
      "Cheking: lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check if weights are loaded correctly into vllm model.\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "\n",
    "    print(\"Cheking:\", n)\n",
    "    \n",
    "    if 'qkv_proj' in n:\n",
    "        if 'absmax' in n: continue\n",
    "        \n",
    "        # Loaded qkv\n",
    "        qkv_weight = model.get_parameter(n)\n",
    "        qkv_absmax = model.get_parameter(n.replace(\".weight\", \".absmax\"))\n",
    "        qkv_shape = [qkv_weight.shape[0], qkv_weight.shape[1] * 2]\n",
    "        q_shape   = [qkv_weight.shape[0], qkv_weight.shape[1] * 2 // 3]\n",
    "        \n",
    "        absmax = qkv_absmax.contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, qkv_shape)\n",
    "        \n",
    "        W_dq = bnb.functional.dequantize_4bit(qkv_weight.contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "        # Saved q proj\n",
    "        q_proj_weight_name = n.replace(\"qkv_proj\", \"q_proj\")\n",
    "        q_proj_absmax_name = n.replace(\"qkv_proj\", \"q_proj\").replace(\".weight\", \".absmax\")\n",
    "        \n",
    "        absmax = quantized_state_dict[q_proj_absmax_name].cuda().contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, q_shape)\n",
    "        \n",
    "        W_q_proj_dq = bnb.functional.dequantize_4bit(quantized_state_dict[q_proj_weight_name].contiguous().view(-1,1).cuda(), quant_state=quant_state)\n",
    "\n",
    "        # Compare with HF model state dict\n",
    "        param = Params4bit(hf_state_dict[q_proj_weight_name].t(), blocksize=64, compress_statistics=False, quant_type='nf4').cuda()\n",
    "        W_q_proj_dq_hf = dequantize_4bit(param.data, param.quant_state)\n",
    "        \n",
    "        # Saved k proj\n",
    "        k_proj_weight_name = n.replace(\"qkv_proj\", \"k_proj\")\n",
    "        k_proj_absmax_name = n.replace(\"qkv_proj\", \"k_proj\").replace(\".weight\", \".absmax\")\n",
    "        \n",
    "        absmax = quantized_state_dict[k_proj_absmax_name].cuda().contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, q_shape)\n",
    "        \n",
    "        W_k_proj_dq = bnb.functional.dequantize_4bit(quantized_state_dict[k_proj_weight_name].contiguous().view(-1,1).cuda(), quant_state=quant_state)\n",
    "\n",
    "        # Compare with HF model state dict\n",
    "        param = Params4bit(hf_state_dict[k_proj_weight_name].t(), blocksize=64, compress_statistics=False, quant_type='nf4').cuda()\n",
    "        W_k_proj_dq_hf = dequantize_4bit(param.data, param.quant_state)        \n",
    "\n",
    "        # Saved v proj\n",
    "        v_proj_weight_name = n.replace(\"qkv_proj\", \"v_proj\")\n",
    "        v_proj_absmax_name = n.replace(\"qkv_proj\", \"v_proj\").replace(\".weight\", \".absmax\")\n",
    "\n",
    "        absmax = quantized_state_dict[v_proj_absmax_name].cuda().contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, q_shape)\n",
    "\n",
    "        W_v_proj_dq = bnb.functional.dequantize_4bit(quantized_state_dict[v_proj_weight_name].contiguous().view(-1,1).cuda(), quant_state=quant_state)\n",
    "\n",
    "         # Compare with HF model state dict\n",
    "        param = Params4bit(hf_state_dict[v_proj_weight_name].t(), blocksize=64, compress_statistics=False, quant_type='nf4').cuda()\n",
    "        W_v_proj_dq_hf = dequantize_4bit(param.data, param.quant_state)       \n",
    "\n",
    "        # Check whether loaded vs saved weights are equal after dequantization.\n",
    "        assert torch.equal(W_dq, torch.cat([W_q_proj_dq, W_k_proj_dq, W_v_proj_dq], dim=1))\n",
    "\n",
    "        assert torch.equal(W_dq, torch.cat([W_q_proj_dq_hf, W_k_proj_dq_hf, W_v_proj_dq_hf], dim=1))\n",
    "    \n",
    "    \n",
    "    elif 'gate_up_proj' in n:\n",
    "        if 'absmax' in n: continue\n",
    "            \n",
    "        # Loaded gate_up\n",
    "        gate_up_weight = model.get_parameter(n)\n",
    "        gate_up_absmax = model.get_parameter(n.replace(\".weight\", \".absmax\"))\n",
    "        gate_up_shape = [gate_up_weight.shape[0], gate_up_weight.shape[1] * 2]\n",
    "        gate_shape    = [gate_up_weight.shape[0], gate_up_weight.shape[1] * 2 // 2]\n",
    "\n",
    "        absmax = gate_up_absmax.contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, gate_up_shape)\n",
    "        \n",
    "        W_dq = bnb.functional.dequantize_4bit(gate_up_weight.contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "        # Saved gate_proj\n",
    "        gate_proj_weight_name = n.replace(\"gate_up_proj\", \"gate_proj\")\n",
    "        gate_proj_absmax_name = n.replace(\"gate_up_proj\", \"gate_proj\").replace(\".weight\", \".absmax\")\n",
    "        \n",
    "        absmax = quantized_state_dict[gate_proj_absmax_name].cuda().contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, gate_shape)\n",
    "        \n",
    "        W_gate_proj_dq = bnb.functional.dequantize_4bit(quantized_state_dict[gate_proj_weight_name].contiguous().view(-1,1).cuda(), quant_state=quant_state)\n",
    "\n",
    "        # Saved up_proj\n",
    "        up_proj_weight_name = n.replace(\"gate_up_proj\", \"up_proj\")\n",
    "        up_proj_absmax_name = n.replace(\"gate_up_proj\", \"up_proj\").replace(\".weight\", \".absmax\")\n",
    "        \n",
    "        absmax = quantized_state_dict[up_proj_absmax_name].cuda().contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, gate_shape)\n",
    "\n",
    "        W_up_proj_dq = bnb.functional.dequantize_4bit(quantized_state_dict[up_proj_weight_name].contiguous().view(-1,1).cuda(), quant_state=quant_state)\n",
    "\n",
    "        # Check whether loaded vs saved weights are equal after dequantization.\n",
    "        assert torch.equal(W_dq, torch.cat([W_gate_proj_dq, W_up_proj_dq], dim=1))\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        # Check whether loaded vs saved weights are equal after dequantization.\n",
    "        assert torch.equal(quantized_state_dict[n].data, p.data.cpu())\n",
    "        \n",
    "        # Loaded gate_up\n",
    "        if any(l in n for l in [\"o_proj\", \"down_proj\"]):\n",
    "            if \"weight\" in n:\n",
    "                weight = model.get_parameter(n)\n",
    "                absmax = model.get_parameter(n.replace(\".weight\", \".absmax\"))\n",
    "                shape = [weight.shape[0], weight.shape[1] * 2]\n",
    "                absmax = absmax.contiguous().view(-1)\n",
    "                quant_state = create_quant_state(absmax, shape)\n",
    "                W_dq = bnb.functional.dequantize_4bit(weight.contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "                # Compare with HF model state dict\n",
    "                param = Params4bit(hf_state_dict[n].t(), blocksize=64, compress_statistics=False, quant_type='nf4').cuda()\n",
    "                assert torch.equal(W_dq, dequantize_4bit(param.data, param.quant_state))\n",
    "                \n",
    "        else:\n",
    "            # Compare with HF model state dict\n",
    "            assert torch.equal(quantized_state_dict[n].data, hf_state_dict[n])\n",
    "\n",
    "    \n",
    "    if any(l in n for l in [\"qkv_proj\", \"o_proj\", \"gate_up_proj\", \"down_proj\"]) and \"weight\" in n:\n",
    "        module = model.get_submodule(n.rpartition(\".\")[0])\n",
    "        input_size = module.weight.shape[0]\n",
    "        x = torch.randn(1,input_size).cuda().to(torch.bfloat16)\n",
    "        out1 = module(x)\n",
    "        if len(out1) > 1: out1 = out1[0]\n",
    "        out2 = x @ W_dq\n",
    "        \n",
    "        # Check forward pass is correct.\n",
    "        assert torch.equal(out1, out2)\n",
    "\n",
    "    # print(p.view(-1)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3e381a01-7457-4cf6-8994-bc9561af3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check activations VLLM bnb vs HF bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207aba1d-b5e5-42cb-b136-bd2fcdb06694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74e0f2-b15e-4457-8ee2-85e67325aad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2aa61-6684-4ce2-b8c7-c31a3a500697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c4344-3699-4f0f-b3a5-ed2045c2c14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde15183-ee7a-4c32-9406-8a5b4238f79d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32091d7a-06b4-488f-a37d-21d37fd1a59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e6dd3-f798-4ea0-8642-6448cffacaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b433344-8ff8-432c-8638-776c9e2545c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258bca3-acae-4213-b12b-d53b6a201751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5694bb7-06bf-4b63-9ddf-f633e3a3f690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279b99cc-49cf-4b7e-a5c7-95c7f98bcba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
