{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22d8c79-3add-4d49-9cae-e212b493467e",
   "metadata": {},
   "source": [
    "### BNB with Tensor Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff26655-6d13-479a-9af8-9d64083ec9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from bitsandbytes.nn.modules import Params4bit, Linear4bit\n",
    "from bitsandbytes.functional import dequantize_4bit\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.functional import dequantize_4bit, QuantState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c906c609-2777-4148-acac-700d72842b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea59055-c341-47c3-86b9-f48415d239d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocksize = 64\n",
    "quant_type = \"nf4\"\n",
    "quant_storage = torch.uint8\n",
    "\n",
    "data = torch.randn(128,256).to(torch.bfloat16)\n",
    "param = Params4bit(data, blocksize=blocksize, quant_type=quant_type, \n",
    "                   quant_storage=quant_storage, compress_statistics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b7929ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if quant_storage == torch.uint8:\n",
    "    pack_factor = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6890d631-c63f-4939-a5f4-00883a324aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Parameter(Params4bit([[-0.1260, -1.6250,  0.0508,  ...,  0.3379, -1.7500,  1.1484],\n",
       "            [-0.2305, -1.3047, -2.0312,  ..., -0.1328, -0.3125,  1.8594],\n",
       "            [-1.1875,  0.9766,  0.0840,  ..., -2.2812,  0.6016,  1.1328],\n",
       "            ...,\n",
       "            [ 1.1875,  1.0234,  1.3281,  ...,  1.5781, -0.7227,  0.5156],\n",
       "            [ 0.0610, -0.3477,  1.2578,  ..., -1.7109,  1.0781,  0.6914],\n",
       "            [ 0.2002, -0.7422,  0.0527,  ...,  1.1953, -0.0952,  0.0854]],\n",
       "           dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "222fae9b-6520-46de-bd43-21867026cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "920862c7-0c28-43cf-ba29-5a624eadaace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16384, 1]), 'nf4', torch.uint8, 64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.shape, param.quant_type, param.quant_storage, param.blocksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78e79d3e-ca1c-49e8-8da1-8578b346d2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a273b007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.numel() / data.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a3b663b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_type': 'nf4',\n",
       " 'absmax': tensor([2.6406, 2.4062, 3.7812, 2.2344, 2.7500, 3.2500, 2.2500, 2.8594, 2.7656,\n",
       "         2.4688, 3.2969, 3.4844, 2.7812, 2.2188, 2.9375, 3.2812, 2.1094, 2.8125,\n",
       "         2.2344, 3.2344, 3.2188, 2.8125, 2.3906, 2.2812, 2.7031, 3.5938, 2.5156,\n",
       "         2.9219, 2.1875, 3.0469, 2.5781, 2.5781, 2.2188, 2.8125, 2.3281, 2.6094,\n",
       "         2.9219, 2.7344, 2.1719, 3.4375, 2.5000, 3.0625, 2.4688, 2.5625, 2.9531,\n",
       "         2.8125, 2.7969, 2.2500, 3.6094, 2.4062, 2.0781, 2.4531, 3.3281, 2.6250,\n",
       "         3.1250, 2.7969, 2.8594, 2.4844, 2.8906, 2.5000, 2.1719, 3.1406, 2.6094,\n",
       "         2.4375, 3.0000, 2.2188, 3.4062, 3.9688, 2.6250, 2.5938, 2.2500, 3.0000,\n",
       "         2.7188, 2.5625, 2.5156, 3.3750, 2.5469, 2.8125, 3.0781, 2.4688, 2.5781,\n",
       "         3.3281, 2.5781, 1.7109, 2.9375, 2.0781, 2.2031, 2.3750, 2.0312, 2.3750,\n",
       "         2.4688, 2.6406, 2.2812, 2.2344, 3.2031, 2.5312, 1.8203, 2.3438, 2.0625,\n",
       "         2.7500, 2.5312, 2.6094, 2.4844, 2.7188, 2.6562, 2.1094, 1.8594, 2.4688,\n",
       "         2.2188, 3.0312, 3.4062, 2.2031, 2.2188, 2.0000, 2.8438, 2.5156, 2.3594,\n",
       "         1.9844, 2.9531, 2.7344, 2.8750, 2.0156, 2.1406, 2.9844, 3.0312, 2.7344,\n",
       "         2.2656, 2.5938, 3.5625, 2.8750, 1.8672, 2.4531, 1.8672, 2.8281, 2.2500,\n",
       "         2.0312, 2.9375, 2.1719, 1.9531, 2.0938, 2.5625, 3.2812, 2.4375, 2.8750,\n",
       "         2.9062, 2.0469, 2.7188, 2.5000, 2.7500, 2.1094, 1.7344, 2.3750, 2.8281,\n",
       "         3.1250, 3.2344, 2.8906, 2.2344, 1.9844, 4.2812, 2.6562, 2.5938, 2.3750,\n",
       "         2.8750, 2.1719, 3.1250, 2.9219, 3.5469, 2.4844, 2.6250, 1.9922, 2.3125,\n",
       "         2.4844, 2.7656, 2.3281, 2.7969, 3.2656, 2.2188, 2.1875, 2.2500, 2.3906,\n",
       "         2.2344, 2.5156, 3.3125, 1.9922, 1.7344, 2.6406, 2.1406, 2.5000, 2.1562,\n",
       "         2.5469, 2.1875, 1.9219, 1.9609, 2.4219, 2.8438, 2.0938, 2.9062, 1.8750,\n",
       "         2.6719, 2.5312, 3.2969, 2.6250, 2.2812, 2.4219, 2.1875, 2.6719, 3.2656,\n",
       "         2.3906, 1.8281, 2.4844, 2.8438, 2.5156, 2.6562, 2.4219, 2.0000, 2.8906,\n",
       "         2.5312, 2.7188, 2.7500, 3.4375, 2.6094, 2.7500, 1.7734, 2.1250, 2.4531,\n",
       "         2.4688, 2.7812, 2.5469, 2.0312, 1.8906, 3.6406, 2.3281, 2.6094, 2.5938,\n",
       "         2.1562, 2.4531, 2.1719, 2.1406, 2.2969, 2.1406, 3.3906, 2.3594, 2.2812,\n",
       "         2.6250, 2.7812, 2.5938, 2.6562, 2.6250, 2.3750, 3.3750, 2.2500, 2.1719,\n",
       "         2.8750, 2.1719, 2.4375, 2.4062, 2.5469, 2.9688, 2.1719, 1.8906, 2.2812,\n",
       "         2.2969, 2.6875, 2.7656, 3.5312, 2.4844, 2.2812, 3.7656, 3.4375, 2.9531,\n",
       "         2.2031, 2.7812, 3.0781, 2.5625, 2.1875, 2.1562, 3.2344, 2.3438, 2.2969,\n",
       "         2.1875, 2.4844, 2.6719, 2.1562, 2.2969, 2.7812, 3.2969, 1.9688, 2.1719,\n",
       "         2.9219, 1.9609, 3.2969, 3.0312, 2.6562, 2.8438, 2.7656, 2.5156, 2.5312,\n",
       "         2.4375, 2.8750, 3.1250, 2.0938, 2.5312, 2.4062, 2.5938, 3.1406, 2.2812,\n",
       "         2.7188, 3.2812, 3.0781, 2.8750, 2.3906, 2.2344, 2.4062, 1.9922, 3.5312,\n",
       "         2.6250, 2.0625, 2.9375, 2.1875, 2.4062, 2.7500, 2.7188, 2.1250, 2.3125,\n",
       "         3.2656, 2.3281, 2.9375, 2.6719, 2.2188, 2.4844, 2.5781, 2.4375, 3.7031,\n",
       "         3.4844, 2.7188, 2.4688, 2.2969, 2.9219, 3.0000, 1.9219, 2.0312, 2.8125,\n",
       "         1.9766, 3.5938, 2.9844, 2.9219, 2.0938, 2.2188, 2.7188, 2.6094, 2.6406,\n",
       "         2.6562, 2.4688, 2.0156, 3.0156, 3.0625, 2.1250, 2.7188, 2.5000, 2.9531,\n",
       "         2.7188, 2.4375, 2.7031, 2.7188, 2.8594, 2.6719, 2.0156, 2.3281, 2.8750,\n",
       "         2.1562, 2.5312, 2.9062, 2.1250, 2.5938, 2.4688, 2.7812, 2.9219, 2.2188,\n",
       "         1.8516, 2.5938, 3.1406, 2.6719, 1.7656, 2.6250, 2.0156, 2.6875, 2.2500,\n",
       "         3.1719, 2.5000, 3.1250, 1.6875, 3.0000, 2.0625, 2.7500, 2.9844, 2.4844,\n",
       "         2.9375, 3.5000, 1.8906, 2.4219, 2.1406, 3.0312, 2.8125, 1.9922, 2.1562,\n",
       "         1.7656, 2.3281, 2.6094, 2.4531, 2.7500, 2.5156, 2.3281, 3.1875, 1.6953,\n",
       "         2.4219, 3.2812, 2.3438, 2.3281, 3.0781, 2.5000, 2.6719, 2.2812, 1.8281,\n",
       "         2.0781, 2.7656, 2.8438, 2.3594, 2.1875, 1.9297, 2.6562, 2.7344, 2.3594,\n",
       "         2.9062, 2.2969, 3.0469, 2.9531, 2.8281, 3.0625, 2.3438, 2.0000, 2.9219,\n",
       "         3.1875, 2.3906, 2.7344, 2.1719, 2.9219, 2.3906, 3.0469, 2.0938, 2.2031,\n",
       "         2.8906, 2.5156, 2.3125, 1.7734, 3.5156, 2.4688, 3.5000, 2.2969, 2.3750,\n",
       "         2.2188, 2.5625, 2.4375, 2.1250, 2.8750, 2.7031, 2.8594, 2.3125, 3.4531,\n",
       "         3.0938, 2.6094, 2.7656, 2.9062, 2.4375, 2.7031, 2.5781, 2.6094, 2.2812,\n",
       "         2.9375, 2.2031, 3.0156, 2.2812, 2.5625, 3.1406, 2.4062, 1.9062, 2.8438,\n",
       "         2.4375, 2.5156, 2.7031, 2.1406, 3.0469, 2.5469, 2.0469, 2.4062, 2.2344,\n",
       "         2.2188, 2.3594, 2.3438, 1.7656, 1.8984, 2.7656, 2.0938, 2.8125, 2.5625,\n",
       "         2.0781, 2.5312, 3.1562, 2.5000, 2.3438, 2.4062, 2.9375, 2.5312],\n",
       "        device='cuda:0'),\n",
       " 'blocksize': 64,\n",
       " 'quant_map': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
       "          0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
       "        device='cuda:0'),\n",
       " 'dtype': 'bfloat16',\n",
       " 'shape': (128, 256)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.quant_state.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b110f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size_per_partition = 64\n",
    "output_size_per_partition = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c5479f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row-major quantization, reshape for vllm tensor parallelism\n",
    "qweight = param.data.reshape(data.size(0), data.size(1) // pack_factor); qweight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd34c2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 97, 119,  55,  ...,  65, 201,  29],\n",
       "        [ 98,  24,  97,  ..., 156, 134, 110],\n",
       "        [ 59, 112,  71,  ...,  58,  81, 155],\n",
       "        ...,\n",
       "        [203, 201, 227,  ...,  20, 173,  73],\n",
       "        [117, 217, 181,  ...,  65,  97, 202],\n",
       "        [132, 122, 134,  ...,  38,  44, 119]], device='cuda:0',\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b3a5601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 97],\n",
       "        [119],\n",
       "        [ 55],\n",
       "        ...,\n",
       "        [ 38],\n",
       "        [ 44],\n",
       "        [119]], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qweight.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11e42bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deqweight = dequantize_4bit(qweight.view(-1,1), param.quant_state, blocksize=blocksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e794c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(254., dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data - torch.randn_like(data)).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4c3eb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.5000, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data - deqweight.cpu()).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c66129c-c120-4b67-bf0c-fee3d8e94a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 128).cuda().to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed100e7d-5097-4682-9a6d-333f4434c877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_type': 'nf4',\n",
       " 'absmax': tensor([2.6406, 2.4062, 3.7812, 2.2344, 2.7500, 3.2500, 2.2500, 2.8594, 2.7656,\n",
       "         2.4688, 3.2969, 3.4844, 2.7812, 2.2188, 2.9375, 3.2812, 2.1094, 2.8125,\n",
       "         2.2344, 3.2344, 3.2188, 2.8125, 2.3906, 2.2812, 2.7031, 3.5938, 2.5156,\n",
       "         2.9219, 2.1875, 3.0469, 2.5781, 2.5781, 2.2188, 2.8125, 2.3281, 2.6094,\n",
       "         2.9219, 2.7344, 2.1719, 3.4375, 2.5000, 3.0625, 2.4688, 2.5625, 2.9531,\n",
       "         2.8125, 2.7969, 2.2500, 3.6094, 2.4062, 2.0781, 2.4531, 3.3281, 2.6250,\n",
       "         3.1250, 2.7969, 2.8594, 2.4844, 2.8906, 2.5000, 2.1719, 3.1406, 2.6094,\n",
       "         2.4375, 3.0000, 2.2188, 3.4062, 3.9688, 2.6250, 2.5938, 2.2500, 3.0000,\n",
       "         2.7188, 2.5625, 2.5156, 3.3750, 2.5469, 2.8125, 3.0781, 2.4688, 2.5781,\n",
       "         3.3281, 2.5781, 1.7109, 2.9375, 2.0781, 2.2031, 2.3750, 2.0312, 2.3750,\n",
       "         2.4688, 2.6406, 2.2812, 2.2344, 3.2031, 2.5312, 1.8203, 2.3438, 2.0625,\n",
       "         2.7500, 2.5312, 2.6094, 2.4844, 2.7188, 2.6562, 2.1094, 1.8594, 2.4688,\n",
       "         2.2188, 3.0312, 3.4062, 2.2031, 2.2188, 2.0000, 2.8438, 2.5156, 2.3594,\n",
       "         1.9844, 2.9531, 2.7344, 2.8750, 2.0156, 2.1406, 2.9844, 3.0312, 2.7344,\n",
       "         2.2656, 2.5938, 3.5625, 2.8750, 1.8672, 2.4531, 1.8672, 2.8281, 2.2500,\n",
       "         2.0312, 2.9375, 2.1719, 1.9531, 2.0938, 2.5625, 3.2812, 2.4375, 2.8750,\n",
       "         2.9062, 2.0469, 2.7188, 2.5000, 2.7500, 2.1094, 1.7344, 2.3750, 2.8281,\n",
       "         3.1250, 3.2344, 2.8906, 2.2344, 1.9844, 4.2812, 2.6562, 2.5938, 2.3750,\n",
       "         2.8750, 2.1719, 3.1250, 2.9219, 3.5469, 2.4844, 2.6250, 1.9922, 2.3125,\n",
       "         2.4844, 2.7656, 2.3281, 2.7969, 3.2656, 2.2188, 2.1875, 2.2500, 2.3906,\n",
       "         2.2344, 2.5156, 3.3125, 1.9922, 1.7344, 2.6406, 2.1406, 2.5000, 2.1562,\n",
       "         2.5469, 2.1875, 1.9219, 1.9609, 2.4219, 2.8438, 2.0938, 2.9062, 1.8750,\n",
       "         2.6719, 2.5312, 3.2969, 2.6250, 2.2812, 2.4219, 2.1875, 2.6719, 3.2656,\n",
       "         2.3906, 1.8281, 2.4844, 2.8438, 2.5156, 2.6562, 2.4219, 2.0000, 2.8906,\n",
       "         2.5312, 2.7188, 2.7500, 3.4375, 2.6094, 2.7500, 1.7734, 2.1250, 2.4531,\n",
       "         2.4688, 2.7812, 2.5469, 2.0312, 1.8906, 3.6406, 2.3281, 2.6094, 2.5938,\n",
       "         2.1562, 2.4531, 2.1719, 2.1406, 2.2969, 2.1406, 3.3906, 2.3594, 2.2812,\n",
       "         2.6250, 2.7812, 2.5938, 2.6562, 2.6250, 2.3750, 3.3750, 2.2500, 2.1719,\n",
       "         2.8750, 2.1719, 2.4375, 2.4062, 2.5469, 2.9688, 2.1719, 1.8906, 2.2812,\n",
       "         2.2969, 2.6875, 2.7656, 3.5312, 2.4844, 2.2812, 3.7656, 3.4375, 2.9531,\n",
       "         2.2031, 2.7812, 3.0781, 2.5625, 2.1875, 2.1562, 3.2344, 2.3438, 2.2969,\n",
       "         2.1875, 2.4844, 2.6719, 2.1562, 2.2969, 2.7812, 3.2969, 1.9688, 2.1719,\n",
       "         2.9219, 1.9609, 3.2969, 3.0312, 2.6562, 2.8438, 2.7656, 2.5156, 2.5312,\n",
       "         2.4375, 2.8750, 3.1250, 2.0938, 2.5312, 2.4062, 2.5938, 3.1406, 2.2812,\n",
       "         2.7188, 3.2812, 3.0781, 2.8750, 2.3906, 2.2344, 2.4062, 1.9922, 3.5312,\n",
       "         2.6250, 2.0625, 2.9375, 2.1875, 2.4062, 2.7500, 2.7188, 2.1250, 2.3125,\n",
       "         3.2656, 2.3281, 2.9375, 2.6719, 2.2188, 2.4844, 2.5781, 2.4375, 3.7031,\n",
       "         3.4844, 2.7188, 2.4688, 2.2969, 2.9219, 3.0000, 1.9219, 2.0312, 2.8125,\n",
       "         1.9766, 3.5938, 2.9844, 2.9219, 2.0938, 2.2188, 2.7188, 2.6094, 2.6406,\n",
       "         2.6562, 2.4688, 2.0156, 3.0156, 3.0625, 2.1250, 2.7188, 2.5000, 2.9531,\n",
       "         2.7188, 2.4375, 2.7031, 2.7188, 2.8594, 2.6719, 2.0156, 2.3281, 2.8750,\n",
       "         2.1562, 2.5312, 2.9062, 2.1250, 2.5938, 2.4688, 2.7812, 2.9219, 2.2188,\n",
       "         1.8516, 2.5938, 3.1406, 2.6719, 1.7656, 2.6250, 2.0156, 2.6875, 2.2500,\n",
       "         3.1719, 2.5000, 3.1250, 1.6875, 3.0000, 2.0625, 2.7500, 2.9844, 2.4844,\n",
       "         2.9375, 3.5000, 1.8906, 2.4219, 2.1406, 3.0312, 2.8125, 1.9922, 2.1562,\n",
       "         1.7656, 2.3281, 2.6094, 2.4531, 2.7500, 2.5156, 2.3281, 3.1875, 1.6953,\n",
       "         2.4219, 3.2812, 2.3438, 2.3281, 3.0781, 2.5000, 2.6719, 2.2812, 1.8281,\n",
       "         2.0781, 2.7656, 2.8438, 2.3594, 2.1875, 1.9297, 2.6562, 2.7344, 2.3594,\n",
       "         2.9062, 2.2969, 3.0469, 2.9531, 2.8281, 3.0625, 2.3438, 2.0000, 2.9219,\n",
       "         3.1875, 2.3906, 2.7344, 2.1719, 2.9219, 2.3906, 3.0469, 2.0938, 2.2031,\n",
       "         2.8906, 2.5156, 2.3125, 1.7734, 3.5156, 2.4688, 3.5000, 2.2969, 2.3750,\n",
       "         2.2188, 2.5625, 2.4375, 2.1250, 2.8750, 2.7031, 2.8594, 2.3125, 3.4531,\n",
       "         3.0938, 2.6094, 2.7656, 2.9062, 2.4375, 2.7031, 2.5781, 2.6094, 2.2812,\n",
       "         2.9375, 2.2031, 3.0156, 2.2812, 2.5625, 3.1406, 2.4062, 1.9062, 2.8438,\n",
       "         2.4375, 2.5156, 2.7031, 2.1406, 3.0469, 2.5469, 2.0469, 2.4062, 2.2344,\n",
       "         2.2188, 2.3594, 2.3438, 1.7656, 1.8984, 2.7656, 2.0938, 2.8125, 2.5625,\n",
       "         2.0781, 2.5312, 3.1562, 2.5000, 2.3438, 2.4062, 2.9375, 2.5312],\n",
       "        device='cuda:0'),\n",
       " 'blocksize': 64,\n",
       " 'quant_map': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
       "          0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
       "        device='cuda:0'),\n",
       " 'dtype': 'bfloat16',\n",
       " 'shape': (128, 256)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.quant_state.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d82d89f-235e-4a2a-b0b3-62a3e5d2adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size, output_size = data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea5b382e-e078-4a23-8a95-c5802eca1cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 256)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size, output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a33876",
   "metadata": {},
   "source": [
    "### Column Parallel\n",
    "\n",
    "The linear layer is defined as Y = XA + b. A is parallelized along its second dimension as A = [A_1, ..., A_p]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54220b7c-a24b-4052-8ccc-d0cbe04827ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5883249c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qweight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec7d2b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size_per_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8e60aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "qweight_partitioned = qweight.split(output_size_per_partition, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2f41ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qweight_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7746431f-166b-4a71-959c-83cead869e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n"
     ]
    }
   ],
   "source": [
    "for w in qweight_partitioned: print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be00bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_absmax = param.quant_state.absmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5c02e37-8815-474e-8b2e-da1414790c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_absmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3d776ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_absmax_reshaped = orig_absmax.reshape(input_size, data.size(1) // blocksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7e1670e-a677-47cc-9e67-1ea10d399e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.Size([128, 4]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_absmax_reshaped.dtype, orig_absmax_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dfd94e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = len(qweight_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f63f1e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "absmax_partitioned = orig_absmax_reshaped.split(orig_absmax_reshaped.size(1) // num_partitions, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d276291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2])\n",
      "torch.Size([128, 2])\n"
     ]
    }
   ],
   "source": [
    "for a in absmax_partitioned: print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e9668e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qweight_partitioned), len(absmax_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e7299d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state = copy.deepcopy(param.quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80b8bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state.shape = torch.Size([quant_state.shape[0], quant_state.shape[1]//num_partitions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de428e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "241a2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state.absmax = absmax_partitioned[0].contiguous().view(-1)\n",
    "deqweight_part1 = dequantize_4bit(qweight_partitioned[0].contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "quant_state.absmax = absmax_partitioned[1].contiguous().view(-1)\n",
    "deqweight_part2 = dequantize_4bit(qweight_partitioned[1].contiguous().view(-1,1), quant_state=quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e15c1671-03b7-45a4-b036-72bf3e82c43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 128]), torch.Size([128, 128]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deqweight_part1.shape, deqweight_part2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4085cb3-2b80-492a-b89b-ca259ce950c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# quant_state.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b63ec5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 256])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deqweight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "978e7a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 256])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([deqweight_part1, deqweight_part2], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d50215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(deqweight, torch.cat([deqweight_part1, deqweight_part2], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a43bafd4-7b83-4314-9019-a41499be5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = (x @ deqweight_part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f2fe31f-1af2-4712-9356-b9894c2d7e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out2 = bnb.matmul_4bit(x, qweight_partitioned[1].contiguous().view(-1,1), quant_state=quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bf8b879-052b-48f4-ba5c-d28e1672d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(out1, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af60fb46-97b8-4b65-8f65-ddb11fd02c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc17f5f4",
   "metadata": {},
   "source": [
    "### Row Parallel\n",
    "\n",
    "The linear layer is defined as Y = XA + b. A is parallelized along\n",
    "its first dimension and X along its second dimension as:\n",
    "\n",
    "```\n",
    "    -   -\n",
    "    | A_1 |\n",
    "    | .   |\n",
    "A = | .   |        X = [X_1, ..., X_p]\n",
    "    | .   |\n",
    "    | A_p |\n",
    "    -   -\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c3dc1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qweight_partitioned = qweight.split(output_size_per_partition, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98d0add2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_partitions = len(qweight_partitioned); num_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77c5bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "for w in qweight_partitioned: print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed58848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_absmax = param.quant_state.absmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e368bab4-38d8-4ee5-9ca7-8a0943fd7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_absmax_reshaped = orig_absmax.reshape(input_size, data.size(1) // blocksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b155bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "absmax_partitioned = orig_absmax.split(len(orig_absmax) // num_partitions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "130b5f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(absmax_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e64143be",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state = copy.deepcopy(param.quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "47f91077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_state.shape = torch.Size([quant_state.shape[0]//num_partitions, quant_state.shape[1]]); quant_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "128e667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state.absmax = absmax_partitioned[0].contiguous().view(-1)\n",
    "deqweight_part1 = dequantize_4bit(qweight_partitioned[0].contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "quant_state.absmax = absmax_partitioned[1].contiguous().view(-1)\n",
    "deqweight_part2 = dequantize_4bit(qweight_partitioned[1].contiguous().view(-1,1), quant_state=quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d735c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(deqweight, torch.cat([deqweight_part1, deqweight_part2], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53f04e-f053-423b-b3f7-88283531858c",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39e0da2-6d9e-4a0e-bec9-7e4007c8af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-29 13:23:10 pynccl_utils.py:13] vLLM is using nccl==2.18.1\n"
     ]
    }
   ],
   "source": [
    "from vllm.model_executor.weight_utils import default_weight_loader, hf_model_weights_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b267b53-a7d4-43af-82e2-ac18b6b66a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_iterator = hf_model_weights_iterator(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdc64de-22cd-413c-9b63-e2351e018c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-29 08:48:45 weight_utils.py:177] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd674cc5dcf7487f923f00c680e6a73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93c344b17544540b2d5f59dac5af4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for name, loaded_weight in weights_iterator: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0853623c-1c05-4599-8b74-26e55d33a310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model.embed_tokens.weight',\n",
       " tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
       "          -6.5565e-06,  8.9407e-07],\n",
       "         [ 1.8616e-03, -3.3722e-03,  3.9864e-04,  ..., -8.3008e-03,\n",
       "           2.5787e-03, -3.9368e-03],\n",
       "         [ 1.0986e-02,  9.8877e-03, -5.0964e-03,  ...,  2.5177e-03,\n",
       "           7.7057e-04, -5.0049e-03],\n",
       "         ...,\n",
       "         [-1.3977e-02, -2.7313e-03, -1.9897e-02,  ..., -1.0437e-02,\n",
       "           9.5825e-03, -1.8005e-03],\n",
       "         [-1.0742e-02,  9.3384e-03,  1.2939e-02,  ..., -3.3203e-02,\n",
       "          -1.6357e-02,  3.3875e-03],\n",
       "         [-8.3008e-03, -4.0588e-03, -1.1063e-03,  ...,  3.4790e-03,\n",
       "          -1.2939e-02,  3.1948e-05]], dtype=torch.float16))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name, loaded_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b31e4388-af56-49ee-b81e-a4d2a333bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_iterator = hf_model_weights_iterator(\"TheBloke/CodeUp-Alpha-13B-HF-AWQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b23f97a-ad91-434b-aa63-2fa548c0193d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.mlp.down_proj.qzeros\n",
      "model.layers.0.mlp.down_proj.scales\n",
      "model.layers.0.mlp.gate_proj.qzeros\n",
      "model.layers.0.mlp.gate_proj.scales\n",
      "model.layers.0.mlp.up_proj.qzeros\n",
      "model.layers.0.mlp.up_proj.scales\n",
      "model.layers.0.self_attn.k_proj.qzeros\n",
      "model.layers.0.self_attn.k_proj.scales\n",
      "model.layers.0.self_attn.o_proj.qzeros\n",
      "model.layers.0.self_attn.o_proj.scales\n",
      "model.layers.0.self_attn.q_proj.qzeros\n",
      "model.layers.0.self_attn.q_proj.scales\n",
      "model.layers.0.self_attn.v_proj.qzeros\n",
      "model.layers.0.self_attn.v_proj.scales\n",
      "model.layers.1.mlp.down_proj.qzeros\n",
      "model.layers.1.mlp.down_proj.scales\n",
      "model.layers.1.mlp.gate_proj.qzeros\n",
      "model.layers.1.mlp.gate_proj.scales\n",
      "model.layers.1.mlp.up_proj.qzeros\n",
      "model.layers.1.mlp.up_proj.scales\n",
      "model.layers.1.self_attn.k_proj.qzeros\n",
      "model.layers.1.self_attn.k_proj.scales\n",
      "model.layers.1.self_attn.o_proj.qzeros\n",
      "model.layers.1.self_attn.o_proj.scales\n",
      "model.layers.1.self_attn.q_proj.qzeros\n",
      "model.layers.1.self_attn.q_proj.scales\n",
      "model.layers.1.self_attn.v_proj.qzeros\n",
      "model.layers.1.self_attn.v_proj.scales\n",
      "model.layers.10.mlp.down_proj.qzeros\n",
      "model.layers.10.mlp.down_proj.scales\n",
      "model.layers.10.mlp.gate_proj.qzeros\n",
      "model.layers.10.mlp.gate_proj.scales\n",
      "model.layers.10.mlp.up_proj.qzeros\n",
      "model.layers.10.mlp.up_proj.scales\n",
      "model.layers.10.self_attn.k_proj.qzeros\n",
      "model.layers.10.self_attn.k_proj.scales\n",
      "model.layers.10.self_attn.o_proj.qzeros\n",
      "model.layers.10.self_attn.o_proj.scales\n",
      "model.layers.10.self_attn.q_proj.qzeros\n",
      "model.layers.10.self_attn.q_proj.scales\n",
      "model.layers.10.self_attn.v_proj.qzeros\n",
      "model.layers.10.self_attn.v_proj.scales\n",
      "model.layers.11.mlp.down_proj.qzeros\n",
      "model.layers.11.mlp.down_proj.scales\n",
      "model.layers.11.mlp.gate_proj.qzeros\n",
      "model.layers.11.mlp.gate_proj.scales\n",
      "model.layers.11.mlp.up_proj.qzeros\n",
      "model.layers.11.mlp.up_proj.scales\n",
      "model.layers.11.self_attn.k_proj.qzeros\n",
      "model.layers.11.self_attn.k_proj.scales\n",
      "model.layers.11.self_attn.o_proj.qzeros\n",
      "model.layers.11.self_attn.o_proj.scales\n",
      "model.layers.11.self_attn.q_proj.qzeros\n",
      "model.layers.11.self_attn.q_proj.scales\n",
      "model.layers.11.self_attn.v_proj.qzeros\n",
      "model.layers.11.self_attn.v_proj.scales\n",
      "model.layers.12.mlp.down_proj.qzeros\n",
      "model.layers.12.mlp.down_proj.scales\n",
      "model.layers.12.mlp.gate_proj.qzeros\n",
      "model.layers.12.mlp.gate_proj.scales\n",
      "model.layers.12.mlp.up_proj.qzeros\n",
      "model.layers.12.mlp.up_proj.scales\n",
      "model.layers.12.self_attn.k_proj.qzeros\n",
      "model.layers.12.self_attn.k_proj.scales\n",
      "model.layers.12.self_attn.o_proj.qzeros\n",
      "model.layers.12.self_attn.o_proj.scales\n",
      "model.layers.12.self_attn.q_proj.qzeros\n",
      "model.layers.12.self_attn.q_proj.scales\n",
      "model.layers.12.self_attn.v_proj.qzeros\n",
      "model.layers.12.self_attn.v_proj.scales\n",
      "model.layers.13.mlp.down_proj.qzeros\n",
      "model.layers.13.mlp.down_proj.scales\n",
      "model.layers.13.mlp.gate_proj.qzeros\n",
      "model.layers.13.mlp.gate_proj.scales\n",
      "model.layers.13.mlp.up_proj.qzeros\n",
      "model.layers.13.mlp.up_proj.scales\n",
      "model.layers.13.self_attn.k_proj.qzeros\n",
      "model.layers.13.self_attn.k_proj.scales\n",
      "model.layers.13.self_attn.o_proj.qzeros\n",
      "model.layers.13.self_attn.o_proj.scales\n",
      "model.layers.13.self_attn.q_proj.qzeros\n",
      "model.layers.13.self_attn.q_proj.scales\n",
      "model.layers.13.self_attn.v_proj.qzeros\n",
      "model.layers.13.self_attn.v_proj.scales\n",
      "model.layers.14.mlp.down_proj.qzeros\n",
      "model.layers.14.mlp.down_proj.scales\n",
      "model.layers.14.mlp.gate_proj.qzeros\n",
      "model.layers.14.mlp.gate_proj.scales\n",
      "model.layers.14.mlp.up_proj.qzeros\n",
      "model.layers.14.mlp.up_proj.scales\n",
      "model.layers.14.self_attn.k_proj.qzeros\n",
      "model.layers.14.self_attn.k_proj.scales\n",
      "model.layers.14.self_attn.o_proj.qzeros\n",
      "model.layers.14.self_attn.o_proj.scales\n",
      "model.layers.14.self_attn.q_proj.qzeros\n",
      "model.layers.14.self_attn.q_proj.scales\n",
      "model.layers.14.self_attn.v_proj.qzeros\n",
      "model.layers.14.self_attn.v_proj.scales\n",
      "model.layers.15.mlp.down_proj.qzeros\n",
      "model.layers.15.mlp.down_proj.scales\n",
      "model.layers.15.mlp.gate_proj.qzeros\n",
      "model.layers.15.mlp.gate_proj.scales\n",
      "model.layers.15.mlp.up_proj.qzeros\n",
      "model.layers.15.mlp.up_proj.scales\n",
      "model.layers.15.self_attn.k_proj.qzeros\n",
      "model.layers.15.self_attn.k_proj.scales\n",
      "model.layers.15.self_attn.o_proj.qzeros\n",
      "model.layers.15.self_attn.o_proj.scales\n",
      "model.layers.15.self_attn.q_proj.qzeros\n",
      "model.layers.15.self_attn.q_proj.scales\n",
      "model.layers.15.self_attn.v_proj.qzeros\n",
      "model.layers.15.self_attn.v_proj.scales\n",
      "model.layers.16.mlp.down_proj.qzeros\n",
      "model.layers.16.mlp.down_proj.scales\n",
      "model.layers.16.mlp.gate_proj.qzeros\n",
      "model.layers.16.mlp.gate_proj.scales\n",
      "model.layers.16.mlp.up_proj.qzeros\n",
      "model.layers.16.mlp.up_proj.scales\n",
      "model.layers.16.self_attn.k_proj.qzeros\n",
      "model.layers.16.self_attn.k_proj.scales\n",
      "model.layers.16.self_attn.o_proj.qzeros\n",
      "model.layers.16.self_attn.o_proj.scales\n",
      "model.layers.16.self_attn.q_proj.qzeros\n",
      "model.layers.16.self_attn.q_proj.scales\n",
      "model.layers.16.self_attn.v_proj.qzeros\n",
      "model.layers.16.self_attn.v_proj.scales\n",
      "model.layers.17.mlp.down_proj.qzeros\n",
      "model.layers.17.mlp.down_proj.scales\n",
      "model.layers.17.mlp.gate_proj.qzeros\n",
      "model.layers.17.mlp.gate_proj.scales\n",
      "model.layers.17.mlp.up_proj.qzeros\n",
      "model.layers.17.mlp.up_proj.scales\n",
      "model.layers.17.self_attn.k_proj.qzeros\n",
      "model.layers.17.self_attn.k_proj.scales\n",
      "model.layers.17.self_attn.o_proj.qzeros\n",
      "model.layers.17.self_attn.o_proj.scales\n",
      "model.layers.17.self_attn.q_proj.qzeros\n",
      "model.layers.17.self_attn.q_proj.scales\n",
      "model.layers.17.self_attn.v_proj.qzeros\n",
      "model.layers.17.self_attn.v_proj.scales\n",
      "model.layers.18.mlp.down_proj.qzeros\n",
      "model.layers.18.mlp.down_proj.scales\n",
      "model.layers.18.mlp.gate_proj.qzeros\n",
      "model.layers.18.mlp.gate_proj.scales\n",
      "model.layers.18.mlp.up_proj.qzeros\n",
      "model.layers.18.mlp.up_proj.scales\n",
      "model.layers.18.self_attn.k_proj.qzeros\n",
      "model.layers.18.self_attn.k_proj.scales\n",
      "model.layers.18.self_attn.o_proj.qzeros\n",
      "model.layers.18.self_attn.o_proj.scales\n",
      "model.layers.18.self_attn.q_proj.qzeros\n",
      "model.layers.18.self_attn.q_proj.scales\n",
      "model.layers.18.self_attn.v_proj.qzeros\n",
      "model.layers.18.self_attn.v_proj.scales\n",
      "model.layers.19.mlp.down_proj.qzeros\n",
      "model.layers.19.mlp.down_proj.scales\n",
      "model.layers.19.mlp.gate_proj.qzeros\n",
      "model.layers.19.mlp.gate_proj.scales\n",
      "model.layers.19.mlp.up_proj.qzeros\n",
      "model.layers.19.mlp.up_proj.scales\n",
      "model.layers.19.self_attn.k_proj.qzeros\n",
      "model.layers.19.self_attn.k_proj.scales\n",
      "model.layers.19.self_attn.o_proj.qzeros\n",
      "model.layers.19.self_attn.o_proj.scales\n",
      "model.layers.19.self_attn.q_proj.qzeros\n",
      "model.layers.19.self_attn.q_proj.scales\n",
      "model.layers.19.self_attn.v_proj.qzeros\n",
      "model.layers.19.self_attn.v_proj.scales\n",
      "model.layers.2.mlp.down_proj.qzeros\n",
      "model.layers.2.mlp.down_proj.scales\n",
      "model.layers.2.mlp.gate_proj.qzeros\n",
      "model.layers.2.mlp.gate_proj.scales\n",
      "model.layers.2.mlp.up_proj.qzeros\n",
      "model.layers.2.mlp.up_proj.scales\n",
      "model.layers.2.self_attn.k_proj.qzeros\n",
      "model.layers.2.self_attn.k_proj.scales\n",
      "model.layers.2.self_attn.o_proj.qzeros\n",
      "model.layers.2.self_attn.o_proj.scales\n",
      "model.layers.2.self_attn.q_proj.qzeros\n",
      "model.layers.2.self_attn.q_proj.scales\n",
      "model.layers.2.self_attn.v_proj.qzeros\n",
      "model.layers.2.self_attn.v_proj.scales\n",
      "model.layers.20.mlp.down_proj.qzeros\n",
      "model.layers.20.mlp.down_proj.scales\n",
      "model.layers.20.mlp.gate_proj.qzeros\n",
      "model.layers.20.mlp.gate_proj.scales\n",
      "model.layers.20.mlp.up_proj.qzeros\n",
      "model.layers.20.mlp.up_proj.scales\n",
      "model.layers.20.self_attn.k_proj.qzeros\n",
      "model.layers.20.self_attn.k_proj.scales\n",
      "model.layers.20.self_attn.o_proj.qzeros\n",
      "model.layers.20.self_attn.o_proj.scales\n",
      "model.layers.20.self_attn.q_proj.qzeros\n",
      "model.layers.20.self_attn.q_proj.scales\n",
      "model.layers.20.self_attn.v_proj.qzeros\n",
      "model.layers.20.self_attn.v_proj.scales\n",
      "model.layers.21.mlp.down_proj.qzeros\n",
      "model.layers.21.mlp.down_proj.scales\n",
      "model.layers.21.mlp.gate_proj.qzeros\n",
      "model.layers.21.mlp.gate_proj.scales\n",
      "model.layers.21.mlp.up_proj.qzeros\n",
      "model.layers.21.mlp.up_proj.scales\n",
      "model.layers.21.self_attn.k_proj.qzeros\n",
      "model.layers.21.self_attn.k_proj.scales\n",
      "model.layers.21.self_attn.o_proj.qzeros\n",
      "model.layers.21.self_attn.o_proj.scales\n",
      "model.layers.21.self_attn.q_proj.qzeros\n",
      "model.layers.21.self_attn.q_proj.scales\n",
      "model.layers.21.self_attn.v_proj.qzeros\n",
      "model.layers.21.self_attn.v_proj.scales\n",
      "model.layers.22.mlp.down_proj.qzeros\n",
      "model.layers.22.mlp.down_proj.scales\n",
      "model.layers.22.mlp.gate_proj.qzeros\n",
      "model.layers.22.mlp.gate_proj.scales\n",
      "model.layers.22.mlp.up_proj.qzeros\n",
      "model.layers.22.mlp.up_proj.scales\n",
      "model.layers.22.self_attn.k_proj.qzeros\n",
      "model.layers.22.self_attn.k_proj.scales\n",
      "model.layers.22.self_attn.o_proj.qzeros\n",
      "model.layers.22.self_attn.o_proj.scales\n",
      "model.layers.22.self_attn.q_proj.qzeros\n",
      "model.layers.22.self_attn.q_proj.scales\n",
      "model.layers.22.self_attn.v_proj.qzeros\n",
      "model.layers.22.self_attn.v_proj.scales\n",
      "model.layers.23.mlp.down_proj.qzeros\n",
      "model.layers.23.mlp.down_proj.scales\n",
      "model.layers.23.mlp.gate_proj.qzeros\n",
      "model.layers.23.mlp.gate_proj.scales\n",
      "model.layers.23.mlp.up_proj.qzeros\n",
      "model.layers.23.mlp.up_proj.scales\n",
      "model.layers.23.self_attn.k_proj.qzeros\n",
      "model.layers.23.self_attn.k_proj.scales\n",
      "model.layers.23.self_attn.o_proj.qzeros\n",
      "model.layers.23.self_attn.o_proj.scales\n",
      "model.layers.23.self_attn.q_proj.qzeros\n",
      "model.layers.23.self_attn.q_proj.scales\n",
      "model.layers.23.self_attn.v_proj.qzeros\n",
      "model.layers.23.self_attn.v_proj.scales\n",
      "model.layers.24.mlp.down_proj.qzeros\n",
      "model.layers.24.mlp.down_proj.scales\n",
      "model.layers.24.mlp.gate_proj.qzeros\n",
      "model.layers.24.mlp.gate_proj.scales\n",
      "model.layers.24.mlp.up_proj.qzeros\n",
      "model.layers.24.mlp.up_proj.scales\n",
      "model.layers.24.self_attn.k_proj.qzeros\n",
      "model.layers.24.self_attn.k_proj.scales\n",
      "model.layers.24.self_attn.o_proj.qzeros\n",
      "model.layers.24.self_attn.o_proj.scales\n",
      "model.layers.24.self_attn.q_proj.qzeros\n",
      "model.layers.24.self_attn.q_proj.scales\n",
      "model.layers.24.self_attn.v_proj.qzeros\n",
      "model.layers.24.self_attn.v_proj.scales\n",
      "model.layers.25.mlp.down_proj.qzeros\n",
      "model.layers.25.mlp.down_proj.scales\n",
      "model.layers.25.mlp.gate_proj.qzeros\n",
      "model.layers.25.mlp.gate_proj.scales\n",
      "model.layers.25.mlp.up_proj.qzeros\n",
      "model.layers.25.mlp.up_proj.scales\n",
      "model.layers.25.self_attn.k_proj.qzeros\n",
      "model.layers.25.self_attn.k_proj.scales\n",
      "model.layers.25.self_attn.o_proj.qzeros\n",
      "model.layers.25.self_attn.o_proj.scales\n",
      "model.layers.25.self_attn.q_proj.qzeros\n",
      "model.layers.25.self_attn.q_proj.scales\n",
      "model.layers.25.self_attn.v_proj.qzeros\n",
      "model.layers.25.self_attn.v_proj.scales\n",
      "model.layers.26.mlp.down_proj.qzeros\n",
      "model.layers.26.mlp.down_proj.scales\n",
      "model.layers.26.mlp.gate_proj.qzeros\n",
      "model.layers.26.mlp.gate_proj.scales\n",
      "model.layers.26.mlp.up_proj.qzeros\n",
      "model.layers.26.mlp.up_proj.scales\n",
      "model.layers.26.self_attn.k_proj.qzeros\n",
      "model.layers.26.self_attn.k_proj.scales\n",
      "model.layers.26.self_attn.o_proj.qzeros\n",
      "model.layers.26.self_attn.o_proj.scales\n",
      "model.layers.26.self_attn.q_proj.qzeros\n",
      "model.layers.26.self_attn.q_proj.scales\n",
      "model.layers.26.self_attn.v_proj.qzeros\n",
      "model.layers.26.self_attn.v_proj.scales\n",
      "model.layers.27.mlp.down_proj.qzeros\n",
      "model.layers.27.mlp.down_proj.scales\n",
      "model.layers.27.mlp.gate_proj.qzeros\n",
      "model.layers.27.mlp.gate_proj.scales\n",
      "model.layers.27.mlp.up_proj.qzeros\n",
      "model.layers.27.mlp.up_proj.scales\n",
      "model.layers.27.self_attn.k_proj.qzeros\n",
      "model.layers.27.self_attn.k_proj.scales\n",
      "model.layers.27.self_attn.o_proj.qzeros\n",
      "model.layers.27.self_attn.o_proj.scales\n",
      "model.layers.27.self_attn.q_proj.qzeros\n",
      "model.layers.27.self_attn.q_proj.scales\n",
      "model.layers.27.self_attn.v_proj.qzeros\n",
      "model.layers.27.self_attn.v_proj.scales\n",
      "model.layers.28.mlp.down_proj.qzeros\n",
      "model.layers.28.mlp.down_proj.scales\n",
      "model.layers.28.mlp.gate_proj.qzeros\n",
      "model.layers.28.mlp.gate_proj.scales\n",
      "model.layers.28.mlp.up_proj.qzeros\n",
      "model.layers.28.mlp.up_proj.scales\n",
      "model.layers.28.self_attn.k_proj.qzeros\n",
      "model.layers.28.self_attn.k_proj.scales\n",
      "model.layers.28.self_attn.o_proj.qzeros\n",
      "model.layers.28.self_attn.o_proj.scales\n",
      "model.layers.28.self_attn.q_proj.qzeros\n",
      "model.layers.28.self_attn.q_proj.scales\n",
      "model.layers.28.self_attn.v_proj.qzeros\n",
      "model.layers.28.self_attn.v_proj.scales\n",
      "model.layers.29.mlp.down_proj.qzeros\n",
      "model.layers.29.mlp.down_proj.scales\n",
      "model.layers.29.mlp.gate_proj.qzeros\n",
      "model.layers.29.mlp.gate_proj.scales\n",
      "model.layers.29.mlp.up_proj.qzeros\n",
      "model.layers.29.mlp.up_proj.scales\n",
      "model.layers.29.self_attn.k_proj.qzeros\n",
      "model.layers.29.self_attn.k_proj.scales\n",
      "model.layers.29.self_attn.o_proj.qzeros\n",
      "model.layers.29.self_attn.o_proj.scales\n",
      "model.layers.29.self_attn.q_proj.qzeros\n",
      "model.layers.29.self_attn.q_proj.scales\n",
      "model.layers.29.self_attn.v_proj.qzeros\n",
      "model.layers.29.self_attn.v_proj.scales\n",
      "model.layers.3.mlp.down_proj.qzeros\n",
      "model.layers.3.mlp.down_proj.scales\n",
      "model.layers.3.mlp.gate_proj.qzeros\n",
      "model.layers.3.mlp.gate_proj.scales\n",
      "model.layers.3.mlp.up_proj.qzeros\n",
      "model.layers.3.mlp.up_proj.scales\n",
      "model.layers.3.self_attn.k_proj.qzeros\n",
      "model.layers.3.self_attn.k_proj.scales\n",
      "model.layers.3.self_attn.o_proj.qzeros\n",
      "model.layers.3.self_attn.o_proj.scales\n",
      "model.layers.3.self_attn.q_proj.qzeros\n",
      "model.layers.3.self_attn.q_proj.scales\n",
      "model.layers.3.self_attn.v_proj.qzeros\n",
      "model.layers.3.self_attn.v_proj.scales\n",
      "model.layers.30.mlp.down_proj.qzeros\n",
      "model.layers.30.mlp.down_proj.scales\n",
      "model.layers.30.mlp.gate_proj.qzeros\n",
      "model.layers.30.mlp.gate_proj.scales\n",
      "model.layers.30.mlp.up_proj.qzeros\n",
      "model.layers.30.mlp.up_proj.scales\n",
      "model.layers.30.self_attn.k_proj.qzeros\n",
      "model.layers.30.self_attn.k_proj.scales\n",
      "model.layers.30.self_attn.o_proj.qzeros\n",
      "model.layers.30.self_attn.o_proj.scales\n",
      "model.layers.30.self_attn.q_proj.qzeros\n",
      "model.layers.30.self_attn.q_proj.scales\n",
      "model.layers.30.self_attn.v_proj.qzeros\n",
      "model.layers.30.self_attn.v_proj.scales\n",
      "model.layers.31.mlp.down_proj.qzeros\n",
      "model.layers.31.mlp.down_proj.scales\n",
      "model.layers.31.mlp.gate_proj.qzeros\n",
      "model.layers.31.mlp.gate_proj.scales\n",
      "model.layers.31.mlp.up_proj.qzeros\n",
      "model.layers.31.mlp.up_proj.scales\n",
      "model.layers.31.self_attn.k_proj.qzeros\n",
      "model.layers.31.self_attn.k_proj.scales\n",
      "model.layers.31.self_attn.o_proj.qzeros\n",
      "model.layers.31.self_attn.o_proj.scales\n",
      "model.layers.31.self_attn.q_proj.qzeros\n",
      "model.layers.31.self_attn.q_proj.scales\n",
      "model.layers.31.self_attn.v_proj.qzeros\n",
      "model.layers.31.self_attn.v_proj.scales\n",
      "model.layers.32.mlp.down_proj.qzeros\n",
      "model.layers.32.mlp.down_proj.scales\n",
      "model.layers.32.mlp.gate_proj.qzeros\n",
      "model.layers.32.mlp.gate_proj.scales\n",
      "model.layers.32.mlp.up_proj.qzeros\n",
      "model.layers.32.mlp.up_proj.scales\n",
      "model.layers.32.self_attn.k_proj.qzeros\n",
      "model.layers.32.self_attn.k_proj.scales\n",
      "model.layers.32.self_attn.o_proj.qzeros\n",
      "model.layers.32.self_attn.o_proj.scales\n",
      "model.layers.32.self_attn.q_proj.qzeros\n",
      "model.layers.32.self_attn.q_proj.scales\n",
      "model.layers.32.self_attn.v_proj.qzeros\n",
      "model.layers.32.self_attn.v_proj.scales\n",
      "model.layers.33.mlp.down_proj.qzeros\n",
      "model.layers.33.mlp.down_proj.scales\n",
      "model.layers.33.mlp.gate_proj.qzeros\n",
      "model.layers.33.mlp.gate_proj.scales\n",
      "model.layers.33.mlp.up_proj.qzeros\n",
      "model.layers.33.mlp.up_proj.scales\n",
      "model.layers.33.self_attn.k_proj.qzeros\n",
      "model.layers.33.self_attn.k_proj.scales\n",
      "model.layers.33.self_attn.o_proj.qzeros\n",
      "model.layers.33.self_attn.o_proj.scales\n",
      "model.layers.33.self_attn.q_proj.qzeros\n",
      "model.layers.33.self_attn.q_proj.scales\n",
      "model.layers.33.self_attn.v_proj.qzeros\n",
      "model.layers.33.self_attn.v_proj.scales\n",
      "model.layers.34.mlp.down_proj.qzeros\n",
      "model.layers.34.mlp.down_proj.scales\n",
      "model.layers.34.mlp.gate_proj.qzeros\n",
      "model.layers.34.mlp.gate_proj.scales\n",
      "model.layers.34.mlp.up_proj.qzeros\n",
      "model.layers.34.mlp.up_proj.scales\n",
      "model.layers.34.self_attn.k_proj.qzeros\n",
      "model.layers.34.self_attn.k_proj.scales\n",
      "model.layers.34.self_attn.o_proj.qzeros\n",
      "model.layers.34.self_attn.o_proj.scales\n",
      "model.layers.34.self_attn.q_proj.qzeros\n",
      "model.layers.34.self_attn.q_proj.scales\n",
      "model.layers.34.self_attn.v_proj.qzeros\n",
      "model.layers.34.self_attn.v_proj.scales\n",
      "model.layers.35.mlp.down_proj.qzeros\n",
      "model.layers.35.mlp.down_proj.scales\n",
      "model.layers.35.mlp.gate_proj.qzeros\n",
      "model.layers.35.mlp.gate_proj.scales\n",
      "model.layers.35.mlp.up_proj.qzeros\n",
      "model.layers.35.mlp.up_proj.scales\n",
      "model.layers.35.self_attn.k_proj.qzeros\n",
      "model.layers.35.self_attn.k_proj.scales\n",
      "model.layers.35.self_attn.o_proj.qzeros\n",
      "model.layers.35.self_attn.o_proj.scales\n",
      "model.layers.35.self_attn.q_proj.qzeros\n",
      "model.layers.35.self_attn.q_proj.scales\n",
      "model.layers.35.self_attn.v_proj.qzeros\n",
      "model.layers.35.self_attn.v_proj.scales\n",
      "model.layers.36.mlp.down_proj.qzeros\n",
      "model.layers.36.mlp.down_proj.scales\n",
      "model.layers.36.mlp.gate_proj.qzeros\n",
      "model.layers.36.mlp.gate_proj.scales\n",
      "model.layers.36.mlp.up_proj.qzeros\n",
      "model.layers.36.mlp.up_proj.scales\n",
      "model.layers.36.self_attn.k_proj.qzeros\n",
      "model.layers.36.self_attn.k_proj.scales\n",
      "model.layers.36.self_attn.o_proj.qzeros\n",
      "model.layers.36.self_attn.o_proj.scales\n",
      "model.layers.36.self_attn.q_proj.qzeros\n",
      "model.layers.36.self_attn.q_proj.scales\n",
      "model.layers.36.self_attn.v_proj.qzeros\n",
      "model.layers.36.self_attn.v_proj.scales\n",
      "model.layers.37.mlp.down_proj.qzeros\n",
      "model.layers.37.mlp.down_proj.scales\n",
      "model.layers.37.mlp.gate_proj.qzeros\n",
      "model.layers.37.mlp.gate_proj.scales\n",
      "model.layers.37.mlp.up_proj.qzeros\n",
      "model.layers.37.mlp.up_proj.scales\n",
      "model.layers.37.self_attn.k_proj.qzeros\n",
      "model.layers.37.self_attn.k_proj.scales\n",
      "model.layers.37.self_attn.o_proj.qzeros\n",
      "model.layers.37.self_attn.o_proj.scales\n",
      "model.layers.37.self_attn.q_proj.qzeros\n",
      "model.layers.37.self_attn.q_proj.scales\n",
      "model.layers.37.self_attn.v_proj.qzeros\n",
      "model.layers.37.self_attn.v_proj.scales\n",
      "model.layers.38.mlp.down_proj.qzeros\n",
      "model.layers.38.mlp.down_proj.scales\n",
      "model.layers.38.mlp.gate_proj.qzeros\n",
      "model.layers.38.mlp.gate_proj.scales\n",
      "model.layers.38.mlp.up_proj.qzeros\n",
      "model.layers.38.mlp.up_proj.scales\n",
      "model.layers.38.self_attn.k_proj.qzeros\n",
      "model.layers.38.self_attn.k_proj.scales\n",
      "model.layers.38.self_attn.o_proj.qzeros\n",
      "model.layers.38.self_attn.o_proj.scales\n",
      "model.layers.38.self_attn.q_proj.qzeros\n",
      "model.layers.38.self_attn.q_proj.scales\n",
      "model.layers.38.self_attn.v_proj.qzeros\n",
      "model.layers.38.self_attn.v_proj.scales\n",
      "model.layers.39.mlp.down_proj.qzeros\n",
      "model.layers.39.mlp.down_proj.scales\n",
      "model.layers.39.mlp.gate_proj.qzeros\n",
      "model.layers.39.mlp.gate_proj.scales\n",
      "model.layers.39.mlp.up_proj.qzeros\n",
      "model.layers.39.mlp.up_proj.scales\n",
      "model.layers.39.self_attn.k_proj.qzeros\n",
      "model.layers.39.self_attn.k_proj.scales\n",
      "model.layers.39.self_attn.o_proj.qzeros\n",
      "model.layers.39.self_attn.o_proj.scales\n",
      "model.layers.39.self_attn.q_proj.qzeros\n",
      "model.layers.39.self_attn.q_proj.scales\n",
      "model.layers.39.self_attn.v_proj.qzeros\n",
      "model.layers.39.self_attn.v_proj.scales\n",
      "model.layers.4.mlp.down_proj.qzeros\n",
      "model.layers.4.mlp.down_proj.scales\n",
      "model.layers.4.mlp.gate_proj.qzeros\n",
      "model.layers.4.mlp.gate_proj.scales\n",
      "model.layers.4.mlp.up_proj.qzeros\n",
      "model.layers.4.mlp.up_proj.scales\n",
      "model.layers.4.self_attn.k_proj.qzeros\n",
      "model.layers.4.self_attn.k_proj.scales\n",
      "model.layers.4.self_attn.o_proj.qzeros\n",
      "model.layers.4.self_attn.o_proj.scales\n",
      "model.layers.4.self_attn.q_proj.qzeros\n",
      "model.layers.4.self_attn.q_proj.scales\n",
      "model.layers.4.self_attn.v_proj.qzeros\n",
      "model.layers.4.self_attn.v_proj.scales\n",
      "model.layers.5.mlp.down_proj.qzeros\n",
      "model.layers.5.mlp.down_proj.scales\n",
      "model.layers.5.mlp.gate_proj.qzeros\n",
      "model.layers.5.mlp.gate_proj.scales\n",
      "model.layers.5.mlp.up_proj.qzeros\n",
      "model.layers.5.mlp.up_proj.scales\n",
      "model.layers.5.self_attn.k_proj.qzeros\n",
      "model.layers.5.self_attn.k_proj.scales\n",
      "model.layers.5.self_attn.o_proj.qzeros\n",
      "model.layers.5.self_attn.o_proj.scales\n",
      "model.layers.5.self_attn.q_proj.qzeros\n",
      "model.layers.5.self_attn.q_proj.scales\n",
      "model.layers.5.self_attn.v_proj.qzeros\n",
      "model.layers.5.self_attn.v_proj.scales\n",
      "model.layers.6.mlp.down_proj.qzeros\n",
      "model.layers.6.mlp.down_proj.scales\n",
      "model.layers.6.mlp.gate_proj.qzeros\n",
      "model.layers.6.mlp.gate_proj.scales\n",
      "model.layers.6.mlp.up_proj.qzeros\n",
      "model.layers.6.mlp.up_proj.scales\n",
      "model.layers.6.self_attn.k_proj.qzeros\n",
      "model.layers.6.self_attn.k_proj.scales\n",
      "model.layers.6.self_attn.o_proj.qzeros\n",
      "model.layers.6.self_attn.o_proj.scales\n",
      "model.layers.6.self_attn.q_proj.qzeros\n",
      "model.layers.6.self_attn.q_proj.scales\n",
      "model.layers.6.self_attn.v_proj.qzeros\n",
      "model.layers.6.self_attn.v_proj.scales\n",
      "model.layers.7.mlp.down_proj.qzeros\n",
      "model.layers.7.mlp.down_proj.scales\n",
      "model.layers.7.mlp.gate_proj.qzeros\n",
      "model.layers.7.mlp.gate_proj.scales\n",
      "model.layers.7.mlp.up_proj.qzeros\n",
      "model.layers.7.mlp.up_proj.scales\n",
      "model.layers.7.self_attn.k_proj.qzeros\n",
      "model.layers.7.self_attn.k_proj.scales\n",
      "model.layers.7.self_attn.o_proj.qzeros\n",
      "model.layers.7.self_attn.o_proj.scales\n",
      "model.layers.7.self_attn.q_proj.qzeros\n",
      "model.layers.7.self_attn.q_proj.scales\n",
      "model.layers.7.self_attn.v_proj.qzeros\n",
      "model.layers.7.self_attn.v_proj.scales\n",
      "model.layers.8.mlp.down_proj.qzeros\n",
      "model.layers.8.mlp.down_proj.scales\n",
      "model.layers.8.mlp.gate_proj.qzeros\n",
      "model.layers.8.mlp.gate_proj.scales\n",
      "model.layers.8.mlp.up_proj.qzeros\n",
      "model.layers.8.mlp.up_proj.scales\n",
      "model.layers.8.self_attn.k_proj.qzeros\n",
      "model.layers.8.self_attn.k_proj.scales\n",
      "model.layers.8.self_attn.o_proj.qzeros\n",
      "model.layers.8.self_attn.o_proj.scales\n",
      "model.layers.8.self_attn.q_proj.qzeros\n",
      "model.layers.8.self_attn.q_proj.scales\n",
      "model.layers.8.self_attn.v_proj.qzeros\n",
      "model.layers.8.self_attn.v_proj.scales\n",
      "model.layers.9.mlp.down_proj.qzeros\n",
      "model.layers.9.mlp.down_proj.scales\n",
      "model.layers.9.mlp.gate_proj.qzeros\n",
      "model.layers.9.mlp.gate_proj.scales\n",
      "model.layers.9.mlp.up_proj.qzeros\n",
      "model.layers.9.mlp.up_proj.scales\n",
      "model.layers.9.self_attn.k_proj.qzeros\n",
      "model.layers.9.self_attn.k_proj.scales\n",
      "model.layers.9.self_attn.o_proj.qzeros\n",
      "model.layers.9.self_attn.o_proj.scales\n",
      "model.layers.9.self_attn.q_proj.qzeros\n",
      "model.layers.9.self_attn.q_proj.scales\n",
      "model.layers.9.self_attn.v_proj.qzeros\n",
      "model.layers.9.self_attn.v_proj.scales\n"
     ]
    }
   ],
   "source": [
    "for name, loaded_weight in weights_iterator: \n",
    "    if 'scales' in name or 'zeros' in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9098d1-2374-4060-b8fe-60052c04110e",
   "metadata": {},
   "source": [
    "### Create Quantized Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7cfc1da-2147-4722-a766-ee216a29ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, json\n",
    "from safetensors.torch import save_file\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7d1dd78-d696-4477-b878-a13a763c270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path(\"/home/ubuntu/models/llama-7b-orca-math-100k-full-quantized\")\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a8cb2-6eff-4645-bcff-c4bf66238802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original quantized layers from fsdp_qlora/train.py\n",
    "# [\"k_proj\", \"q_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afaa582c-95b9-4e22-a209-485cfb73f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to AWQ for now\n",
    "quantized_layers = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65b4afb3-d1ff-434d-8e8d-e9537b1ccda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_math_model_dir = \"/home/ubuntu/models/llama-7b-orca-math-100k-full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e3067be-c874-4550-b999-1452d7e8f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = safetensors.torch.load_file(glob(os.path.join(orca_math_model_dir, \"*.safetensors\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eaef503-678f-4084-b3f7-faa0af7f6d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_state_dict = copy.deepcopy(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3686d0a-497a-4486-bf84-169dc6924ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pack_factor = 2\n",
    "blocksize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "776a5270-682e-441b-aa58-6ec8e92cfb0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 12.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00, 10.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:01, 11.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:01, 10.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.1.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:01, 10.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.1.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:01, 10.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.1.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:02, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.10.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:02,  7.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.10.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [00:02, 10.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [00:02, 10.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.11.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:03, 10.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.11.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:03, 10.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.11.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:03, 11.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.11.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:03, 10.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.12.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [00:04, 10.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.12.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:04, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.12.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [00:04, 10.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.13.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:05, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.13.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [00:05, 11.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.13.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [00:05, 10.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:06, 10.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65it [00:06, 11.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.14.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67it [00:06, 10.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.15.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:07, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.15.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:07, 11.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.15.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76it [00:07, 10.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.16.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:08, 10.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.16.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:08, 11.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.16.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:08, 10.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.17.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [00:08, 10.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.17.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:09, 11.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.17.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94it [00:09, 10.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.18.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [00:09, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.18.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:09, 11.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.18.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [00:10, 10.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.19.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108it [00:10, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.19.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:10, 11.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.19.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:11, 10.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.2.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114it [00:11,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.2.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "117it [00:11, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:11, 10.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.20.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126it [00:12, 10.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.20.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128it [00:12, 11.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.20.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130it [00:12, 10.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.21.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "135it [00:13, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.21.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "137it [00:13, 11.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.21.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "139it [00:13, 10.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.22.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144it [00:14, 10.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.22.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146it [00:14, 11.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.22.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [00:14, 10.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.23.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "153it [00:15, 10.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.23.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "155it [00:15, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.23.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:15, 10.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.24.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "162it [00:16, 10.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.24.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [00:16, 11.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.24.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "166it [00:16, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.25.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "171it [00:17, 10.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.25.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173it [00:17, 11.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.25.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "175it [00:17, 10.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.26.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180it [00:17, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.26.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182it [00:18, 11.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.26.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "184it [00:18, 10.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.27.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "186it [00:18,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.27.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "189it [00:18, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "193it [00:19, 10.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.28.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "195it [00:19,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.28.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "198it [00:19, 10.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "202it [00:20, 10.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.29.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "207it [00:20, 10.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.29.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "209it [00:20, 11.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.29.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [00:20, 10.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.3.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "216it [00:21, 10.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.3.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218it [00:21, 11.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.3.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "220it [00:21, 10.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.30.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "225it [00:22, 10.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.30.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [00:22, 11.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.30.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "229it [00:22, 10.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.31.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [00:23, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.31.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "236it [00:23, 11.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.31.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "238it [00:23, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.4.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "243it [00:24, 10.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.4.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [00:24, 11.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.4.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "247it [00:24, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.5.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "249it [00:24,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.5.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "252it [00:24, 10.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:25, 11.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.6.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "261it [00:25, 10.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.6.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "263it [00:25, 11.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.6.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "265it [00:26, 11.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.7.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "270it [00:26, 10.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.7.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "272it [00:26, 11.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.7.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "274it [00:27, 11.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.8.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "279it [00:27, 10.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.8.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "281it [00:27, 11.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.8.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([4096, 11008]) torch.Size([11008, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "283it [00:27, 10.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.9.mlp.gate_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([11008, 4096]) torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "288it [00:28, 10.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.9.self_attn.k_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "291it [00:28, 10.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.9.self_attn.v_proj.weight torch.Size([4096, 4096]) torch.Size([4096, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for n,p in iter(weights.items()):\n",
    "    if any(l in n for l in quantized_layers) and \"weight\" in n:\n",
    "        # output_size x input_size\n",
    "        print(n, p.shape, p.t().shape)\n",
    "        param = Params4bit(p.t(), quant_type=\"nf4\", blocksize=blocksize, compress_statistics=False, quant_storage=torch.uint8)\n",
    "        input_size, output_size = p.t().shape\n",
    "        param.cuda();\n",
    "\n",
    "        # reshape for tensor parallelism\n",
    "        qweight, absmax = param.data.cpu(), param.quant_state.absmax.cpu()        \n",
    "        qweight = qweight.reshape(input_size, output_size // pack_factor)\n",
    "        absmax = absmax.reshape(input_size, output_size // blocksize)\n",
    "                \n",
    "        quantized_state_dict[n] = qweight\n",
    "        quantized_state_dict[n.replace(\".weight\", \".absmax\")] = absmax\n",
    "\n",
    "        param = None\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07d917df-7614-41b5-bac7-c85fb73c9ef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save quantized weights\n",
    "save_file(quantized_state_dict, model_dir/\"model_state_dict.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4857052b-6de3-44ca-be4a-b36e77a71817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save quantization config\n",
    "quant_config_filename = model_dir/\"quantize_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6130b09-c496-40b5-b022-fbd6237c994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config_dict = {\n",
    "    \"weight_bits\" : 4,\n",
    "    \"blocksize\" : 64,\n",
    "    \"quant_type\" : \"nf4\",\n",
    "    \"quant_storage\" : \"uint8\",\n",
    "    \"compress_statistics\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "772b7e27-53ec-4a22-8970-d2a455e580f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(quant_config_filename, \"w+\") as f: json.dump(quant_config_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6324272-3832-45bb-95da-f98a992937fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10e9d27d-c2d9-48e5-8e75-3f1ddb1b2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model config\n",
    "model_config_filename = model_dir/\"config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "863d3eb1-d461-4c6c-9c5b-df04a7addb8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(model_config_filename, \"w+\") as f: json.dump(model_config.to_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a28ac0-9d96-46a6-a0e8-a28211728cb4",
   "metadata": {},
   "source": [
    "### BNB Quantized VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f8c396-e4ec-4bc5-81b0-0bf7239a73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# import os\n",
    "# os.makedirs(\"/home/ubuntu/models/llama-7b-orca-math-100k-full\", exist_ok=True)\n",
    "# hf_hub_download(repo_id=\"answerdotai/llama-7b-orca-math-100k-full\", \n",
    "#                 filename=\"model_state_dict.safetensors\",\n",
    "#                 local_dir=\"/home/ubuntu/models/llama-7b-orca-math-100k-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4eca361-0895-4858-ad46-c705ff64451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "import safetensors\n",
    "import safetensors.torch\n",
    "from pathlib import Path\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.functional import dequantize_4bit, QuantState\n",
    "from bitsandbytes.nn.modules import Params4bit, Linear4bit\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from accelerate import init_empty_weights\n",
    "from glob import glob\n",
    "import os\n",
    "from fastcore.parallel import parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c98b3fff-f6ad-488b-b41a-0f108bb7688b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5951483a-d5d6-4c82-bab0-c07d868f8c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c017efd-f1b9-4f72-8902-f6540fb3824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\"\"###Question:\n",
    "A certain tax rate is some amount per $100.00. The rate, expressed as a percent, is 65%. What is the tax rate in dollars per $100.00?\n",
    "###Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c61c1c1c-6717-42d7-870f-713b4923bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear(model:nn.Module, linear_replacement:nn.Module, quant_config:dict|None=None,\n",
    "                   skip_modules:List[str]=[\"lm_head\"], **kwargs):\n",
    "    \"\"\"\n",
    "    Replace linear modules with a new Linear module.\n",
    "    Parameters:\n",
    "        model (`torch.nn.Module`):\n",
    "            Input model or `torch.nn.Module` as the function is run recursively.\n",
    "        linear_replacement (`torch.nn.Module`):\n",
    "            The linear module that replaces the old one. Only expects standard arguments.\n",
    "            If other arguments need to be passed, use a lambda.\n",
    "        skip_modules (`List[str]`, *optional*, defaults to `lm_head`):\n",
    "            List of modules names not to convert. Defaults to `lm_head`.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if name in skip_modules:\n",
    "            print(f\"Skipping {name}\")\n",
    "            continue\n",
    "        \n",
    "        if len(list(module.children())) > 0:\n",
    "            replace_linear(module, linear_replacement, quant_config, skip_modules, **kwargs)\n",
    "\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            if issubclass(linear_replacement, Linear4bit):\n",
    "                model._modules[name] = linear_replacement(\n",
    "                    module.in_features,\n",
    "                    module.out_features,\n",
    "                    module.bias is not None,\n",
    "                    **kwargs\n",
    "                )\n",
    "            # elif issubclass(linear_replacement, HQQLinear):\n",
    "            #     model._modules[name] = linear_replacement(module, quant_config, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported linear replacement: {type(linear_replacement)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd8402bd-a3b6-4479-8ab3-21fb9c8c8ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_quantize(module:nn.Module, name:str, value:torch.Tensor, device:torch.device=None, dtype:torch.dtype=None,\n",
    "                      skip_names:list[str]=[], is_meta_rank:bool=False, low_memory:bool=True, verbose:bool=False,\n",
    "                      quant_method:str='bnb', is_dora:bool=False):\n",
    "    \"\"\"\n",
    "    Loads `value` tensor into submodule of `module`, optionally skipping `skip_names` and converting to `dtype`.\n",
    "\n",
    "    Quantizes `Params4bit` on `device` then places on \"cpu\" if low_memory=True or \"meta\" if is_meta_rank=True.\n",
    "    \"\"\"\n",
    "    def place_on_device(value):\n",
    "        if is_meta_rank:\n",
    "            device = 'meta'\n",
    "        elif low_memory:\n",
    "            device = 'cpu'\n",
    "        return value.to(device=device, dtype=dtype)\n",
    "\n",
    "    if any([skip_name in name for skip_name in skip_names]):\n",
    "        if verbose:\n",
    "            print(f\"Skipping {name} because it is in skip_names\")\n",
    "        return\n",
    "\n",
    "    module_key, _, value_key = name.rpartition('.')\n",
    "    try:\n",
    "        submodule = module.get_submodule(module_key)\n",
    "    except AttributeError as e:\n",
    "        print(f\"Module {module_key} not found:\\n{e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        if quant_method=='bnb':\n",
    "            param = submodule.get_parameter(value_key)\n",
    "            if isinstance(param, Params4bit):\n",
    "                # With `sync_module_states=True`, a meta device Params4bit needs to be the same\n",
    "                # shape as the quantized Params4bit with an initialized quant_state. However,\n",
    "                # FSDP only syncs parameters and buffers, so the quant_state isn't copied. This\n",
    "                # workaround quantizes Params4bit to initialize quant_state on all ranks, then\n",
    "                # replaces Params4bit's data with a meta tensor to free memory on non-rank 0.\n",
    "                if is_dora:\n",
    "                    setattr(submodule, \"dora_scale\", value.norm(p=2, dim=1).to(dtype=dtype).to(\"cpu\"))                \n",
    "                    print(\"DORA scale initialized\")\n",
    "                value = type(param)(value.to(device=device, dtype=dtype).data, **param.__dict__).cuda(device)\n",
    "                if is_meta_rank:\n",
    "                    value = type(param)(value.data.to(\"meta\"), **value.__dict__)\n",
    "                elif low_memory:\n",
    "                    value = type(param)(value.data.to(\"cpu\"), **value.__dict__)\n",
    "                # print(\"Loaded quantized layer\")\n",
    "            else:\n",
    "                value = type(param)(place_on_device(value).data)\n",
    "                # print(\"Loaded regular layer\")\n",
    "    except AttributeError:\n",
    "        # it's a buffer\n",
    "        value = place_on_device(value)\n",
    "        pass\n",
    "    setattr(submodule, value_key, value)\n",
    "\n",
    "def load_and_quantize_parallel(name_param, model, **kwargs):\n",
    "    name, param = name_param\n",
    "    load_and_quantize(model, name, param, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c859ec51-9ea5-462f-a01e-afbe15fb3e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_math_model_dir = \"/home/ubuntu/models/llama-7b-orca-math-100k-full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815e7ff7-55e5-4f2f-90e7-65a35cf1a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "cfg = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "cfg._attn_implementation = \"sdpa\"\n",
    "skip_modules = [\"lm_head\"]\n",
    "load_param_skip_names = ['inv_freq']\n",
    "compute_dtype = torch_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adcd81a1-ee01-4e43-9f27-e0647a536a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(cfg)\n",
    "    model.model = replace_linear(model.model, Linear4bit, compute_dtype=compute_dtype,\n",
    "                                 quant_type='nf4', compress_statistics=False, quant_storage=torch_dtype, skip_modules=skip_modules)\n",
    "model.is_loaded_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a7d4b70-42b2-4ee3-9afd-feb4193f75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = safetensors.torch.load_file(glob(os.path.join(orca_math_model_dir, \"*.safetensors\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d13a7fef-3cc5-4c8a-93e6-80c4c8ee2b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#291) [None,None,None,None,None,None,None,None,None,None...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel(load_and_quantize_parallel, \n",
    "         iter(weights.items()), \n",
    "         n_workers=8, \n",
    "         threadpool=True,\n",
    "         model=model, \n",
    "         dtype=torch_dtype, \n",
    "         device=torch.cuda.current_device(),\n",
    "         skip_names=load_param_skip_names,\n",
    "         is_meta_rank=False,\n",
    "         verbose=True,\n",
    "         quant_method=\"bnb\",\n",
    "         is_dora=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea38000d-dcc5-41da-bb24-386cefcfd954",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09a4740c-d5f6-4a69-a885-f256597d7940",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21cd4521-7bba-44a0-9ee8-eea7f1f6ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer.pad_token_id = hf_tokenizer.unk_token_id\n",
    "hf_tokenizer.pad_token = hf_tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "841ece6f-600f-4789-b4a2-047eb26584a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = model.generate(hf_tokenizer(input, return_tensors=\"pt\")['input_ids'].cuda(),\n",
    "                        generation_config=GenerationConfig(do_sample=False, max_new_tokens=16, use_cache=True, pad_token_id=hf_tokenizer.pad_token_id)).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3a17837-53d8-40fa-98d0-333e6d5a98c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ###Question:\n",
      "A certain tax rate is some amount per $100.00. The rate, expressed as a percent, is 65%. What is the tax rate in dollars per $100.00?\n",
      "###Answer:\n",
      "To find the tax rate in dollars per $100.00\n"
     ]
    }
   ],
   "source": [
    "print(hf_tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4d8ff3b-995b-4ecd-a6e1-d692087e2a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_math_model_dir = \"/home/ubuntu/models/llama-7b-orca-math-100k-full-quantized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "213d5792-dca5-406e-8ff7-9f1a182bcb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-03 09:23:26 config.py:744] Casting torch.float16 to torch.bfloat16.\n",
      "WARNING 04-03 09:23:26 config.py:208] bnb quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 04-03 09:23:26 llm_engine.py:70] Initializing an LLM engine (v0.3.3) with config: model='/home/ubuntu/models/llama-7b-orca-math-100k-full-quantized', tokenizer='meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=bnb, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 04-03 09:23:26 pynccl_utils.py:13] vLLM is using nccl==2.18.1\n",
      "INFO 04-03 09:23:26 selector.py:44] flash_attn is not found.\n",
      "INFO 04-03 09:23:26 selector.py:20] Using XFormers backend.\n",
      "INFO 04-03 09:23:30 model_runner.py:104] Loading model weights took 3.9593 GB\n",
      "INFO 04-03 09:23:31 gpu_executor.py:94] # GPU blocks: 927, # CPU blocks: 512\n",
      "INFO 04-03 09:23:33 model_runner.py:770] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-03 09:23:33 model_runner.py:774] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-03 09:23:42 model_runner.py:846] Graph capturing finished in 9 secs.\n",
      "INFO 04-03 09:23:42 block_manager_v1.py:239] disable automatic prefix caching\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=orca_math_model_dir, tokenizer=\"meta-llama/Llama-2-7b-hf\", dtype=\"float32\", quantization=\"bnb\", gpu_memory_utilization=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "803928ae-f941-469e-bf0f-4d777c56e8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|| 1/1 [00:00<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate([input], sampling_params=SamplingParams(max_tokens=16, temperature=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de878d0f-63c8-4c9d-a4d6-fc2e56d3c162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Question:\n",
      "A certain tax rate is some amount per $100.00. The rate, expressed as a percent, is 65%. What is the tax rate in dollars per $100.00?\n",
      "###Answer:\n",
      "- Int ha Int ha ha ha halint halintint hal hal sc\n"
     ]
    }
   ],
   "source": [
    "print(input + outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "877f7f30-d42a-4a7e-b6ff-625615f48bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_model = llm.llm_engine.model_executor.driver_worker.model_runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d072a4c-2dc9-4044-85b3-2d7c8b32ef33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89e92310-7a0f-4006-b34c-7030187e3fc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): VocabParallelEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (qkv_proj): QKVParallelLinear()\n",
       "          (o_proj): RowParallelLinear()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (attn): Attention()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_up_proj): MergedColumnParallelLinear()\n",
       "          (down_proj): RowParallelLinear()\n",
       "          (act_fn): SiluAndMul()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): ParallelLMHead()\n",
       "  (logits_processor): LogitsProcessor()\n",
       "  (sampler): Sampler()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vllm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ceca3c-ee2d-49c5-b54f-478b60945cff",
   "metadata": {},
   "source": [
    "### Compare Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "3e7ca28d-df4e-4c9e-8d68-244dc4d98eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b9546560-2af1-4c77-986f-0a60c57d7505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.sequence import SequenceGroupMetadata, SequenceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b450e2d2-418c-432c-a0cd-502bbfad2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import collections\n",
    "from typing import DefaultDict, Tuple, List, Dict\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def save_activations(\n",
    "        activations: DefaultDict,\n",
    "        name: str,\n",
    "        module: nn.Module,\n",
    "        inp: Tuple,\n",
    "        out: torch.Tensor\n",
    ") -> None:\n",
    "    \"\"\"PyTorch Forward hook to save outputs at each forward\n",
    "    pass. Mutates specified dict objects with each fwd pass.\n",
    "    \"\"\"\n",
    "    if (\"qkv\" in name or \"o_proj\" in name or \"gate_up_proj\" in name or \"down_proj\" in name):\n",
    "        if len(out) > 1:\n",
    "            out = out[0]\n",
    "    activations[name].append(out.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "85dfb724-1c57-47c4-8c4a-27feafa568e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_activation_hooks(\n",
    "        model: nn.Module,\n",
    "        layers_to_save: List[str]\n",
    ") -> DefaultDict[List, torch.Tensor]:\n",
    "    \"\"\"Registers forward hooks in specified layers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model:\n",
    "        PyTorch model\n",
    "    layers_to_save:\n",
    "        Module names within ``model`` whose activations we want to save.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    activations_dict:\n",
    "        dict of lists containing activations of specified layers in\n",
    "        ``layers_to_save``.\n",
    "    \"\"\"\n",
    "    activations_dict = collections.defaultdict(list)\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if any(l in name for l in layers_to_save):\n",
    "            module.register_forward_hook(\n",
    "                partial(save_activations, activations_dict, name)\n",
    "            )\n",
    "    return activations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "09740976-a219-4148-af45-9cbcbd9b0b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules(): \n",
    "    module._forward_hooks = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "302e1a59-0b32-49be-8249-554ef4704717",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_saved_activations = register_activation_hooks(model, \n",
    "                                              layers_to_save=[\"embed\", \"q_proj\", \"k_proj\", \"v_proj\", \n",
    "                                                              \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ac4cbb47-941a-48ef-b497-227e9c25ccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor([[7]]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "890a098d-2da9-4fe8-8f08-48ea9c75e30d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _ = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "500b4a00-1155-453d-ad3f-13924f57da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in vllm_model.named_modules(): \n",
    "    module._forward_hooks = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b48d0307-7160-4146-a0b6-281e9775bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_saved_activations = register_activation_hooks(vllm_model, \n",
    "                                              layers_to_save=[\"embed\", \"qkv_proj\", \"o_proj\", \"gate_up_proj\", \"down_proj\", \"lm_head\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "1dd40d7b-54b9-448e-80e4-d5ce2566b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor([7]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a29434c5-d674-4072-ba53-83a6c64eeab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_caches = [None] * len(vllm_model.model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "044d13c0-0feb-405a-8f46-df022d780fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = torch.tensor([0]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "fd142820-099c-44ca-81be-4a426eaa80b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [SequenceGroupMetadata(\"1\", True, {0:SequenceData([7,42,1003])}, sampling_params=SamplingParams(temperature=0.0), block_tables=None)]\n",
    "attn_metadata = llm.llm_engine.model_executor.driver_worker.model_runner._prepare_prompt(s)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "180e049a-e8d9-45de-836d-f42577953b61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _ = vllm_model(inp, positions, kv_caches, attn_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "4b340558-3242-46f4-887d-d57a3969dac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 4096])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_saved_activations['model.layers.0.self_attn.q_proj'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "8783794c-ec6b-47c3-9b31-d8c00037da1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12288])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vllm_saved_activations['model.layers.0.self_attn.qkv_proj'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "d162397d-56d6-426c-be34-c849a0167edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "712ada76-5a1e-4592-b356-94f3a9f19464",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_embed_output = hf_saved_activations['model.embed_tokens'][0]\n",
    "vllm_embed_output = vllm_saved_activations['model.embed_tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "eddd20ff-dfd1-4cf1-80f4-be3fed05bc73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hf_embed_output - vllm_embed_output).norm() / hf_embed_output.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "8736f8d8-10b6-471b-a4a4-1ccb340ad2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = {k:[] for k in [\"q\",\"k\",\"v\",\"o\",\"gate\",\"up\",\"down\"]}\n",
    "\n",
    "for layer in range(len(vllm_model.model.layers)):\n",
    "    hf_q_output = hf_saved_activations[f'model.layers.{layer}.self_attn.q_proj'][0]\n",
    "    vllm_q_output = vllm_saved_activations[f'model.layers.{layer}.self_attn.qkv_proj'][0][:,:4096]\n",
    "    deltas[\"q\"].append((hf_q_output - vllm_q_output).norm(p=2))\n",
    "    \n",
    "    hf_k_output = hf_saved_activations[f'model.layers.{layer}.self_attn.k_proj'][0]\n",
    "    vllm_k_output = vllm_saved_activations[f'model.layers.{layer}.self_attn.qkv_proj'][0][:,4096:4096*2]\n",
    "    deltas[\"k\"].append((hf_k_output - vllm_k_output).norm(p=2))\n",
    "    \n",
    "    hf_v_output = hf_saved_activations[f'model.layers.{layer}.self_attn.v_proj'][0]\n",
    "    vllm_v_output = vllm_saved_activations[f'model.layers.{layer}.self_attn.qkv_proj'][0][:,4096*2:]\n",
    "    deltas[\"v\"].append((hf_v_output - vllm_v_output).norm(p=2))\n",
    "    \n",
    "    hf_o_output = hf_saved_activations[f'model.layers.{layer}.self_attn.o_proj'][0]\n",
    "    vllm_o_output = vllm_saved_activations[f'model.layers.{layer}.self_attn.o_proj'][0]\n",
    "    deltas[\"o\"].append((hf_o_output - vllm_o_output).norm(p=2))\n",
    "    \n",
    "    hf_gate_output = hf_saved_activations[f'model.layers.{layer}.mlp.gate_proj'][0]\n",
    "    vllm_gate_output = vllm_saved_activations[f'model.layers.{layer}.mlp.gate_up_proj'][0][:,:11008]\n",
    "    deltas[\"gate\"].append((hf_gate_output - vllm_gate_output).norm(p=2))\n",
    "    \n",
    "    hf_up_output = hf_saved_activations[f'model.layers.{layer}.mlp.up_proj'][0]\n",
    "    vllm_up_output = vllm_saved_activations[f'model.layers.{layer}.mlp.gate_up_proj'][0][:,11008:]\n",
    "    deltas[\"up\"].append((hf_up_output - vllm_up_output).norm(p=2))\n",
    "    \n",
    "    hf_down_output = hf_saved_activations[f'model.layers.{layer}.mlp.down_proj'][0]\n",
    "    vllm_down_output = vllm_saved_activations[f'model.layers.{layer}.mlp.down_proj'][0]\n",
    "    deltas[\"down\"].append((hf_down_output - vllm_down_output).norm(p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "033f6a59-08d2-48c0-9c76-510bfc344d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = {k:torch.stack(v).float().numpy() for k,v in deltas.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "3ca8f38a-abff-4ca2-90a6-8b969768d4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1803896350>"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,7))\n",
    "\n",
    "# Loop through the dictionary and plot each line with a label\n",
    "for key, values in deltas.items():\n",
    "    ax.plot(values, label=key)\n",
    "\n",
    "# Adding a legend to distinguish the lines\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c63e3-5af5-4550-92b6-13d737d60816",
   "metadata": {},
   "source": [
    "### Loaded vs Saved Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cbda28c-cab0-476e-924b-6a6cced74626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quant_state(absmax, shape):\n",
    "    quant_state = QuantState(absmax, dtype=torch.bfloat16)\n",
    "    quant_state.shape = torch.Size(shape)\n",
    "    quant_state.blocksize = 64\n",
    "    quant_state.quant_type = \"nf4\"\n",
    "    quant_state.code = quant_map\n",
    "    return quant_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3ecb93c-cce9-44a7-91a8-a5f3e7aa1dff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cheking: model.embed_tokens.weight\n",
      "Cheking: model.layers.0.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.0.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.0.self_attn.o_proj.weight\n",
      "Cheking: model.layers.0.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.0.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.0.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.0.mlp.down_proj.weight\n",
      "Cheking: model.layers.0.mlp.down_proj.absmax\n",
      "Cheking: model.layers.0.input_layernorm.weight\n",
      "Cheking: model.layers.0.post_attention_layernorm.weight\n",
      "Cheking: model.layers.1.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.1.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.1.self_attn.o_proj.weight\n",
      "Cheking: model.layers.1.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.1.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.1.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.1.mlp.down_proj.weight\n",
      "Cheking: model.layers.1.mlp.down_proj.absmax\n",
      "Cheking: model.layers.1.input_layernorm.weight\n",
      "Cheking: model.layers.1.post_attention_layernorm.weight\n",
      "Cheking: model.layers.2.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.2.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.2.self_attn.o_proj.weight\n",
      "Cheking: model.layers.2.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.2.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.2.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.2.mlp.down_proj.weight\n",
      "Cheking: model.layers.2.mlp.down_proj.absmax\n",
      "Cheking: model.layers.2.input_layernorm.weight\n",
      "Cheking: model.layers.2.post_attention_layernorm.weight\n",
      "Cheking: model.layers.3.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.3.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.3.self_attn.o_proj.weight\n",
      "Cheking: model.layers.3.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.3.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.3.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.3.mlp.down_proj.weight\n",
      "Cheking: model.layers.3.mlp.down_proj.absmax\n",
      "Cheking: model.layers.3.input_layernorm.weight\n",
      "Cheking: model.layers.3.post_attention_layernorm.weight\n",
      "Cheking: model.layers.4.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.4.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.4.self_attn.o_proj.weight\n",
      "Cheking: model.layers.4.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.4.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.4.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.4.mlp.down_proj.weight\n",
      "Cheking: model.layers.4.mlp.down_proj.absmax\n",
      "Cheking: model.layers.4.input_layernorm.weight\n",
      "Cheking: model.layers.4.post_attention_layernorm.weight\n",
      "Cheking: model.layers.5.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.5.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.5.self_attn.o_proj.weight\n",
      "Cheking: model.layers.5.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.5.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.5.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.5.mlp.down_proj.weight\n",
      "Cheking: model.layers.5.mlp.down_proj.absmax\n",
      "Cheking: model.layers.5.input_layernorm.weight\n",
      "Cheking: model.layers.5.post_attention_layernorm.weight\n",
      "Cheking: model.layers.6.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.6.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.6.self_attn.o_proj.weight\n",
      "Cheking: model.layers.6.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.6.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.6.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.6.mlp.down_proj.weight\n",
      "Cheking: model.layers.6.mlp.down_proj.absmax\n",
      "Cheking: model.layers.6.input_layernorm.weight\n",
      "Cheking: model.layers.6.post_attention_layernorm.weight\n",
      "Cheking: model.layers.7.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.7.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.7.self_attn.o_proj.weight\n",
      "Cheking: model.layers.7.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.7.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.7.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.7.mlp.down_proj.weight\n",
      "Cheking: model.layers.7.mlp.down_proj.absmax\n",
      "Cheking: model.layers.7.input_layernorm.weight\n",
      "Cheking: model.layers.7.post_attention_layernorm.weight\n",
      "Cheking: model.layers.8.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.8.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.8.self_attn.o_proj.weight\n",
      "Cheking: model.layers.8.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.8.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.8.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.8.mlp.down_proj.weight\n",
      "Cheking: model.layers.8.mlp.down_proj.absmax\n",
      "Cheking: model.layers.8.input_layernorm.weight\n",
      "Cheking: model.layers.8.post_attention_layernorm.weight\n",
      "Cheking: model.layers.9.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.9.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.9.self_attn.o_proj.weight\n",
      "Cheking: model.layers.9.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.9.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.9.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.9.mlp.down_proj.weight\n",
      "Cheking: model.layers.9.mlp.down_proj.absmax\n",
      "Cheking: model.layers.9.input_layernorm.weight\n",
      "Cheking: model.layers.9.post_attention_layernorm.weight\n",
      "Cheking: model.layers.10.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.10.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.10.self_attn.o_proj.weight\n",
      "Cheking: model.layers.10.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.10.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.10.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.10.mlp.down_proj.weight\n",
      "Cheking: model.layers.10.mlp.down_proj.absmax\n",
      "Cheking: model.layers.10.input_layernorm.weight\n",
      "Cheking: model.layers.10.post_attention_layernorm.weight\n",
      "Cheking: model.layers.11.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.11.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.11.self_attn.o_proj.weight\n",
      "Cheking: model.layers.11.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.11.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.11.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.11.mlp.down_proj.weight\n",
      "Cheking: model.layers.11.mlp.down_proj.absmax\n",
      "Cheking: model.layers.11.input_layernorm.weight\n",
      "Cheking: model.layers.11.post_attention_layernorm.weight\n",
      "Cheking: model.layers.12.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.12.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.12.self_attn.o_proj.weight\n",
      "Cheking: model.layers.12.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.12.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.12.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.12.mlp.down_proj.weight\n",
      "Cheking: model.layers.12.mlp.down_proj.absmax\n",
      "Cheking: model.layers.12.input_layernorm.weight\n",
      "Cheking: model.layers.12.post_attention_layernorm.weight\n",
      "Cheking: model.layers.13.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.13.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.13.self_attn.o_proj.weight\n",
      "Cheking: model.layers.13.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.13.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.13.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.13.mlp.down_proj.weight\n",
      "Cheking: model.layers.13.mlp.down_proj.absmax\n",
      "Cheking: model.layers.13.input_layernorm.weight\n",
      "Cheking: model.layers.13.post_attention_layernorm.weight\n",
      "Cheking: model.layers.14.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.14.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.14.self_attn.o_proj.weight\n",
      "Cheking: model.layers.14.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.14.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.14.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.14.mlp.down_proj.weight\n",
      "Cheking: model.layers.14.mlp.down_proj.absmax\n",
      "Cheking: model.layers.14.input_layernorm.weight\n",
      "Cheking: model.layers.14.post_attention_layernorm.weight\n",
      "Cheking: model.layers.15.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.15.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.15.self_attn.o_proj.weight\n",
      "Cheking: model.layers.15.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.15.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.15.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.15.mlp.down_proj.weight\n",
      "Cheking: model.layers.15.mlp.down_proj.absmax\n",
      "Cheking: model.layers.15.input_layernorm.weight\n",
      "Cheking: model.layers.15.post_attention_layernorm.weight\n",
      "Cheking: model.layers.16.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.16.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.16.self_attn.o_proj.weight\n",
      "Cheking: model.layers.16.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.16.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.16.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.16.mlp.down_proj.weight\n",
      "Cheking: model.layers.16.mlp.down_proj.absmax\n",
      "Cheking: model.layers.16.input_layernorm.weight\n",
      "Cheking: model.layers.16.post_attention_layernorm.weight\n",
      "Cheking: model.layers.17.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.17.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.17.self_attn.o_proj.weight\n",
      "Cheking: model.layers.17.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.17.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.17.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.17.mlp.down_proj.weight\n",
      "Cheking: model.layers.17.mlp.down_proj.absmax\n",
      "Cheking: model.layers.17.input_layernorm.weight\n",
      "Cheking: model.layers.17.post_attention_layernorm.weight\n",
      "Cheking: model.layers.18.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.18.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.18.self_attn.o_proj.weight\n",
      "Cheking: model.layers.18.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.18.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.18.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.18.mlp.down_proj.weight\n",
      "Cheking: model.layers.18.mlp.down_proj.absmax\n",
      "Cheking: model.layers.18.input_layernorm.weight\n",
      "Cheking: model.layers.18.post_attention_layernorm.weight\n",
      "Cheking: model.layers.19.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.19.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.19.self_attn.o_proj.weight\n",
      "Cheking: model.layers.19.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.19.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.19.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.19.mlp.down_proj.weight\n",
      "Cheking: model.layers.19.mlp.down_proj.absmax\n",
      "Cheking: model.layers.19.input_layernorm.weight\n",
      "Cheking: model.layers.19.post_attention_layernorm.weight\n",
      "Cheking: model.layers.20.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.20.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.20.self_attn.o_proj.weight\n",
      "Cheking: model.layers.20.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.20.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.20.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.20.mlp.down_proj.weight\n",
      "Cheking: model.layers.20.mlp.down_proj.absmax\n",
      "Cheking: model.layers.20.input_layernorm.weight\n",
      "Cheking: model.layers.20.post_attention_layernorm.weight\n",
      "Cheking: model.layers.21.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.21.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.21.self_attn.o_proj.weight\n",
      "Cheking: model.layers.21.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.21.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.21.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.21.mlp.down_proj.weight\n",
      "Cheking: model.layers.21.mlp.down_proj.absmax\n",
      "Cheking: model.layers.21.input_layernorm.weight\n",
      "Cheking: model.layers.21.post_attention_layernorm.weight\n",
      "Cheking: model.layers.22.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.22.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.22.self_attn.o_proj.weight\n",
      "Cheking: model.layers.22.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.22.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.22.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.22.mlp.down_proj.weight\n",
      "Cheking: model.layers.22.mlp.down_proj.absmax\n",
      "Cheking: model.layers.22.input_layernorm.weight\n",
      "Cheking: model.layers.22.post_attention_layernorm.weight\n",
      "Cheking: model.layers.23.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.23.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.23.self_attn.o_proj.weight\n",
      "Cheking: model.layers.23.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.23.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.23.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.23.mlp.down_proj.weight\n",
      "Cheking: model.layers.23.mlp.down_proj.absmax\n",
      "Cheking: model.layers.23.input_layernorm.weight\n",
      "Cheking: model.layers.23.post_attention_layernorm.weight\n",
      "Cheking: model.layers.24.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.24.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.24.self_attn.o_proj.weight\n",
      "Cheking: model.layers.24.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.24.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.24.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.24.mlp.down_proj.weight\n",
      "Cheking: model.layers.24.mlp.down_proj.absmax\n",
      "Cheking: model.layers.24.input_layernorm.weight\n",
      "Cheking: model.layers.24.post_attention_layernorm.weight\n",
      "Cheking: model.layers.25.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.25.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.25.self_attn.o_proj.weight\n",
      "Cheking: model.layers.25.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.25.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.25.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.25.mlp.down_proj.weight\n",
      "Cheking: model.layers.25.mlp.down_proj.absmax\n",
      "Cheking: model.layers.25.input_layernorm.weight\n",
      "Cheking: model.layers.25.post_attention_layernorm.weight\n",
      "Cheking: model.layers.26.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.26.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.26.self_attn.o_proj.weight\n",
      "Cheking: model.layers.26.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.26.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.26.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.26.mlp.down_proj.weight\n",
      "Cheking: model.layers.26.mlp.down_proj.absmax\n",
      "Cheking: model.layers.26.input_layernorm.weight\n",
      "Cheking: model.layers.26.post_attention_layernorm.weight\n",
      "Cheking: model.layers.27.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.27.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.27.self_attn.o_proj.weight\n",
      "Cheking: model.layers.27.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.27.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.27.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.27.mlp.down_proj.weight\n",
      "Cheking: model.layers.27.mlp.down_proj.absmax\n",
      "Cheking: model.layers.27.input_layernorm.weight\n",
      "Cheking: model.layers.27.post_attention_layernorm.weight\n",
      "Cheking: model.layers.28.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.28.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.28.self_attn.o_proj.weight\n",
      "Cheking: model.layers.28.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.28.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.28.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.28.mlp.down_proj.weight\n",
      "Cheking: model.layers.28.mlp.down_proj.absmax\n",
      "Cheking: model.layers.28.input_layernorm.weight\n",
      "Cheking: model.layers.28.post_attention_layernorm.weight\n",
      "Cheking: model.layers.29.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.29.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.29.self_attn.o_proj.weight\n",
      "Cheking: model.layers.29.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.29.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.29.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.29.mlp.down_proj.weight\n",
      "Cheking: model.layers.29.mlp.down_proj.absmax\n",
      "Cheking: model.layers.29.input_layernorm.weight\n",
      "Cheking: model.layers.29.post_attention_layernorm.weight\n",
      "Cheking: model.layers.30.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.30.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.30.self_attn.o_proj.weight\n",
      "Cheking: model.layers.30.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.30.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.30.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.30.mlp.down_proj.weight\n",
      "Cheking: model.layers.30.mlp.down_proj.absmax\n",
      "Cheking: model.layers.30.input_layernorm.weight\n",
      "Cheking: model.layers.30.post_attention_layernorm.weight\n",
      "Cheking: model.layers.31.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.31.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.31.self_attn.o_proj.weight\n",
      "Cheking: model.layers.31.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.31.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.31.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.31.mlp.down_proj.weight\n",
      "Cheking: model.layers.31.mlp.down_proj.absmax\n",
      "Cheking: model.layers.31.input_layernorm.weight\n",
      "Cheking: model.layers.31.post_attention_layernorm.weight\n",
      "Cheking: model.norm.weight\n",
      "Cheking: lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check if weights are loaded correctly into vllm model.\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "\n",
    "    print(\"Cheking:\", n)\n",
    "    \n",
    "    if 'qkv_proj' in n:\n",
    "        if 'absmax' in n: continue\n",
    "        \n",
    "        # Loaded qkv\n",
    "        qkv_weight = model.get_parameter(n)\n",
    "        qkv_absmax = model.get_parameter(n.replace(\".weight\", \".absmax\"))\n",
    "        qkv_shape = [qkv_weight.shape[0], qkv_weight.shape[1] * 2]\n",
    "        q_shape   = [qkv_weight.shape[0], qkv_weight.shape[1] * 2 // 3]\n",
    "        \n",
    "        absmax = qkv_absmax.contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, qkv_shape)\n",
    "        \n",
    "        W_dq = bnb.functional.dequantize_4bit(qkv_weight.contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "        # Saved q proj\n",
    "        q_proj_weight_name = n.replace(\"qkv_proj\", \"q_proj\")\n",
    "        q_proj_absmax_name = n.replace(\"qkv_proj\", \"q_proj\").replace(\".weight\", \".absmax\")\n",
    "        \n",
    "        absmax = quantized_state_dict[q_proj_absmax_name].cuda().contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, q_shape)\n",
    "        \n",
    "        W_q_proj_dq = bnb.functional.dequantize_4bit(quantized_state_dict[q_proj_weight_name].contiguous().view(-1,1).cuda(), quant_state=quant_state)\n",
    "\n",
    "        # Compare with HF model state dict\n",
    "        param = Params4bit(hf_state_dict[q_proj_weight_name].t(), blocksize=64, compress_statistics=False, quant_type='nf4').cuda()\n",
    "        W_q_proj_dq_hf = dequantize_4bit(param.data, param.quant_state)\n",
    "        \n",
    "        # Saved k proj\n",
    "        k_proj_weight_name = n.replace(\"qkv_proj\", \"k_proj\")\n",
    "        k_proj_absmax_name = n.replace(\"qkv_proj\", \"k_proj\").replace(\".weight\", \".absmax\")\n",
    "        \n",
    "        absmax = quantized_state_dict[k_proj_absmax_name].cuda().contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, q_shape)\n",
    "        \n",
    "        W_k_proj_dq = bnb.functional.dequantize_4bit(quantized_state_dict[k_proj_weight_name].contiguous().view(-1,1).cuda(), quant_state=quant_state)\n",
    "\n",
    "        # Compare with HF model state dict\n",
    "        param = Params4bit(hf_state_dict[k_proj_weight_name].t(), blocksize=64, compress_statistics=False, quant_type='nf4').cuda()\n",
    "        W_k_proj_dq_hf = dequantize_4bit(param.data, param.quant_state)        \n",
    "\n",
    "        # Saved v proj\n",
    "        v_proj_weight_name = n.replace(\"qkv_proj\", \"v_proj\")\n",
    "        v_proj_absmax_name = n.replace(\"qkv_proj\", \"v_proj\").replace(\".weight\", \".absmax\")\n",
    "\n",
    "        absmax = quantized_state_dict[v_proj_absmax_name].cuda().contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, q_shape)\n",
    "\n",
    "        W_v_proj_dq = bnb.functional.dequantize_4bit(quantized_state_dict[v_proj_weight_name].contiguous().view(-1,1).cuda(), quant_state=quant_state)\n",
    "\n",
    "         # Compare with HF model state dict\n",
    "        param = Params4bit(hf_state_dict[v_proj_weight_name].t(), blocksize=64, compress_statistics=False, quant_type='nf4').cuda()\n",
    "        W_v_proj_dq_hf = dequantize_4bit(param.data, param.quant_state)       \n",
    "\n",
    "        # Check whether loaded vs saved weights are equal after dequantization.\n",
    "        assert torch.equal(W_dq, torch.cat([W_q_proj_dq, W_k_proj_dq, W_v_proj_dq], dim=1))\n",
    "\n",
    "        assert torch.equal(W_dq, torch.cat([W_q_proj_dq_hf, W_k_proj_dq_hf, W_v_proj_dq_hf], dim=1))\n",
    "    \n",
    "    \n",
    "    elif 'gate_up_proj' in n:\n",
    "        if 'absmax' in n: continue\n",
    "            \n",
    "        # Loaded gate_up\n",
    "        gate_up_weight = model.get_parameter(n)\n",
    "        gate_up_absmax = model.get_parameter(n.replace(\".weight\", \".absmax\"))\n",
    "        gate_up_shape = [gate_up_weight.shape[0], gate_up_weight.shape[1] * 2]\n",
    "        gate_shape    = [gate_up_weight.shape[0], gate_up_weight.shape[1] * 2 // 2]\n",
    "\n",
    "        absmax = gate_up_absmax.contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, gate_up_shape)\n",
    "        \n",
    "        W_dq = bnb.functional.dequantize_4bit(gate_up_weight.contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "        # Saved gate_proj\n",
    "        gate_proj_weight_name = n.replace(\"gate_up_proj\", \"gate_proj\")\n",
    "        gate_proj_absmax_name = n.replace(\"gate_up_proj\", \"gate_proj\").replace(\".weight\", \".absmax\")\n",
    "        \n",
    "        absmax = quantized_state_dict[gate_proj_absmax_name].cuda().contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, gate_shape)\n",
    "        \n",
    "        W_gate_proj_dq = bnb.functional.dequantize_4bit(quantized_state_dict[gate_proj_weight_name].contiguous().view(-1,1).cuda(), quant_state=quant_state)\n",
    "\n",
    "        # Saved up_proj\n",
    "        up_proj_weight_name = n.replace(\"gate_up_proj\", \"up_proj\")\n",
    "        up_proj_absmax_name = n.replace(\"gate_up_proj\", \"up_proj\").replace(\".weight\", \".absmax\")\n",
    "        \n",
    "        absmax = quantized_state_dict[up_proj_absmax_name].cuda().contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, gate_shape)\n",
    "\n",
    "        W_up_proj_dq = bnb.functional.dequantize_4bit(quantized_state_dict[up_proj_weight_name].contiguous().view(-1,1).cuda(), quant_state=quant_state)\n",
    "\n",
    "        # Check whether loaded vs saved weights are equal after dequantization.\n",
    "        assert torch.equal(W_dq, torch.cat([W_gate_proj_dq, W_up_proj_dq], dim=1))\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        # Check whether loaded vs saved weights are equal after dequantization.\n",
    "        assert torch.equal(quantized_state_dict[n].data, p.data.cpu())\n",
    "        \n",
    "        # Loaded gate_up\n",
    "        if any(l in n for l in [\"o_proj\", \"down_proj\"]):\n",
    "            if \"weight\" in n:\n",
    "                weight = model.get_parameter(n)\n",
    "                absmax = model.get_parameter(n.replace(\".weight\", \".absmax\"))\n",
    "                shape = [weight.shape[0], weight.shape[1] * 2]\n",
    "                absmax = absmax.contiguous().view(-1)\n",
    "                quant_state = create_quant_state(absmax, shape)\n",
    "                W_dq = bnb.functional.dequantize_4bit(weight.contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "                # Compare with HF model state dict\n",
    "                param = Params4bit(hf_state_dict[n].t(), blocksize=64, compress_statistics=False, quant_type='nf4').cuda()\n",
    "                assert torch.equal(W_dq, dequantize_4bit(param.data, param.quant_state))\n",
    "                \n",
    "        else:\n",
    "            # Compare with HF model state dict\n",
    "            assert torch.equal(quantized_state_dict[n].data, hf_state_dict[n])\n",
    "\n",
    "    \n",
    "    if any(l in n for l in [\"qkv_proj\", \"o_proj\", \"gate_up_proj\", \"down_proj\"]) and \"weight\" in n:\n",
    "        module = model.get_submodule(n.rpartition(\".\")[0])\n",
    "        input_size = module.weight.shape[0]\n",
    "        x = torch.randn(1,input_size).cuda().to(torch.bfloat16)\n",
    "        out1 = module(x)\n",
    "        if len(out1) > 1: out1 = out1[0]\n",
    "        out2 = x @ W_dq\n",
    "        \n",
    "        # Check forward pass is correct.\n",
    "        assert torch.equal(out1, out2)\n",
    "\n",
    "    # print(p.view(-1)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3e381a01-7457-4cf6-8994-bc9561af3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check activations VLLM bnb vs HF bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207aba1d-b5e5-42cb-b136-bd2fcdb06694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74e0f2-b15e-4457-8ee2-85e67325aad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2aa61-6684-4ce2-b8c7-c31a3a500697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c4344-3699-4f0f-b3a5-ed2045c2c14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde15183-ee7a-4c32-9406-8a5b4238f79d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32091d7a-06b4-488f-a37d-21d37fd1a59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e6dd3-f798-4ea0-8642-6448cffacaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b433344-8ff8-432c-8638-776c9e2545c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258bca3-acae-4213-b12b-d53b6a201751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5694bb7-06bf-4b63-9ddf-f633e3a3f690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279b99cc-49cf-4b7e-a5c7-95c7f98bcba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
