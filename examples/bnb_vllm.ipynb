{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22d8c79-3add-4d49-9cae-e212b493467e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### BNB with Tensor Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff26655-6d13-479a-9af8-9d64083ec9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from bitsandbytes.nn.modules import Params4bit, Linear4bit\n",
    "from bitsandbytes.functional import dequantize_4bit\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.functional import dequantize_4bit, QuantState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c906c609-2777-4148-acac-700d72842b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea59055-c341-47c3-86b9-f48415d239d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocksize = 64\n",
    "quant_type = \"nf4\"\n",
    "quant_storage = torch.uint8\n",
    "\n",
    "data = torch.randn(128,256).to(torch.bfloat16)\n",
    "param = Params4bit(data, blocksize=blocksize, quant_type=quant_type, \n",
    "                   quant_storage=quant_storage, compress_statistics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b7929ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if quant_storage == torch.uint8:\n",
    "    pack_factor = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6890d631-c63f-4939-a5f4-00883a324aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Parameter(Params4bit([[-0.1260, -1.6250,  0.0508,  ...,  0.3379, -1.7500,  1.1484],\n",
       "            [-0.2305, -1.3047, -2.0312,  ..., -0.1328, -0.3125,  1.8594],\n",
       "            [-1.1875,  0.9766,  0.0840,  ..., -2.2812,  0.6016,  1.1328],\n",
       "            ...,\n",
       "            [ 1.1875,  1.0234,  1.3281,  ...,  1.5781, -0.7227,  0.5156],\n",
       "            [ 0.0610, -0.3477,  1.2578,  ..., -1.7109,  1.0781,  0.6914],\n",
       "            [ 0.2002, -0.7422,  0.0527,  ...,  1.1953, -0.0952,  0.0854]],\n",
       "           dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "222fae9b-6520-46de-bd43-21867026cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "920862c7-0c28-43cf-ba29-5a624eadaace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16384, 1]), 'nf4', torch.uint8, 64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.shape, param.quant_type, param.quant_storage, param.blocksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78e79d3e-ca1c-49e8-8da1-8578b346d2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a273b007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.numel() / data.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a3b663b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_type': 'nf4',\n",
       " 'absmax': tensor([2.6406, 2.4062, 3.7812, 2.2344, 2.7500, 3.2500, 2.2500, 2.8594, 2.7656,\n",
       "         2.4688, 3.2969, 3.4844, 2.7812, 2.2188, 2.9375, 3.2812, 2.1094, 2.8125,\n",
       "         2.2344, 3.2344, 3.2188, 2.8125, 2.3906, 2.2812, 2.7031, 3.5938, 2.5156,\n",
       "         2.9219, 2.1875, 3.0469, 2.5781, 2.5781, 2.2188, 2.8125, 2.3281, 2.6094,\n",
       "         2.9219, 2.7344, 2.1719, 3.4375, 2.5000, 3.0625, 2.4688, 2.5625, 2.9531,\n",
       "         2.8125, 2.7969, 2.2500, 3.6094, 2.4062, 2.0781, 2.4531, 3.3281, 2.6250,\n",
       "         3.1250, 2.7969, 2.8594, 2.4844, 2.8906, 2.5000, 2.1719, 3.1406, 2.6094,\n",
       "         2.4375, 3.0000, 2.2188, 3.4062, 3.9688, 2.6250, 2.5938, 2.2500, 3.0000,\n",
       "         2.7188, 2.5625, 2.5156, 3.3750, 2.5469, 2.8125, 3.0781, 2.4688, 2.5781,\n",
       "         3.3281, 2.5781, 1.7109, 2.9375, 2.0781, 2.2031, 2.3750, 2.0312, 2.3750,\n",
       "         2.4688, 2.6406, 2.2812, 2.2344, 3.2031, 2.5312, 1.8203, 2.3438, 2.0625,\n",
       "         2.7500, 2.5312, 2.6094, 2.4844, 2.7188, 2.6562, 2.1094, 1.8594, 2.4688,\n",
       "         2.2188, 3.0312, 3.4062, 2.2031, 2.2188, 2.0000, 2.8438, 2.5156, 2.3594,\n",
       "         1.9844, 2.9531, 2.7344, 2.8750, 2.0156, 2.1406, 2.9844, 3.0312, 2.7344,\n",
       "         2.2656, 2.5938, 3.5625, 2.8750, 1.8672, 2.4531, 1.8672, 2.8281, 2.2500,\n",
       "         2.0312, 2.9375, 2.1719, 1.9531, 2.0938, 2.5625, 3.2812, 2.4375, 2.8750,\n",
       "         2.9062, 2.0469, 2.7188, 2.5000, 2.7500, 2.1094, 1.7344, 2.3750, 2.8281,\n",
       "         3.1250, 3.2344, 2.8906, 2.2344, 1.9844, 4.2812, 2.6562, 2.5938, 2.3750,\n",
       "         2.8750, 2.1719, 3.1250, 2.9219, 3.5469, 2.4844, 2.6250, 1.9922, 2.3125,\n",
       "         2.4844, 2.7656, 2.3281, 2.7969, 3.2656, 2.2188, 2.1875, 2.2500, 2.3906,\n",
       "         2.2344, 2.5156, 3.3125, 1.9922, 1.7344, 2.6406, 2.1406, 2.5000, 2.1562,\n",
       "         2.5469, 2.1875, 1.9219, 1.9609, 2.4219, 2.8438, 2.0938, 2.9062, 1.8750,\n",
       "         2.6719, 2.5312, 3.2969, 2.6250, 2.2812, 2.4219, 2.1875, 2.6719, 3.2656,\n",
       "         2.3906, 1.8281, 2.4844, 2.8438, 2.5156, 2.6562, 2.4219, 2.0000, 2.8906,\n",
       "         2.5312, 2.7188, 2.7500, 3.4375, 2.6094, 2.7500, 1.7734, 2.1250, 2.4531,\n",
       "         2.4688, 2.7812, 2.5469, 2.0312, 1.8906, 3.6406, 2.3281, 2.6094, 2.5938,\n",
       "         2.1562, 2.4531, 2.1719, 2.1406, 2.2969, 2.1406, 3.3906, 2.3594, 2.2812,\n",
       "         2.6250, 2.7812, 2.5938, 2.6562, 2.6250, 2.3750, 3.3750, 2.2500, 2.1719,\n",
       "         2.8750, 2.1719, 2.4375, 2.4062, 2.5469, 2.9688, 2.1719, 1.8906, 2.2812,\n",
       "         2.2969, 2.6875, 2.7656, 3.5312, 2.4844, 2.2812, 3.7656, 3.4375, 2.9531,\n",
       "         2.2031, 2.7812, 3.0781, 2.5625, 2.1875, 2.1562, 3.2344, 2.3438, 2.2969,\n",
       "         2.1875, 2.4844, 2.6719, 2.1562, 2.2969, 2.7812, 3.2969, 1.9688, 2.1719,\n",
       "         2.9219, 1.9609, 3.2969, 3.0312, 2.6562, 2.8438, 2.7656, 2.5156, 2.5312,\n",
       "         2.4375, 2.8750, 3.1250, 2.0938, 2.5312, 2.4062, 2.5938, 3.1406, 2.2812,\n",
       "         2.7188, 3.2812, 3.0781, 2.8750, 2.3906, 2.2344, 2.4062, 1.9922, 3.5312,\n",
       "         2.6250, 2.0625, 2.9375, 2.1875, 2.4062, 2.7500, 2.7188, 2.1250, 2.3125,\n",
       "         3.2656, 2.3281, 2.9375, 2.6719, 2.2188, 2.4844, 2.5781, 2.4375, 3.7031,\n",
       "         3.4844, 2.7188, 2.4688, 2.2969, 2.9219, 3.0000, 1.9219, 2.0312, 2.8125,\n",
       "         1.9766, 3.5938, 2.9844, 2.9219, 2.0938, 2.2188, 2.7188, 2.6094, 2.6406,\n",
       "         2.6562, 2.4688, 2.0156, 3.0156, 3.0625, 2.1250, 2.7188, 2.5000, 2.9531,\n",
       "         2.7188, 2.4375, 2.7031, 2.7188, 2.8594, 2.6719, 2.0156, 2.3281, 2.8750,\n",
       "         2.1562, 2.5312, 2.9062, 2.1250, 2.5938, 2.4688, 2.7812, 2.9219, 2.2188,\n",
       "         1.8516, 2.5938, 3.1406, 2.6719, 1.7656, 2.6250, 2.0156, 2.6875, 2.2500,\n",
       "         3.1719, 2.5000, 3.1250, 1.6875, 3.0000, 2.0625, 2.7500, 2.9844, 2.4844,\n",
       "         2.9375, 3.5000, 1.8906, 2.4219, 2.1406, 3.0312, 2.8125, 1.9922, 2.1562,\n",
       "         1.7656, 2.3281, 2.6094, 2.4531, 2.7500, 2.5156, 2.3281, 3.1875, 1.6953,\n",
       "         2.4219, 3.2812, 2.3438, 2.3281, 3.0781, 2.5000, 2.6719, 2.2812, 1.8281,\n",
       "         2.0781, 2.7656, 2.8438, 2.3594, 2.1875, 1.9297, 2.6562, 2.7344, 2.3594,\n",
       "         2.9062, 2.2969, 3.0469, 2.9531, 2.8281, 3.0625, 2.3438, 2.0000, 2.9219,\n",
       "         3.1875, 2.3906, 2.7344, 2.1719, 2.9219, 2.3906, 3.0469, 2.0938, 2.2031,\n",
       "         2.8906, 2.5156, 2.3125, 1.7734, 3.5156, 2.4688, 3.5000, 2.2969, 2.3750,\n",
       "         2.2188, 2.5625, 2.4375, 2.1250, 2.8750, 2.7031, 2.8594, 2.3125, 3.4531,\n",
       "         3.0938, 2.6094, 2.7656, 2.9062, 2.4375, 2.7031, 2.5781, 2.6094, 2.2812,\n",
       "         2.9375, 2.2031, 3.0156, 2.2812, 2.5625, 3.1406, 2.4062, 1.9062, 2.8438,\n",
       "         2.4375, 2.5156, 2.7031, 2.1406, 3.0469, 2.5469, 2.0469, 2.4062, 2.2344,\n",
       "         2.2188, 2.3594, 2.3438, 1.7656, 1.8984, 2.7656, 2.0938, 2.8125, 2.5625,\n",
       "         2.0781, 2.5312, 3.1562, 2.5000, 2.3438, 2.4062, 2.9375, 2.5312],\n",
       "        device='cuda:0'),\n",
       " 'blocksize': 64,\n",
       " 'quant_map': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
       "          0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
       "        device='cuda:0'),\n",
       " 'dtype': 'bfloat16',\n",
       " 'shape': (128, 256)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.quant_state.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b110f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size_per_partition = 64\n",
    "output_size_per_partition = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c5479f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row-major quantization, reshape for vllm tensor parallelism\n",
    "qweight = param.data.reshape(data.size(0), data.size(1) // pack_factor); qweight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd34c2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 97, 119,  55,  ...,  65, 201,  29],\n",
       "        [ 98,  24,  97,  ..., 156, 134, 110],\n",
       "        [ 59, 112,  71,  ...,  58,  81, 155],\n",
       "        ...,\n",
       "        [203, 201, 227,  ...,  20, 173,  73],\n",
       "        [117, 217, 181,  ...,  65,  97, 202],\n",
       "        [132, 122, 134,  ...,  38,  44, 119]], device='cuda:0',\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b3a5601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 97],\n",
       "        [119],\n",
       "        [ 55],\n",
       "        ...,\n",
       "        [ 38],\n",
       "        [ 44],\n",
       "        [119]], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qweight.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11e42bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deqweight = dequantize_4bit(qweight.view(-1,1), param.quant_state, blocksize=blocksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e794c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(254., dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data - torch.randn_like(data)).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4c3eb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.5000, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data - deqweight.cpu()).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c66129c-c120-4b67-bf0c-fee3d8e94a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 128).cuda().to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed100e7d-5097-4682-9a6d-333f4434c877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_type': 'nf4',\n",
       " 'absmax': tensor([2.6406, 2.4062, 3.7812, 2.2344, 2.7500, 3.2500, 2.2500, 2.8594, 2.7656,\n",
       "         2.4688, 3.2969, 3.4844, 2.7812, 2.2188, 2.9375, 3.2812, 2.1094, 2.8125,\n",
       "         2.2344, 3.2344, 3.2188, 2.8125, 2.3906, 2.2812, 2.7031, 3.5938, 2.5156,\n",
       "         2.9219, 2.1875, 3.0469, 2.5781, 2.5781, 2.2188, 2.8125, 2.3281, 2.6094,\n",
       "         2.9219, 2.7344, 2.1719, 3.4375, 2.5000, 3.0625, 2.4688, 2.5625, 2.9531,\n",
       "         2.8125, 2.7969, 2.2500, 3.6094, 2.4062, 2.0781, 2.4531, 3.3281, 2.6250,\n",
       "         3.1250, 2.7969, 2.8594, 2.4844, 2.8906, 2.5000, 2.1719, 3.1406, 2.6094,\n",
       "         2.4375, 3.0000, 2.2188, 3.4062, 3.9688, 2.6250, 2.5938, 2.2500, 3.0000,\n",
       "         2.7188, 2.5625, 2.5156, 3.3750, 2.5469, 2.8125, 3.0781, 2.4688, 2.5781,\n",
       "         3.3281, 2.5781, 1.7109, 2.9375, 2.0781, 2.2031, 2.3750, 2.0312, 2.3750,\n",
       "         2.4688, 2.6406, 2.2812, 2.2344, 3.2031, 2.5312, 1.8203, 2.3438, 2.0625,\n",
       "         2.7500, 2.5312, 2.6094, 2.4844, 2.7188, 2.6562, 2.1094, 1.8594, 2.4688,\n",
       "         2.2188, 3.0312, 3.4062, 2.2031, 2.2188, 2.0000, 2.8438, 2.5156, 2.3594,\n",
       "         1.9844, 2.9531, 2.7344, 2.8750, 2.0156, 2.1406, 2.9844, 3.0312, 2.7344,\n",
       "         2.2656, 2.5938, 3.5625, 2.8750, 1.8672, 2.4531, 1.8672, 2.8281, 2.2500,\n",
       "         2.0312, 2.9375, 2.1719, 1.9531, 2.0938, 2.5625, 3.2812, 2.4375, 2.8750,\n",
       "         2.9062, 2.0469, 2.7188, 2.5000, 2.7500, 2.1094, 1.7344, 2.3750, 2.8281,\n",
       "         3.1250, 3.2344, 2.8906, 2.2344, 1.9844, 4.2812, 2.6562, 2.5938, 2.3750,\n",
       "         2.8750, 2.1719, 3.1250, 2.9219, 3.5469, 2.4844, 2.6250, 1.9922, 2.3125,\n",
       "         2.4844, 2.7656, 2.3281, 2.7969, 3.2656, 2.2188, 2.1875, 2.2500, 2.3906,\n",
       "         2.2344, 2.5156, 3.3125, 1.9922, 1.7344, 2.6406, 2.1406, 2.5000, 2.1562,\n",
       "         2.5469, 2.1875, 1.9219, 1.9609, 2.4219, 2.8438, 2.0938, 2.9062, 1.8750,\n",
       "         2.6719, 2.5312, 3.2969, 2.6250, 2.2812, 2.4219, 2.1875, 2.6719, 3.2656,\n",
       "         2.3906, 1.8281, 2.4844, 2.8438, 2.5156, 2.6562, 2.4219, 2.0000, 2.8906,\n",
       "         2.5312, 2.7188, 2.7500, 3.4375, 2.6094, 2.7500, 1.7734, 2.1250, 2.4531,\n",
       "         2.4688, 2.7812, 2.5469, 2.0312, 1.8906, 3.6406, 2.3281, 2.6094, 2.5938,\n",
       "         2.1562, 2.4531, 2.1719, 2.1406, 2.2969, 2.1406, 3.3906, 2.3594, 2.2812,\n",
       "         2.6250, 2.7812, 2.5938, 2.6562, 2.6250, 2.3750, 3.3750, 2.2500, 2.1719,\n",
       "         2.8750, 2.1719, 2.4375, 2.4062, 2.5469, 2.9688, 2.1719, 1.8906, 2.2812,\n",
       "         2.2969, 2.6875, 2.7656, 3.5312, 2.4844, 2.2812, 3.7656, 3.4375, 2.9531,\n",
       "         2.2031, 2.7812, 3.0781, 2.5625, 2.1875, 2.1562, 3.2344, 2.3438, 2.2969,\n",
       "         2.1875, 2.4844, 2.6719, 2.1562, 2.2969, 2.7812, 3.2969, 1.9688, 2.1719,\n",
       "         2.9219, 1.9609, 3.2969, 3.0312, 2.6562, 2.8438, 2.7656, 2.5156, 2.5312,\n",
       "         2.4375, 2.8750, 3.1250, 2.0938, 2.5312, 2.4062, 2.5938, 3.1406, 2.2812,\n",
       "         2.7188, 3.2812, 3.0781, 2.8750, 2.3906, 2.2344, 2.4062, 1.9922, 3.5312,\n",
       "         2.6250, 2.0625, 2.9375, 2.1875, 2.4062, 2.7500, 2.7188, 2.1250, 2.3125,\n",
       "         3.2656, 2.3281, 2.9375, 2.6719, 2.2188, 2.4844, 2.5781, 2.4375, 3.7031,\n",
       "         3.4844, 2.7188, 2.4688, 2.2969, 2.9219, 3.0000, 1.9219, 2.0312, 2.8125,\n",
       "         1.9766, 3.5938, 2.9844, 2.9219, 2.0938, 2.2188, 2.7188, 2.6094, 2.6406,\n",
       "         2.6562, 2.4688, 2.0156, 3.0156, 3.0625, 2.1250, 2.7188, 2.5000, 2.9531,\n",
       "         2.7188, 2.4375, 2.7031, 2.7188, 2.8594, 2.6719, 2.0156, 2.3281, 2.8750,\n",
       "         2.1562, 2.5312, 2.9062, 2.1250, 2.5938, 2.4688, 2.7812, 2.9219, 2.2188,\n",
       "         1.8516, 2.5938, 3.1406, 2.6719, 1.7656, 2.6250, 2.0156, 2.6875, 2.2500,\n",
       "         3.1719, 2.5000, 3.1250, 1.6875, 3.0000, 2.0625, 2.7500, 2.9844, 2.4844,\n",
       "         2.9375, 3.5000, 1.8906, 2.4219, 2.1406, 3.0312, 2.8125, 1.9922, 2.1562,\n",
       "         1.7656, 2.3281, 2.6094, 2.4531, 2.7500, 2.5156, 2.3281, 3.1875, 1.6953,\n",
       "         2.4219, 3.2812, 2.3438, 2.3281, 3.0781, 2.5000, 2.6719, 2.2812, 1.8281,\n",
       "         2.0781, 2.7656, 2.8438, 2.3594, 2.1875, 1.9297, 2.6562, 2.7344, 2.3594,\n",
       "         2.9062, 2.2969, 3.0469, 2.9531, 2.8281, 3.0625, 2.3438, 2.0000, 2.9219,\n",
       "         3.1875, 2.3906, 2.7344, 2.1719, 2.9219, 2.3906, 3.0469, 2.0938, 2.2031,\n",
       "         2.8906, 2.5156, 2.3125, 1.7734, 3.5156, 2.4688, 3.5000, 2.2969, 2.3750,\n",
       "         2.2188, 2.5625, 2.4375, 2.1250, 2.8750, 2.7031, 2.8594, 2.3125, 3.4531,\n",
       "         3.0938, 2.6094, 2.7656, 2.9062, 2.4375, 2.7031, 2.5781, 2.6094, 2.2812,\n",
       "         2.9375, 2.2031, 3.0156, 2.2812, 2.5625, 3.1406, 2.4062, 1.9062, 2.8438,\n",
       "         2.4375, 2.5156, 2.7031, 2.1406, 3.0469, 2.5469, 2.0469, 2.4062, 2.2344,\n",
       "         2.2188, 2.3594, 2.3438, 1.7656, 1.8984, 2.7656, 2.0938, 2.8125, 2.5625,\n",
       "         2.0781, 2.5312, 3.1562, 2.5000, 2.3438, 2.4062, 2.9375, 2.5312],\n",
       "        device='cuda:0'),\n",
       " 'blocksize': 64,\n",
       " 'quant_map': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
       "          0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
       "        device='cuda:0'),\n",
       " 'dtype': 'bfloat16',\n",
       " 'shape': (128, 256)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.quant_state.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d82d89f-235e-4a2a-b0b3-62a3e5d2adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size, output_size = data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea5b382e-e078-4a23-8a95-c5802eca1cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 256)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size, output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a33876",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Column Parallel\n",
    "\n",
    "The linear layer is defined as Y = XA + b. A is parallelized along its second dimension as A = [A_1, ..., A_p]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54220b7c-a24b-4052-8ccc-d0cbe04827ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5883249c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qweight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec7d2b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size_per_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8e60aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "qweight_partitioned = qweight.split(output_size_per_partition, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2f41ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qweight_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7746431f-166b-4a71-959c-83cead869e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n"
     ]
    }
   ],
   "source": [
    "for w in qweight_partitioned: print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be00bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_absmax = param.quant_state.absmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5c02e37-8815-474e-8b2e-da1414790c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_absmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3d776ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_absmax_reshaped = orig_absmax.reshape(input_size, data.size(1) // blocksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7e1670e-a677-47cc-9e67-1ea10d399e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.Size([128, 4]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_absmax_reshaped.dtype, orig_absmax_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dfd94e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = len(qweight_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f63f1e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "absmax_partitioned = orig_absmax_reshaped.split(orig_absmax_reshaped.size(1) // num_partitions, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d276291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2])\n",
      "torch.Size([128, 2])\n"
     ]
    }
   ],
   "source": [
    "for a in absmax_partitioned: print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e9668e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qweight_partitioned), len(absmax_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e7299d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state = copy.deepcopy(param.quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80b8bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state.shape = torch.Size([quant_state.shape[0], quant_state.shape[1]//num_partitions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de428e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "241a2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state.absmax = absmax_partitioned[0].contiguous().view(-1)\n",
    "deqweight_part1 = dequantize_4bit(qweight_partitioned[0].contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "quant_state.absmax = absmax_partitioned[1].contiguous().view(-1)\n",
    "deqweight_part2 = dequantize_4bit(qweight_partitioned[1].contiguous().view(-1,1), quant_state=quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e15c1671-03b7-45a4-b036-72bf3e82c43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 128]), torch.Size([128, 128]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deqweight_part1.shape, deqweight_part2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4085cb3-2b80-492a-b89b-ca259ce950c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# quant_state.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b63ec5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 256])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deqweight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "978e7a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 256])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([deqweight_part1, deqweight_part2], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d50215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(deqweight, torch.cat([deqweight_part1, deqweight_part2], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a43bafd4-7b83-4314-9019-a41499be5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = (x @ deqweight_part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f2fe31f-1af2-4712-9356-b9894c2d7e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out2 = bnb.matmul_4bit(x, qweight_partitioned[1].contiguous().view(-1,1), quant_state=quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bf8b879-052b-48f4-ba5c-d28e1672d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(out1, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af60fb46-97b8-4b65-8f65-ddb11fd02c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc17f5f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Row Parallel\n",
    "\n",
    "The linear layer is defined as Y = XA + b. A is parallelized along\n",
    "its first dimension and X along its second dimension as:\n",
    "\n",
    "```\n",
    "    -   -\n",
    "    | A_1 |\n",
    "    | .   |\n",
    "A = | .   |        X = [X_1, ..., X_p]\n",
    "    | .   |\n",
    "    | A_p |\n",
    "    -   -\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c3dc1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qweight_partitioned = qweight.split(output_size_per_partition, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98d0add2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_partitions = len(qweight_partitioned); num_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77c5bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "for w in qweight_partitioned: print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed58848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_absmax = param.quant_state.absmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e368bab4-38d8-4ee5-9ca7-8a0943fd7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_absmax_reshaped = orig_absmax.reshape(input_size, data.size(1) // blocksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b155bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "absmax_partitioned = orig_absmax.split(len(orig_absmax) // num_partitions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "130b5f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(absmax_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e64143be",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state = copy.deepcopy(param.quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "47f91077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_state.shape = torch.Size([quant_state.shape[0]//num_partitions, quant_state.shape[1]]); quant_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "128e667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_state.absmax = absmax_partitioned[0].contiguous().view(-1)\n",
    "deqweight_part1 = dequantize_4bit(qweight_partitioned[0].contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "quant_state.absmax = absmax_partitioned[1].contiguous().view(-1)\n",
    "deqweight_part2 = dequantize_4bit(qweight_partitioned[1].contiguous().view(-1,1), quant_state=quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d735c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(deqweight, torch.cat([deqweight_part1, deqweight_part2], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53f04e-f053-423b-b3f7-88283531858c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39e0da2-6d9e-4a0e-bec9-7e4007c8af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-29 13:23:10 pynccl_utils.py:13] vLLM is using nccl==2.18.1\n"
     ]
    }
   ],
   "source": [
    "from vllm.model_executor.weight_utils import default_weight_loader, hf_model_weights_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b267b53-a7d4-43af-82e2-ac18b6b66a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_iterator = hf_model_weights_iterator(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdc64de-22cd-413c-9b63-e2351e018c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-29 08:48:45 weight_utils.py:177] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd674cc5dcf7487f923f00c680e6a73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93c344b17544540b2d5f59dac5af4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for name, loaded_weight in weights_iterator: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0853623c-1c05-4599-8b74-26e55d33a310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model.embed_tokens.weight',\n",
       " tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
       "          -6.5565e-06,  8.9407e-07],\n",
       "         [ 1.8616e-03, -3.3722e-03,  3.9864e-04,  ..., -8.3008e-03,\n",
       "           2.5787e-03, -3.9368e-03],\n",
       "         [ 1.0986e-02,  9.8877e-03, -5.0964e-03,  ...,  2.5177e-03,\n",
       "           7.7057e-04, -5.0049e-03],\n",
       "         ...,\n",
       "         [-1.3977e-02, -2.7313e-03, -1.9897e-02,  ..., -1.0437e-02,\n",
       "           9.5825e-03, -1.8005e-03],\n",
       "         [-1.0742e-02,  9.3384e-03,  1.2939e-02,  ..., -3.3203e-02,\n",
       "          -1.6357e-02,  3.3875e-03],\n",
       "         [-8.3008e-03, -4.0588e-03, -1.1063e-03,  ...,  3.4790e-03,\n",
       "          -1.2939e-02,  3.1948e-05]], dtype=torch.float16))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name, loaded_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b31e4388-af56-49ee-b81e-a4d2a333bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_iterator = hf_model_weights_iterator(\"TheBloke/CodeUp-Alpha-13B-HF-AWQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b23f97a-ad91-434b-aa63-2fa548c0193d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.mlp.down_proj.qzeros\n",
      "model.layers.0.mlp.down_proj.scales\n",
      "model.layers.0.mlp.gate_proj.qzeros\n",
      "model.layers.0.mlp.gate_proj.scales\n",
      "model.layers.0.mlp.up_proj.qzeros\n",
      "model.layers.0.mlp.up_proj.scales\n",
      "model.layers.0.self_attn.k_proj.qzeros\n",
      "model.layers.0.self_attn.k_proj.scales\n",
      "model.layers.0.self_attn.o_proj.qzeros\n",
      "model.layers.0.self_attn.o_proj.scales\n",
      "model.layers.0.self_attn.q_proj.qzeros\n",
      "model.layers.0.self_attn.q_proj.scales\n",
      "model.layers.0.self_attn.v_proj.qzeros\n",
      "model.layers.0.self_attn.v_proj.scales\n",
      "model.layers.1.mlp.down_proj.qzeros\n",
      "model.layers.1.mlp.down_proj.scales\n",
      "model.layers.1.mlp.gate_proj.qzeros\n",
      "model.layers.1.mlp.gate_proj.scales\n",
      "model.layers.1.mlp.up_proj.qzeros\n",
      "model.layers.1.mlp.up_proj.scales\n",
      "model.layers.1.self_attn.k_proj.qzeros\n",
      "model.layers.1.self_attn.k_proj.scales\n",
      "model.layers.1.self_attn.o_proj.qzeros\n",
      "model.layers.1.self_attn.o_proj.scales\n",
      "model.layers.1.self_attn.q_proj.qzeros\n",
      "model.layers.1.self_attn.q_proj.scales\n",
      "model.layers.1.self_attn.v_proj.qzeros\n",
      "model.layers.1.self_attn.v_proj.scales\n",
      "model.layers.10.mlp.down_proj.qzeros\n",
      "model.layers.10.mlp.down_proj.scales\n",
      "model.layers.10.mlp.gate_proj.qzeros\n",
      "model.layers.10.mlp.gate_proj.scales\n",
      "model.layers.10.mlp.up_proj.qzeros\n",
      "model.layers.10.mlp.up_proj.scales\n",
      "model.layers.10.self_attn.k_proj.qzeros\n",
      "model.layers.10.self_attn.k_proj.scales\n",
      "model.layers.10.self_attn.o_proj.qzeros\n",
      "model.layers.10.self_attn.o_proj.scales\n",
      "model.layers.10.self_attn.q_proj.qzeros\n",
      "model.layers.10.self_attn.q_proj.scales\n",
      "model.layers.10.self_attn.v_proj.qzeros\n",
      "model.layers.10.self_attn.v_proj.scales\n",
      "model.layers.11.mlp.down_proj.qzeros\n",
      "model.layers.11.mlp.down_proj.scales\n",
      "model.layers.11.mlp.gate_proj.qzeros\n",
      "model.layers.11.mlp.gate_proj.scales\n",
      "model.layers.11.mlp.up_proj.qzeros\n",
      "model.layers.11.mlp.up_proj.scales\n",
      "model.layers.11.self_attn.k_proj.qzeros\n",
      "model.layers.11.self_attn.k_proj.scales\n",
      "model.layers.11.self_attn.o_proj.qzeros\n",
      "model.layers.11.self_attn.o_proj.scales\n",
      "model.layers.11.self_attn.q_proj.qzeros\n",
      "model.layers.11.self_attn.q_proj.scales\n",
      "model.layers.11.self_attn.v_proj.qzeros\n",
      "model.layers.11.self_attn.v_proj.scales\n",
      "model.layers.12.mlp.down_proj.qzeros\n",
      "model.layers.12.mlp.down_proj.scales\n",
      "model.layers.12.mlp.gate_proj.qzeros\n",
      "model.layers.12.mlp.gate_proj.scales\n",
      "model.layers.12.mlp.up_proj.qzeros\n",
      "model.layers.12.mlp.up_proj.scales\n",
      "model.layers.12.self_attn.k_proj.qzeros\n",
      "model.layers.12.self_attn.k_proj.scales\n",
      "model.layers.12.self_attn.o_proj.qzeros\n",
      "model.layers.12.self_attn.o_proj.scales\n",
      "model.layers.12.self_attn.q_proj.qzeros\n",
      "model.layers.12.self_attn.q_proj.scales\n",
      "model.layers.12.self_attn.v_proj.qzeros\n",
      "model.layers.12.self_attn.v_proj.scales\n",
      "model.layers.13.mlp.down_proj.qzeros\n",
      "model.layers.13.mlp.down_proj.scales\n",
      "model.layers.13.mlp.gate_proj.qzeros\n",
      "model.layers.13.mlp.gate_proj.scales\n",
      "model.layers.13.mlp.up_proj.qzeros\n",
      "model.layers.13.mlp.up_proj.scales\n",
      "model.layers.13.self_attn.k_proj.qzeros\n",
      "model.layers.13.self_attn.k_proj.scales\n",
      "model.layers.13.self_attn.o_proj.qzeros\n",
      "model.layers.13.self_attn.o_proj.scales\n",
      "model.layers.13.self_attn.q_proj.qzeros\n",
      "model.layers.13.self_attn.q_proj.scales\n",
      "model.layers.13.self_attn.v_proj.qzeros\n",
      "model.layers.13.self_attn.v_proj.scales\n",
      "model.layers.14.mlp.down_proj.qzeros\n",
      "model.layers.14.mlp.down_proj.scales\n",
      "model.layers.14.mlp.gate_proj.qzeros\n",
      "model.layers.14.mlp.gate_proj.scales\n",
      "model.layers.14.mlp.up_proj.qzeros\n",
      "model.layers.14.mlp.up_proj.scales\n",
      "model.layers.14.self_attn.k_proj.qzeros\n",
      "model.layers.14.self_attn.k_proj.scales\n",
      "model.layers.14.self_attn.o_proj.qzeros\n",
      "model.layers.14.self_attn.o_proj.scales\n",
      "model.layers.14.self_attn.q_proj.qzeros\n",
      "model.layers.14.self_attn.q_proj.scales\n",
      "model.layers.14.self_attn.v_proj.qzeros\n",
      "model.layers.14.self_attn.v_proj.scales\n",
      "model.layers.15.mlp.down_proj.qzeros\n",
      "model.layers.15.mlp.down_proj.scales\n",
      "model.layers.15.mlp.gate_proj.qzeros\n",
      "model.layers.15.mlp.gate_proj.scales\n",
      "model.layers.15.mlp.up_proj.qzeros\n",
      "model.layers.15.mlp.up_proj.scales\n",
      "model.layers.15.self_attn.k_proj.qzeros\n",
      "model.layers.15.self_attn.k_proj.scales\n",
      "model.layers.15.self_attn.o_proj.qzeros\n",
      "model.layers.15.self_attn.o_proj.scales\n",
      "model.layers.15.self_attn.q_proj.qzeros\n",
      "model.layers.15.self_attn.q_proj.scales\n",
      "model.layers.15.self_attn.v_proj.qzeros\n",
      "model.layers.15.self_attn.v_proj.scales\n",
      "model.layers.16.mlp.down_proj.qzeros\n",
      "model.layers.16.mlp.down_proj.scales\n",
      "model.layers.16.mlp.gate_proj.qzeros\n",
      "model.layers.16.mlp.gate_proj.scales\n",
      "model.layers.16.mlp.up_proj.qzeros\n",
      "model.layers.16.mlp.up_proj.scales\n",
      "model.layers.16.self_attn.k_proj.qzeros\n",
      "model.layers.16.self_attn.k_proj.scales\n",
      "model.layers.16.self_attn.o_proj.qzeros\n",
      "model.layers.16.self_attn.o_proj.scales\n",
      "model.layers.16.self_attn.q_proj.qzeros\n",
      "model.layers.16.self_attn.q_proj.scales\n",
      "model.layers.16.self_attn.v_proj.qzeros\n",
      "model.layers.16.self_attn.v_proj.scales\n",
      "model.layers.17.mlp.down_proj.qzeros\n",
      "model.layers.17.mlp.down_proj.scales\n",
      "model.layers.17.mlp.gate_proj.qzeros\n",
      "model.layers.17.mlp.gate_proj.scales\n",
      "model.layers.17.mlp.up_proj.qzeros\n",
      "model.layers.17.mlp.up_proj.scales\n",
      "model.layers.17.self_attn.k_proj.qzeros\n",
      "model.layers.17.self_attn.k_proj.scales\n",
      "model.layers.17.self_attn.o_proj.qzeros\n",
      "model.layers.17.self_attn.o_proj.scales\n",
      "model.layers.17.self_attn.q_proj.qzeros\n",
      "model.layers.17.self_attn.q_proj.scales\n",
      "model.layers.17.self_attn.v_proj.qzeros\n",
      "model.layers.17.self_attn.v_proj.scales\n",
      "model.layers.18.mlp.down_proj.qzeros\n",
      "model.layers.18.mlp.down_proj.scales\n",
      "model.layers.18.mlp.gate_proj.qzeros\n",
      "model.layers.18.mlp.gate_proj.scales\n",
      "model.layers.18.mlp.up_proj.qzeros\n",
      "model.layers.18.mlp.up_proj.scales\n",
      "model.layers.18.self_attn.k_proj.qzeros\n",
      "model.layers.18.self_attn.k_proj.scales\n",
      "model.layers.18.self_attn.o_proj.qzeros\n",
      "model.layers.18.self_attn.o_proj.scales\n",
      "model.layers.18.self_attn.q_proj.qzeros\n",
      "model.layers.18.self_attn.q_proj.scales\n",
      "model.layers.18.self_attn.v_proj.qzeros\n",
      "model.layers.18.self_attn.v_proj.scales\n",
      "model.layers.19.mlp.down_proj.qzeros\n",
      "model.layers.19.mlp.down_proj.scales\n",
      "model.layers.19.mlp.gate_proj.qzeros\n",
      "model.layers.19.mlp.gate_proj.scales\n",
      "model.layers.19.mlp.up_proj.qzeros\n",
      "model.layers.19.mlp.up_proj.scales\n",
      "model.layers.19.self_attn.k_proj.qzeros\n",
      "model.layers.19.self_attn.k_proj.scales\n",
      "model.layers.19.self_attn.o_proj.qzeros\n",
      "model.layers.19.self_attn.o_proj.scales\n",
      "model.layers.19.self_attn.q_proj.qzeros\n",
      "model.layers.19.self_attn.q_proj.scales\n",
      "model.layers.19.self_attn.v_proj.qzeros\n",
      "model.layers.19.self_attn.v_proj.scales\n",
      "model.layers.2.mlp.down_proj.qzeros\n",
      "model.layers.2.mlp.down_proj.scales\n",
      "model.layers.2.mlp.gate_proj.qzeros\n",
      "model.layers.2.mlp.gate_proj.scales\n",
      "model.layers.2.mlp.up_proj.qzeros\n",
      "model.layers.2.mlp.up_proj.scales\n",
      "model.layers.2.self_attn.k_proj.qzeros\n",
      "model.layers.2.self_attn.k_proj.scales\n",
      "model.layers.2.self_attn.o_proj.qzeros\n",
      "model.layers.2.self_attn.o_proj.scales\n",
      "model.layers.2.self_attn.q_proj.qzeros\n",
      "model.layers.2.self_attn.q_proj.scales\n",
      "model.layers.2.self_attn.v_proj.qzeros\n",
      "model.layers.2.self_attn.v_proj.scales\n",
      "model.layers.20.mlp.down_proj.qzeros\n",
      "model.layers.20.mlp.down_proj.scales\n",
      "model.layers.20.mlp.gate_proj.qzeros\n",
      "model.layers.20.mlp.gate_proj.scales\n",
      "model.layers.20.mlp.up_proj.qzeros\n",
      "model.layers.20.mlp.up_proj.scales\n",
      "model.layers.20.self_attn.k_proj.qzeros\n",
      "model.layers.20.self_attn.k_proj.scales\n",
      "model.layers.20.self_attn.o_proj.qzeros\n",
      "model.layers.20.self_attn.o_proj.scales\n",
      "model.layers.20.self_attn.q_proj.qzeros\n",
      "model.layers.20.self_attn.q_proj.scales\n",
      "model.layers.20.self_attn.v_proj.qzeros\n",
      "model.layers.20.self_attn.v_proj.scales\n",
      "model.layers.21.mlp.down_proj.qzeros\n",
      "model.layers.21.mlp.down_proj.scales\n",
      "model.layers.21.mlp.gate_proj.qzeros\n",
      "model.layers.21.mlp.gate_proj.scales\n",
      "model.layers.21.mlp.up_proj.qzeros\n",
      "model.layers.21.mlp.up_proj.scales\n",
      "model.layers.21.self_attn.k_proj.qzeros\n",
      "model.layers.21.self_attn.k_proj.scales\n",
      "model.layers.21.self_attn.o_proj.qzeros\n",
      "model.layers.21.self_attn.o_proj.scales\n",
      "model.layers.21.self_attn.q_proj.qzeros\n",
      "model.layers.21.self_attn.q_proj.scales\n",
      "model.layers.21.self_attn.v_proj.qzeros\n",
      "model.layers.21.self_attn.v_proj.scales\n",
      "model.layers.22.mlp.down_proj.qzeros\n",
      "model.layers.22.mlp.down_proj.scales\n",
      "model.layers.22.mlp.gate_proj.qzeros\n",
      "model.layers.22.mlp.gate_proj.scales\n",
      "model.layers.22.mlp.up_proj.qzeros\n",
      "model.layers.22.mlp.up_proj.scales\n",
      "model.layers.22.self_attn.k_proj.qzeros\n",
      "model.layers.22.self_attn.k_proj.scales\n",
      "model.layers.22.self_attn.o_proj.qzeros\n",
      "model.layers.22.self_attn.o_proj.scales\n",
      "model.layers.22.self_attn.q_proj.qzeros\n",
      "model.layers.22.self_attn.q_proj.scales\n",
      "model.layers.22.self_attn.v_proj.qzeros\n",
      "model.layers.22.self_attn.v_proj.scales\n",
      "model.layers.23.mlp.down_proj.qzeros\n",
      "model.layers.23.mlp.down_proj.scales\n",
      "model.layers.23.mlp.gate_proj.qzeros\n",
      "model.layers.23.mlp.gate_proj.scales\n",
      "model.layers.23.mlp.up_proj.qzeros\n",
      "model.layers.23.mlp.up_proj.scales\n",
      "model.layers.23.self_attn.k_proj.qzeros\n",
      "model.layers.23.self_attn.k_proj.scales\n",
      "model.layers.23.self_attn.o_proj.qzeros\n",
      "model.layers.23.self_attn.o_proj.scales\n",
      "model.layers.23.self_attn.q_proj.qzeros\n",
      "model.layers.23.self_attn.q_proj.scales\n",
      "model.layers.23.self_attn.v_proj.qzeros\n",
      "model.layers.23.self_attn.v_proj.scales\n",
      "model.layers.24.mlp.down_proj.qzeros\n",
      "model.layers.24.mlp.down_proj.scales\n",
      "model.layers.24.mlp.gate_proj.qzeros\n",
      "model.layers.24.mlp.gate_proj.scales\n",
      "model.layers.24.mlp.up_proj.qzeros\n",
      "model.layers.24.mlp.up_proj.scales\n",
      "model.layers.24.self_attn.k_proj.qzeros\n",
      "model.layers.24.self_attn.k_proj.scales\n",
      "model.layers.24.self_attn.o_proj.qzeros\n",
      "model.layers.24.self_attn.o_proj.scales\n",
      "model.layers.24.self_attn.q_proj.qzeros\n",
      "model.layers.24.self_attn.q_proj.scales\n",
      "model.layers.24.self_attn.v_proj.qzeros\n",
      "model.layers.24.self_attn.v_proj.scales\n",
      "model.layers.25.mlp.down_proj.qzeros\n",
      "model.layers.25.mlp.down_proj.scales\n",
      "model.layers.25.mlp.gate_proj.qzeros\n",
      "model.layers.25.mlp.gate_proj.scales\n",
      "model.layers.25.mlp.up_proj.qzeros\n",
      "model.layers.25.mlp.up_proj.scales\n",
      "model.layers.25.self_attn.k_proj.qzeros\n",
      "model.layers.25.self_attn.k_proj.scales\n",
      "model.layers.25.self_attn.o_proj.qzeros\n",
      "model.layers.25.self_attn.o_proj.scales\n",
      "model.layers.25.self_attn.q_proj.qzeros\n",
      "model.layers.25.self_attn.q_proj.scales\n",
      "model.layers.25.self_attn.v_proj.qzeros\n",
      "model.layers.25.self_attn.v_proj.scales\n",
      "model.layers.26.mlp.down_proj.qzeros\n",
      "model.layers.26.mlp.down_proj.scales\n",
      "model.layers.26.mlp.gate_proj.qzeros\n",
      "model.layers.26.mlp.gate_proj.scales\n",
      "model.layers.26.mlp.up_proj.qzeros\n",
      "model.layers.26.mlp.up_proj.scales\n",
      "model.layers.26.self_attn.k_proj.qzeros\n",
      "model.layers.26.self_attn.k_proj.scales\n",
      "model.layers.26.self_attn.o_proj.qzeros\n",
      "model.layers.26.self_attn.o_proj.scales\n",
      "model.layers.26.self_attn.q_proj.qzeros\n",
      "model.layers.26.self_attn.q_proj.scales\n",
      "model.layers.26.self_attn.v_proj.qzeros\n",
      "model.layers.26.self_attn.v_proj.scales\n",
      "model.layers.27.mlp.down_proj.qzeros\n",
      "model.layers.27.mlp.down_proj.scales\n",
      "model.layers.27.mlp.gate_proj.qzeros\n",
      "model.layers.27.mlp.gate_proj.scales\n",
      "model.layers.27.mlp.up_proj.qzeros\n",
      "model.layers.27.mlp.up_proj.scales\n",
      "model.layers.27.self_attn.k_proj.qzeros\n",
      "model.layers.27.self_attn.k_proj.scales\n",
      "model.layers.27.self_attn.o_proj.qzeros\n",
      "model.layers.27.self_attn.o_proj.scales\n",
      "model.layers.27.self_attn.q_proj.qzeros\n",
      "model.layers.27.self_attn.q_proj.scales\n",
      "model.layers.27.self_attn.v_proj.qzeros\n",
      "model.layers.27.self_attn.v_proj.scales\n",
      "model.layers.28.mlp.down_proj.qzeros\n",
      "model.layers.28.mlp.down_proj.scales\n",
      "model.layers.28.mlp.gate_proj.qzeros\n",
      "model.layers.28.mlp.gate_proj.scales\n",
      "model.layers.28.mlp.up_proj.qzeros\n",
      "model.layers.28.mlp.up_proj.scales\n",
      "model.layers.28.self_attn.k_proj.qzeros\n",
      "model.layers.28.self_attn.k_proj.scales\n",
      "model.layers.28.self_attn.o_proj.qzeros\n",
      "model.layers.28.self_attn.o_proj.scales\n",
      "model.layers.28.self_attn.q_proj.qzeros\n",
      "model.layers.28.self_attn.q_proj.scales\n",
      "model.layers.28.self_attn.v_proj.qzeros\n",
      "model.layers.28.self_attn.v_proj.scales\n",
      "model.layers.29.mlp.down_proj.qzeros\n",
      "model.layers.29.mlp.down_proj.scales\n",
      "model.layers.29.mlp.gate_proj.qzeros\n",
      "model.layers.29.mlp.gate_proj.scales\n",
      "model.layers.29.mlp.up_proj.qzeros\n",
      "model.layers.29.mlp.up_proj.scales\n",
      "model.layers.29.self_attn.k_proj.qzeros\n",
      "model.layers.29.self_attn.k_proj.scales\n",
      "model.layers.29.self_attn.o_proj.qzeros\n",
      "model.layers.29.self_attn.o_proj.scales\n",
      "model.layers.29.self_attn.q_proj.qzeros\n",
      "model.layers.29.self_attn.q_proj.scales\n",
      "model.layers.29.self_attn.v_proj.qzeros\n",
      "model.layers.29.self_attn.v_proj.scales\n",
      "model.layers.3.mlp.down_proj.qzeros\n",
      "model.layers.3.mlp.down_proj.scales\n",
      "model.layers.3.mlp.gate_proj.qzeros\n",
      "model.layers.3.mlp.gate_proj.scales\n",
      "model.layers.3.mlp.up_proj.qzeros\n",
      "model.layers.3.mlp.up_proj.scales\n",
      "model.layers.3.self_attn.k_proj.qzeros\n",
      "model.layers.3.self_attn.k_proj.scales\n",
      "model.layers.3.self_attn.o_proj.qzeros\n",
      "model.layers.3.self_attn.o_proj.scales\n",
      "model.layers.3.self_attn.q_proj.qzeros\n",
      "model.layers.3.self_attn.q_proj.scales\n",
      "model.layers.3.self_attn.v_proj.qzeros\n",
      "model.layers.3.self_attn.v_proj.scales\n",
      "model.layers.30.mlp.down_proj.qzeros\n",
      "model.layers.30.mlp.down_proj.scales\n",
      "model.layers.30.mlp.gate_proj.qzeros\n",
      "model.layers.30.mlp.gate_proj.scales\n",
      "model.layers.30.mlp.up_proj.qzeros\n",
      "model.layers.30.mlp.up_proj.scales\n",
      "model.layers.30.self_attn.k_proj.qzeros\n",
      "model.layers.30.self_attn.k_proj.scales\n",
      "model.layers.30.self_attn.o_proj.qzeros\n",
      "model.layers.30.self_attn.o_proj.scales\n",
      "model.layers.30.self_attn.q_proj.qzeros\n",
      "model.layers.30.self_attn.q_proj.scales\n",
      "model.layers.30.self_attn.v_proj.qzeros\n",
      "model.layers.30.self_attn.v_proj.scales\n",
      "model.layers.31.mlp.down_proj.qzeros\n",
      "model.layers.31.mlp.down_proj.scales\n",
      "model.layers.31.mlp.gate_proj.qzeros\n",
      "model.layers.31.mlp.gate_proj.scales\n",
      "model.layers.31.mlp.up_proj.qzeros\n",
      "model.layers.31.mlp.up_proj.scales\n",
      "model.layers.31.self_attn.k_proj.qzeros\n",
      "model.layers.31.self_attn.k_proj.scales\n",
      "model.layers.31.self_attn.o_proj.qzeros\n",
      "model.layers.31.self_attn.o_proj.scales\n",
      "model.layers.31.self_attn.q_proj.qzeros\n",
      "model.layers.31.self_attn.q_proj.scales\n",
      "model.layers.31.self_attn.v_proj.qzeros\n",
      "model.layers.31.self_attn.v_proj.scales\n",
      "model.layers.32.mlp.down_proj.qzeros\n",
      "model.layers.32.mlp.down_proj.scales\n",
      "model.layers.32.mlp.gate_proj.qzeros\n",
      "model.layers.32.mlp.gate_proj.scales\n",
      "model.layers.32.mlp.up_proj.qzeros\n",
      "model.layers.32.mlp.up_proj.scales\n",
      "model.layers.32.self_attn.k_proj.qzeros\n",
      "model.layers.32.self_attn.k_proj.scales\n",
      "model.layers.32.self_attn.o_proj.qzeros\n",
      "model.layers.32.self_attn.o_proj.scales\n",
      "model.layers.32.self_attn.q_proj.qzeros\n",
      "model.layers.32.self_attn.q_proj.scales\n",
      "model.layers.32.self_attn.v_proj.qzeros\n",
      "model.layers.32.self_attn.v_proj.scales\n",
      "model.layers.33.mlp.down_proj.qzeros\n",
      "model.layers.33.mlp.down_proj.scales\n",
      "model.layers.33.mlp.gate_proj.qzeros\n",
      "model.layers.33.mlp.gate_proj.scales\n",
      "model.layers.33.mlp.up_proj.qzeros\n",
      "model.layers.33.mlp.up_proj.scales\n",
      "model.layers.33.self_attn.k_proj.qzeros\n",
      "model.layers.33.self_attn.k_proj.scales\n",
      "model.layers.33.self_attn.o_proj.qzeros\n",
      "model.layers.33.self_attn.o_proj.scales\n",
      "model.layers.33.self_attn.q_proj.qzeros\n",
      "model.layers.33.self_attn.q_proj.scales\n",
      "model.layers.33.self_attn.v_proj.qzeros\n",
      "model.layers.33.self_attn.v_proj.scales\n",
      "model.layers.34.mlp.down_proj.qzeros\n",
      "model.layers.34.mlp.down_proj.scales\n",
      "model.layers.34.mlp.gate_proj.qzeros\n",
      "model.layers.34.mlp.gate_proj.scales\n",
      "model.layers.34.mlp.up_proj.qzeros\n",
      "model.layers.34.mlp.up_proj.scales\n",
      "model.layers.34.self_attn.k_proj.qzeros\n",
      "model.layers.34.self_attn.k_proj.scales\n",
      "model.layers.34.self_attn.o_proj.qzeros\n",
      "model.layers.34.self_attn.o_proj.scales\n",
      "model.layers.34.self_attn.q_proj.qzeros\n",
      "model.layers.34.self_attn.q_proj.scales\n",
      "model.layers.34.self_attn.v_proj.qzeros\n",
      "model.layers.34.self_attn.v_proj.scales\n",
      "model.layers.35.mlp.down_proj.qzeros\n",
      "model.layers.35.mlp.down_proj.scales\n",
      "model.layers.35.mlp.gate_proj.qzeros\n",
      "model.layers.35.mlp.gate_proj.scales\n",
      "model.layers.35.mlp.up_proj.qzeros\n",
      "model.layers.35.mlp.up_proj.scales\n",
      "model.layers.35.self_attn.k_proj.qzeros\n",
      "model.layers.35.self_attn.k_proj.scales\n",
      "model.layers.35.self_attn.o_proj.qzeros\n",
      "model.layers.35.self_attn.o_proj.scales\n",
      "model.layers.35.self_attn.q_proj.qzeros\n",
      "model.layers.35.self_attn.q_proj.scales\n",
      "model.layers.35.self_attn.v_proj.qzeros\n",
      "model.layers.35.self_attn.v_proj.scales\n",
      "model.layers.36.mlp.down_proj.qzeros\n",
      "model.layers.36.mlp.down_proj.scales\n",
      "model.layers.36.mlp.gate_proj.qzeros\n",
      "model.layers.36.mlp.gate_proj.scales\n",
      "model.layers.36.mlp.up_proj.qzeros\n",
      "model.layers.36.mlp.up_proj.scales\n",
      "model.layers.36.self_attn.k_proj.qzeros\n",
      "model.layers.36.self_attn.k_proj.scales\n",
      "model.layers.36.self_attn.o_proj.qzeros\n",
      "model.layers.36.self_attn.o_proj.scales\n",
      "model.layers.36.self_attn.q_proj.qzeros\n",
      "model.layers.36.self_attn.q_proj.scales\n",
      "model.layers.36.self_attn.v_proj.qzeros\n",
      "model.layers.36.self_attn.v_proj.scales\n",
      "model.layers.37.mlp.down_proj.qzeros\n",
      "model.layers.37.mlp.down_proj.scales\n",
      "model.layers.37.mlp.gate_proj.qzeros\n",
      "model.layers.37.mlp.gate_proj.scales\n",
      "model.layers.37.mlp.up_proj.qzeros\n",
      "model.layers.37.mlp.up_proj.scales\n",
      "model.layers.37.self_attn.k_proj.qzeros\n",
      "model.layers.37.self_attn.k_proj.scales\n",
      "model.layers.37.self_attn.o_proj.qzeros\n",
      "model.layers.37.self_attn.o_proj.scales\n",
      "model.layers.37.self_attn.q_proj.qzeros\n",
      "model.layers.37.self_attn.q_proj.scales\n",
      "model.layers.37.self_attn.v_proj.qzeros\n",
      "model.layers.37.self_attn.v_proj.scales\n",
      "model.layers.38.mlp.down_proj.qzeros\n",
      "model.layers.38.mlp.down_proj.scales\n",
      "model.layers.38.mlp.gate_proj.qzeros\n",
      "model.layers.38.mlp.gate_proj.scales\n",
      "model.layers.38.mlp.up_proj.qzeros\n",
      "model.layers.38.mlp.up_proj.scales\n",
      "model.layers.38.self_attn.k_proj.qzeros\n",
      "model.layers.38.self_attn.k_proj.scales\n",
      "model.layers.38.self_attn.o_proj.qzeros\n",
      "model.layers.38.self_attn.o_proj.scales\n",
      "model.layers.38.self_attn.q_proj.qzeros\n",
      "model.layers.38.self_attn.q_proj.scales\n",
      "model.layers.38.self_attn.v_proj.qzeros\n",
      "model.layers.38.self_attn.v_proj.scales\n",
      "model.layers.39.mlp.down_proj.qzeros\n",
      "model.layers.39.mlp.down_proj.scales\n",
      "model.layers.39.mlp.gate_proj.qzeros\n",
      "model.layers.39.mlp.gate_proj.scales\n",
      "model.layers.39.mlp.up_proj.qzeros\n",
      "model.layers.39.mlp.up_proj.scales\n",
      "model.layers.39.self_attn.k_proj.qzeros\n",
      "model.layers.39.self_attn.k_proj.scales\n",
      "model.layers.39.self_attn.o_proj.qzeros\n",
      "model.layers.39.self_attn.o_proj.scales\n",
      "model.layers.39.self_attn.q_proj.qzeros\n",
      "model.layers.39.self_attn.q_proj.scales\n",
      "model.layers.39.self_attn.v_proj.qzeros\n",
      "model.layers.39.self_attn.v_proj.scales\n",
      "model.layers.4.mlp.down_proj.qzeros\n",
      "model.layers.4.mlp.down_proj.scales\n",
      "model.layers.4.mlp.gate_proj.qzeros\n",
      "model.layers.4.mlp.gate_proj.scales\n",
      "model.layers.4.mlp.up_proj.qzeros\n",
      "model.layers.4.mlp.up_proj.scales\n",
      "model.layers.4.self_attn.k_proj.qzeros\n",
      "model.layers.4.self_attn.k_proj.scales\n",
      "model.layers.4.self_attn.o_proj.qzeros\n",
      "model.layers.4.self_attn.o_proj.scales\n",
      "model.layers.4.self_attn.q_proj.qzeros\n",
      "model.layers.4.self_attn.q_proj.scales\n",
      "model.layers.4.self_attn.v_proj.qzeros\n",
      "model.layers.4.self_attn.v_proj.scales\n",
      "model.layers.5.mlp.down_proj.qzeros\n",
      "model.layers.5.mlp.down_proj.scales\n",
      "model.layers.5.mlp.gate_proj.qzeros\n",
      "model.layers.5.mlp.gate_proj.scales\n",
      "model.layers.5.mlp.up_proj.qzeros\n",
      "model.layers.5.mlp.up_proj.scales\n",
      "model.layers.5.self_attn.k_proj.qzeros\n",
      "model.layers.5.self_attn.k_proj.scales\n",
      "model.layers.5.self_attn.o_proj.qzeros\n",
      "model.layers.5.self_attn.o_proj.scales\n",
      "model.layers.5.self_attn.q_proj.qzeros\n",
      "model.layers.5.self_attn.q_proj.scales\n",
      "model.layers.5.self_attn.v_proj.qzeros\n",
      "model.layers.5.self_attn.v_proj.scales\n",
      "model.layers.6.mlp.down_proj.qzeros\n",
      "model.layers.6.mlp.down_proj.scales\n",
      "model.layers.6.mlp.gate_proj.qzeros\n",
      "model.layers.6.mlp.gate_proj.scales\n",
      "model.layers.6.mlp.up_proj.qzeros\n",
      "model.layers.6.mlp.up_proj.scales\n",
      "model.layers.6.self_attn.k_proj.qzeros\n",
      "model.layers.6.self_attn.k_proj.scales\n",
      "model.layers.6.self_attn.o_proj.qzeros\n",
      "model.layers.6.self_attn.o_proj.scales\n",
      "model.layers.6.self_attn.q_proj.qzeros\n",
      "model.layers.6.self_attn.q_proj.scales\n",
      "model.layers.6.self_attn.v_proj.qzeros\n",
      "model.layers.6.self_attn.v_proj.scales\n",
      "model.layers.7.mlp.down_proj.qzeros\n",
      "model.layers.7.mlp.down_proj.scales\n",
      "model.layers.7.mlp.gate_proj.qzeros\n",
      "model.layers.7.mlp.gate_proj.scales\n",
      "model.layers.7.mlp.up_proj.qzeros\n",
      "model.layers.7.mlp.up_proj.scales\n",
      "model.layers.7.self_attn.k_proj.qzeros\n",
      "model.layers.7.self_attn.k_proj.scales\n",
      "model.layers.7.self_attn.o_proj.qzeros\n",
      "model.layers.7.self_attn.o_proj.scales\n",
      "model.layers.7.self_attn.q_proj.qzeros\n",
      "model.layers.7.self_attn.q_proj.scales\n",
      "model.layers.7.self_attn.v_proj.qzeros\n",
      "model.layers.7.self_attn.v_proj.scales\n",
      "model.layers.8.mlp.down_proj.qzeros\n",
      "model.layers.8.mlp.down_proj.scales\n",
      "model.layers.8.mlp.gate_proj.qzeros\n",
      "model.layers.8.mlp.gate_proj.scales\n",
      "model.layers.8.mlp.up_proj.qzeros\n",
      "model.layers.8.mlp.up_proj.scales\n",
      "model.layers.8.self_attn.k_proj.qzeros\n",
      "model.layers.8.self_attn.k_proj.scales\n",
      "model.layers.8.self_attn.o_proj.qzeros\n",
      "model.layers.8.self_attn.o_proj.scales\n",
      "model.layers.8.self_attn.q_proj.qzeros\n",
      "model.layers.8.self_attn.q_proj.scales\n",
      "model.layers.8.self_attn.v_proj.qzeros\n",
      "model.layers.8.self_attn.v_proj.scales\n",
      "model.layers.9.mlp.down_proj.qzeros\n",
      "model.layers.9.mlp.down_proj.scales\n",
      "model.layers.9.mlp.gate_proj.qzeros\n",
      "model.layers.9.mlp.gate_proj.scales\n",
      "model.layers.9.mlp.up_proj.qzeros\n",
      "model.layers.9.mlp.up_proj.scales\n",
      "model.layers.9.self_attn.k_proj.qzeros\n",
      "model.layers.9.self_attn.k_proj.scales\n",
      "model.layers.9.self_attn.o_proj.qzeros\n",
      "model.layers.9.self_attn.o_proj.scales\n",
      "model.layers.9.self_attn.q_proj.qzeros\n",
      "model.layers.9.self_attn.q_proj.scales\n",
      "model.layers.9.self_attn.v_proj.qzeros\n",
      "model.layers.9.self_attn.v_proj.scales\n"
     ]
    }
   ],
   "source": [
    "for name, loaded_weight in weights_iterator: \n",
    "    if 'scales' in name or 'zeros' in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9098d1-2374-4060-b8fe-60052c04110e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create Quantized Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7cfc1da-2147-4722-a766-ee216a29ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, json\n",
    "from safetensors.torch import save_file\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7d1dd78-d696-4477-b878-a13a763c270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path(\"/home/ubuntu/models/llama-7b-orca-math-100k-full-quantized\")\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "714a8cb2-6eff-4645-bcff-c4bf66238802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original quantized layers from fsdp_qlora/train.py\n",
    "# [\"k_proj\", \"q_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "afaa582c-95b9-4e22-a209-485cfb73f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to AWQ for now\n",
    "quantized_layers = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65b4afb3-d1ff-434d-8e8d-e9537b1ccda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_math_model_dir = \"/home/ubuntu/models/llama-7b-orca-math-100k-full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4e3067be-c874-4550-b999-1452d7e8f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = safetensors.torch.load_file(glob(os.path.join(orca_math_model_dir, \"*.safetensors\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9eaef503-678f-4084-b3f7-faa0af7f6d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_state_dict = copy.deepcopy(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a3686d0a-497a-4486-bf84-169dc6924ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pack_factor = 2\n",
    "blocksize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7c23baa0-6dc7-4939-986d-9bf0e6b2722a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([4096, 4096])\n"
     ]
    }
   ],
   "source": [
    "for n,p in iter(weights.items()):\n",
    "    if any(l in n for l in quantized_layers) and \"weight\" in n:\n",
    "        # output_size x input_size\n",
    "        print(n, p.shape)\n",
    "        input_size, output_size = p.shape\n",
    "        param = Params4bit(p, quant_type=\"nf4\", blocksize=blocksize, compress_statistics=False, quant_storage=torch.uint8)\n",
    "        param.cuda();\n",
    "\n",
    "        # reshape for tensor parallelism\n",
    "        qweight, absmax = param.data.cpu(), param.quant_state.absmax.cpu()        \n",
    "        qweight = qweight.reshape(input_size, output_size // pack_factor)\n",
    "        absmax = absmax.reshape(input_size, output_size // blocksize)\n",
    "                \n",
    "        quantized_state_dict[n] = qweight\n",
    "        quantized_state_dict[n.replace(\".weight\", \".absmax\")] = absmax\n",
    "\n",
    "        param = None\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "07d917df-7614-41b5-bac7-c85fb73c9ef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save quantized weights\n",
    "save_file(quantized_state_dict, model_dir/\"model_state_dict.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3c27a023-7ef7-456f-a3e9-d40c453b60cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save quant config\n",
    "quant_config_dict = {\n",
    "    \"weight_bits\" : 4,\n",
    "    \"blocksize\" : 64,\n",
    "    \"quant_type\" : \"nf4\",\n",
    "    \"quant_storage\" : \"uint8\",\n",
    "    \"compress_statistics\" : False\n",
    "}\n",
    "quant_config_filename = model_dir/\"quantize_config.json\"\n",
    "with open(quant_config_filename, \"w+\") as f: json.dump(quant_config_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4254ffdf-8b6e-47da-ae80-27f9e19f4b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save model config\n",
    "model_config = AutoConfig.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "model_config_filename = model_dir/\"config.json\"\n",
    "with open(model_config_filename, \"w+\") as f: json.dump(model_config.to_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a28ac0-9d96-46a6-a0e8-a28211728cb4",
   "metadata": {},
   "source": [
    "### BNB Quantized VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81f8c396-e4ec-4bc5-81b0-0bf7239a73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# import os\n",
    "# os.makedirs(\"/home/ubuntu/models/llama-7b-orca-math-100k-full\", exist_ok=True)\n",
    "# hf_hub_download(repo_id=\"answerdotai/llama-7b-orca-math-100k-full\", \n",
    "#                 filename=\"model_state_dict.safetensors\",\n",
    "#                 local_dir=\"/home/ubuntu/models/llama-7b-orca-math-100k-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4eca361-0895-4858-ad46-c705ff64451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "import safetensors\n",
    "import safetensors.torch\n",
    "from pathlib import Path\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.functional import dequantize_4bit, QuantState\n",
    "from bitsandbytes.nn.modules import Params4bit, Linear4bit\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from accelerate import init_empty_weights\n",
    "from glob import glob\n",
    "import os\n",
    "from fastcore.parallel import parallel\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c98b3fff-f6ad-488b-b41a-0f108bb7688b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f053d9b4-a087-4301-b069-def8705ec4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.sequence import SequenceGroupMetadata, SequenceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5951483a-d5d6-4c82-bab0-c07d868f8c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c61c1c1c-6717-42d7-870f-713b4923bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear(model:nn.Module, linear_replacement:nn.Module, quant_config:dict|None=None,\n",
    "                   skip_modules:List[str]=[\"lm_head\"], **kwargs):\n",
    "    \"\"\"\n",
    "    Replace linear modules with a new Linear module.\n",
    "    Parameters:\n",
    "        model (`torch.nn.Module`):\n",
    "            Input model or `torch.nn.Module` as the function is run recursively.\n",
    "        linear_replacement (`torch.nn.Module`):\n",
    "            The linear module that replaces the old one. Only expects standard arguments.\n",
    "            If other arguments need to be passed, use a lambda.\n",
    "        skip_modules (`List[str]`, *optional*, defaults to `lm_head`):\n",
    "            List of modules names not to convert. Defaults to `lm_head`.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if name in skip_modules:\n",
    "            print(f\"Skipping {name}\")\n",
    "            continue\n",
    "        \n",
    "        if len(list(module.children())) > 0:\n",
    "            replace_linear(module, linear_replacement, quant_config, skip_modules, **kwargs)\n",
    "\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            if issubclass(linear_replacement, Linear4bit):\n",
    "                model._modules[name] = linear_replacement(\n",
    "                    module.in_features,\n",
    "                    module.out_features,\n",
    "                    module.bias is not None,\n",
    "                    **kwargs\n",
    "                )\n",
    "            # elif issubclass(linear_replacement, HQQLinear):\n",
    "            #     model._modules[name] = linear_replacement(module, quant_config, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported linear replacement: {type(linear_replacement)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd8402bd-a3b6-4479-8ab3-21fb9c8c8ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_quantize(module:nn.Module, name:str, value:torch.Tensor, device:torch.device=None, dtype:torch.dtype=None,\n",
    "                      skip_names:list[str]=[], is_meta_rank:bool=False, low_memory:bool=True, verbose:bool=False,\n",
    "                      quant_method:str='bnb', is_dora:bool=False):\n",
    "    \"\"\"\n",
    "    Loads `value` tensor into submodule of `module`, optionally skipping `skip_names` and converting to `dtype`.\n",
    "\n",
    "    Quantizes `Params4bit` on `device` then places on \"cpu\" if low_memory=True or \"meta\" if is_meta_rank=True.\n",
    "    \"\"\"\n",
    "    def place_on_device(value):\n",
    "        if is_meta_rank:\n",
    "            device = 'meta'\n",
    "        elif low_memory:\n",
    "            device = 'cpu'\n",
    "        return value.to(device=device, dtype=dtype)\n",
    "\n",
    "    if any([skip_name in name for skip_name in skip_names]):\n",
    "        if verbose:\n",
    "            print(f\"Skipping {name} because it is in skip_names\")\n",
    "        return\n",
    "\n",
    "    module_key, _, value_key = name.rpartition('.')\n",
    "    try:\n",
    "        submodule = module.get_submodule(module_key)\n",
    "    except AttributeError as e:\n",
    "        print(f\"Module {module_key} not found:\\n{e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        if quant_method=='bnb':\n",
    "            param = submodule.get_parameter(value_key)\n",
    "            if isinstance(param, Params4bit):\n",
    "                # With `sync_module_states=True`, a meta device Params4bit needs to be the same\n",
    "                # shape as the quantized Params4bit with an initialized quant_state. However,\n",
    "                # FSDP only syncs parameters and buffers, so the quant_state isn't copied. This\n",
    "                # workaround quantizes Params4bit to initialize quant_state on all ranks, then\n",
    "                # replaces Params4bit's data with a meta tensor to free memory on non-rank 0.\n",
    "                if is_dora:\n",
    "                    setattr(submodule, \"dora_scale\", value.norm(p=2, dim=1).to(dtype=dtype).to(\"cpu\"))                \n",
    "                    print(\"DORA scale initialized\")\n",
    "                value = type(param)(value.to(device=device, dtype=dtype).data, **param.__dict__).cuda(device)\n",
    "                if is_meta_rank:\n",
    "                    value = type(param)(value.data.to(\"meta\"), **value.__dict__)\n",
    "                elif low_memory:\n",
    "                    value = type(param)(value.data.to(\"cpu\"), **value.__dict__)\n",
    "                # print(\"Loaded quantized layer\")\n",
    "            else:\n",
    "                value = type(param)(place_on_device(value).data)\n",
    "                # print(\"Loaded regular layer\")\n",
    "    except AttributeError:\n",
    "        # it's a buffer\n",
    "        value = place_on_device(value)\n",
    "        pass\n",
    "    setattr(submodule, value_key, value)\n",
    "\n",
    "def load_and_quantize_parallel(name_param, model, **kwargs):\n",
    "    name, param = name_param\n",
    "    load_and_quantize(model, name, param, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c859ec51-9ea5-462f-a01e-afbe15fb3e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_math_model_dir = \"/home/ubuntu/models/llama-7b-orca-math-100k-full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815e7ff7-55e5-4f2f-90e7-65a35cf1a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "cfg = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "cfg._attn_implementation = \"sdpa\"\n",
    "skip_modules = [\"lm_head\"]\n",
    "load_param_skip_names = ['inv_freq']\n",
    "compute_dtype = torch_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adcd81a1-ee01-4e43-9f27-e0647a536a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(cfg)\n",
    "    model.model = replace_linear(model.model, Linear4bit, compute_dtype=compute_dtype,\n",
    "                                 quant_type='nf4', compress_statistics=False,\n",
    "                                 quant_storage=torch.uint8, skip_modules=skip_modules)\n",
    "model.is_loaded_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a7d4b70-42b2-4ee3-9afd-feb4193f75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = safetensors.torch.load_file(glob(os.path.join(orca_math_model_dir, \"*.safetensors\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d13a7fef-3cc5-4c8a-93e6-80c4c8ee2b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#291) [None,None,None,None,None,None,None,None,None,None...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel(load_and_quantize_parallel, \n",
    "         iter(weights.items()), \n",
    "         n_workers=8, \n",
    "         threadpool=True,\n",
    "         model=model, \n",
    "         dtype=torch_dtype, \n",
    "         device=torch.cuda.current_device(),\n",
    "         skip_names=load_param_skip_names,\n",
    "         is_meta_rank=False,\n",
    "         verbose=True,\n",
    "         quant_method=\"bnb\",\n",
    "         is_dora=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea38000d-dcc5-41da-bb24-386cefcfd954",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dc23e20-ae9e-4a2c-98cd-b7195bdd1b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "hf_tokenizer.pad_token_id = hf_tokenizer.unk_token_id\n",
    "hf_tokenizer.pad_token = hf_tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb62eb87-228d-4269-aa03-3710b5db3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bed4abb8-d7ee-4417-ae26-d41be9df0462",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\"\"###Question:\n",
    "A certain tax rate is some amount per $100.00. The rate, expressed as a percent, is 65%. What is the tax rate in dollars per $100.00?\n",
    "###Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f421631-8499-4d9c-8398-9a92c48c15ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ###Question:\n",
      "A certain tax rate is some amount per $100.00. The rate, expressed as a percent, is 65%. What is the tax rate in dollars per $100.00?\n",
      "###Answer:\n",
      "To find the tax rate in dollars per $100.00\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(hf_tokenizer(input, return_tensors=\"pt\")['input_ids'].cuda(),\n",
    "                        generation_config=GenerationConfig(do_sample=False, max_new_tokens=16, use_cache=True, \n",
    "                                                           pad_token_id=hf_tokenizer.pad_token_id)).cpu()\n",
    "print(hf_tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5531a763-2014-4487-9051-1b22764153f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This works fine.\n",
    "# orca_math_model_dir = \"/home/ubuntu/models/llama-7b-orca-math-100k-full\"\n",
    "# llm = LLM(model=orca_math_model_dir, tokenizer=\"meta-llama/Llama-2-7b-hf\", dtype=\"bfloat16\",\n",
    "#           gpu_memory_utilization=0.8)\n",
    "\n",
    "# vllm_model = llm.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "\n",
    "# outputs = llm.generate([input], sampling_params=SamplingParams(max_tokens=16, temperature=0.))\n",
    "# print(input + outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d77d66c6-3eb0-42fa-b6e2-a4df44db6010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-04 13:46:18 config.py:744] Casting torch.float16 to torch.bfloat16.\n",
      "WARNING 04-04 13:46:18 config.py:208] bnb quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 04-04 13:46:18 llm_engine.py:70] Initializing an LLM engine (v0.3.3) with config: model='/home/ubuntu/models/llama-7b-orca-math-100k-full-quantized', tokenizer='meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=bnb, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 04-04 13:46:18 pynccl_utils.py:13] vLLM is using nccl==2.18.1\n",
      "INFO 04-04 13:46:18 selector.py:15] Using FlashAttention backend.\n",
      "INFO 04-04 13:46:21 model_runner.py:104] Loading model weights took 3.9292 GB\n",
      "INFO 04-04 13:46:22 gpu_executor.py:94] # GPU blocks: 897, # CPU blocks: 512\n",
      "INFO 04-04 13:46:25 model_runner.py:770] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-04 13:46:25 model_runner.py:774] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-04 13:46:34 model_runner.py:846] Graph capturing finished in 9 secs.\n",
      "INFO 04-04 13:46:34 block_manager_v1.py:239] disable automatic prefix caching\n"
     ]
    }
   ],
   "source": [
    "orca_math_model_dir = \"/home/ubuntu/models/llama-7b-orca-math-100k-full-quantized\"\n",
    "llm = LLM(model=orca_math_model_dir, tokenizer=\"meta-llama/Llama-2-7b-hf\", dtype=\"bfloat16\",\n",
    "          quantization=\"bnb\", gpu_memory_utilization=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab010da7-0afb-4312-87ef-791b92e9e068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Question:\n",
      "A certain tax rate is some amount per $100.00. The rate, expressed as a percent, is 65%. What is the tax rate in dollars per $100.00?\n",
      "###Answer:\n",
      " Cat Abs Abs Abs Cat Cat Cat Cat Cat Cat Cat Cat Cat Cat Cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate([input], sampling_params=SamplingParams(max_tokens=16, temperature=0.))\n",
    "print(input + outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "877f7f30-d42a-4a7e-b6ff-625615f48bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_model = llm.llm_engine.model_executor.driver_worker.model_runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53e9af6f-36d1-4557-b491-1769779c1cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_layers = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35c37141-98b8-4f9b-8745-5b2d4841f944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for n,p in model.named_parameters():\n",
    "#     # if \"qkv\" in n: break # quantized layers are not casted to dtype, which is correct.\n",
    "#     # if \"lm_head\" in n: break\n",
    "#     if any(l in n for l in quantized_layers):\n",
    "#         print(n, p.dtype)\n",
    "#         # assert p.dtype == torch.uint8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0f1544-42d8-4707-abfd-4a9f5a27f5e9",
   "metadata": {},
   "source": [
    "#### Compare QKV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fe3ffa5-729f-4996-9583-cf5638b2e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(model.config.num_hidden_layers):\n",
    "    q = model.get_submodule(f'model.layers.{layer}.self_attn.q_proj')\n",
    "    k = model.get_submodule(f'model.layers.{layer}.self_attn.k_proj')\n",
    "    v = model.get_submodule(f'model.layers.{layer}.self_attn.v_proj')\n",
    "    \n",
    "    # qkv = vllm_model.get_submodule(f'model.layers.{layer}.self_attn.qkv_proj')\n",
    "    \n",
    "    # assert torch.equal(q.weight, qkv.weight.split(4096, dim=0)[0].reshape(-1,1))\n",
    "    # assert torch.equal(k.weight, qkv.weight.split(4096, dim=0)[1].reshape(-1,1))\n",
    "    # assert torch.equal(v.weight, qkv.weight.split(4096, dim=0)[2].reshape(-1,1))\n",
    "    \n",
    "    # assert torch.equal(q.quant_state.absmax, qkv.absmax.view(-1).split(4096*4096//64)[0])\n",
    "    # assert torch.equal(k.quant_state.absmax, qkv.absmax.view(-1).split(4096*4096//64)[1])\n",
    "    # assert torch.equal(v.quant_state.absmax, qkv.absmax.view(-1).split(4096*4096//64)[2])\n",
    "    \n",
    "    q2 = vllm_model.get_submodule(f'model.layers.{layer}.self_attn.q_proj')\n",
    "    k2 = vllm_model.get_submodule(f'model.layers.{layer}.self_attn.k_proj')\n",
    "    v2 = vllm_model.get_submodule(f'model.layers.{layer}.self_attn.v_proj')\n",
    "    \n",
    "    assert torch.equal(q.weight, q2.weight.reshape(-1,1))\n",
    "    assert torch.equal(k.weight, k2.weight.reshape(-1,1))\n",
    "    assert torch.equal(v.weight, v2.weight.reshape(-1,1))\n",
    "    \n",
    "    assert torch.equal(q.quant_state.absmax, q2.absmax.view(-1))\n",
    "    assert torch.equal(k.quant_state.absmax, k2.absmax.view(-1))\n",
    "    assert torch.equal(v.quant_state.absmax, v2.absmax.view(-1))\n",
    "    \n",
    "    # quant_state = QuantState(qkv.absmax.view(-1), dtype=torch.bfloat16)\n",
    "    # quant_state.shape = torch.Size([qkv.weight.shape[0], qkv.weight.shape[1] * 2])\n",
    "    # quant_state.blocksize = 64\n",
    "    # quant_state.quant_type = \"nf4\"\n",
    "    # quant_state.code = q.quant_state.code\n",
    "    \n",
    "    # dq, dk, dv = dequantize_4bit(qkv.weight.view(-1,1), quant_state).split(4096,dim=0)\n",
    "    \n",
    "    # assert torch.equal(dq, dequantize_4bit(q.weight, q.quant_state))\n",
    "    # assert torch.equal(dk, dequantize_4bit(k.weight, k.quant_state))\n",
    "    # assert torch.equal(dv, dequantize_4bit(v.weight, v.quant_state))\n",
    "    \n",
    "    x = torch.randn(2,4096).cuda().to(torch.bfloat16)\n",
    "    \n",
    "    assert torch.allclose(q(x),q2(x)[0])\n",
    "    assert torch.allclose(k(x),k2(x)[0])\n",
    "    assert torch.allclose(v(x),v2(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e576bf3f-db69-4827-9134-ee521a6a8f9d",
   "metadata": {},
   "source": [
    "#### Compare Attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6af03052-f659-47a6-a2cf-47b1bfba0d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 14 closeness: 0.978515625\n"
     ]
    }
   ],
   "source": [
    "for layer in range(model.config.num_hidden_layers):\n",
    "    \n",
    "    attn = model.get_submodule(f'model.layers.{layer}.self_attn')\n",
    "    attn2 = vllm_model.get_submodule(f'model.layers.{layer}.self_attn')\n",
    "    \n",
    "    attn_out1 = attn(x[None,...], position_ids=torch.tensor([[0,1]]).cuda(), attention_mask=torch.tensor([[[[1,0],[1,1]]]]).bool().cuda())[0]\n",
    "    \n",
    "    s = [SequenceGroupMetadata(\"request_id\", True, {0:SequenceData([1,100])}, sampling_params=SamplingParams(temperature=0.0), block_tables=None)]\n",
    "    attn_metadata = llm.llm_engine.model_executor.driver_worker.model_runner._prepare_prompt(s)[2]\n",
    "    \n",
    "    attn_out2 = attn2(torch.tensor([0,1]).cuda(), x, kv_cache=None, attn_metadata=attn_metadata)\n",
    "\n",
    "    if not torch.allclose(attn_out1, attn_out2):\n",
    "        print(\"layer:\",layer, \"closeness:\", torch.isclose(attn_out1, attn_out2).float().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfba6f0e-593a-4495-a5d0-7d203042fa68",
   "metadata": {},
   "source": [
    "#### Compare O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bd6c2c4-8227-40b4-b3d1-8887bf291c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(model.config.num_hidden_layers):\n",
    "\n",
    "    o = model.get_submodule(f'model.layers.{layer}.self_attn.o_proj')    \n",
    "    o2 = vllm_model.get_submodule(f'model.layers.{layer}.self_attn.o_proj')    \n",
    "    o.weight.shape, o2.weight.shape\n",
    "    o_out,_ = o2(x)\n",
    "    # o_out = o2.linear_method.apply_weights(o2.linear_weights, x, None)\n",
    "    assert torch.allclose(o_out,o(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e0c02-29b6-4f38-afe4-25ced50687b0",
   "metadata": {},
   "source": [
    "#### Compare gate up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74095ea3-a048-43cf-8e0f-ae509ae2bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(model.config.num_hidden_layers):\n",
    "    \n",
    "    gate = model.get_submodule(f'model.layers.{layer}.mlp.gate_proj')\n",
    "    up   = model.get_submodule(f'model.layers.{layer}.mlp.up_proj')\n",
    "    \n",
    "    # gate_up = vllm_model.get_submodule(f'model.layers.{layer}.mlp.gate_up_proj')\n",
    "    gate2 = vllm_model.get_submodule(f'model.layers.{layer}.mlp.gate_proj')\n",
    "    up2 = vllm_model.get_submodule(f'model.layers.{layer}.mlp.up_proj')\n",
    "    \n",
    "    # gate_out = gate2.linear_method.apply_weights(gate2.linear_weights, x, None)\n",
    "    gate_out,_ = gate2(x)\n",
    "    \n",
    "    torch.isclose(gate(x), gate_out).float().mean()\n",
    "    \n",
    "    # up_out = up2.linear_method.apply_weights(up2.linear_weights, x, None)\n",
    "    up_out,_ = up2(x)\n",
    "    \n",
    "    assert torch.allclose(up(x), up_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75fac54-b9b3-4021-8902-39dcb82e10bc",
   "metadata": {},
   "source": [
    "#### Compare down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60e9a05e-f737-4014-81d3-83688b5f197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(model.config.num_hidden_layers):\n",
    "    down = model.get_submodule(f'model.layers.{layer}.mlp.down_proj')\n",
    "    \n",
    "    down2 = vllm_model.get_submodule(f'model.layers.{layer}.mlp.down_proj')\n",
    "    \n",
    "    assert torch.equal(down.weight, down2.weight.reshape(-1,1))\n",
    "    \n",
    "    assert torch.equal(down.quant_state.absmax, down2.absmax.view(-1))\n",
    "    \n",
    "    x2 = torch.randn(2,11008).cuda().to(torch.bfloat16)\n",
    "    \n",
    "    # down_out = down2.linear_method.apply_weights(down2.linear_weights, x2, None)\n",
    "    down_out,_ = down2(x2)\n",
    "    \n",
    "    assert torch.allclose(down_out,down(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40cb18-6317-4cc0-b314-7eeffbb325ba",
   "metadata": {},
   "source": [
    "#### Compare MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfa92778-6f59-428b-93c4-2b69907dc2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(model.config.num_hidden_layers):\n",
    "    \n",
    "    x.shape\n",
    "    \n",
    "    mlp = model.get_submodule(f'model.layers.{layer}.mlp')\n",
    "    mlp2 = vllm_model.get_submodule(f'model.layers.{layer}.mlp')\n",
    "    \n",
    "    mlp_out1 = mlp(x)\n",
    "    \n",
    "    mlp_out2 = mlp2(x)\n",
    "    \n",
    "    assert torch.allclose(mlp_out1, mlp_out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506dfcf-7803-4109-829e-ca0e6e97c8fd",
   "metadata": {},
   "source": [
    "#### Compare decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1fb177ee-00ab-40d5-b085-5225c3dcdd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 5 closeness: 0.9818115234375\n"
     ]
    }
   ],
   "source": [
    "for layer in range(model.config.num_hidden_layers):\n",
    "    dec = model.model.layers[layer]\n",
    "    dec2 = vllm_model.model.layers[layer]\n",
    "    \n",
    "    x, x.shape\n",
    "    \n",
    "    out1 = dec(x[None,...], \n",
    "                 position_ids=torch.tensor([[0,1]]).cuda(), \n",
    "                 attention_mask=torch.tensor([[[[1,0],[1,1]]]]).bool().cuda())\n",
    "    \n",
    "    out1[0]\n",
    "    \n",
    "    s = [SequenceGroupMetadata(\"request_id\", True, {0:SequenceData([7,42])}, sampling_params=SamplingParams(temperature=0.0), block_tables=None)]\n",
    "    attn_metadata = llm.llm_engine.model_executor.driver_worker.model_runner._prepare_prompt(s)[2]\n",
    "    \n",
    "    out2 = dec2(hidden_states=x, \n",
    "                      positions=torch.tensor([0,1]).cuda(), \n",
    "                      kv_cache=None, \n",
    "                      attn_metadata=attn_metadata, \n",
    "                      residual=x)\n",
    "    \n",
    "    if not torch.allclose(out1[0], out2[0]):\n",
    "        print(\"layer:\",layer, \"closeness:\", torch.isclose(out1[0], out2[0]).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf971e9e-fce6-4a27-8775-7261086abcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import collections\n",
    "from typing import DefaultDict, Tuple, List, Dict\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "def save_activations(\n",
    "        activations: DefaultDict,\n",
    "        name: str,\n",
    "        module: nn.Module,\n",
    "        inp: Tuple,\n",
    "        out: torch.Tensor\n",
    ") -> None:\n",
    "    \"\"\"PyTorch Forward hook to save outputs at each forward\n",
    "    pass. Mutates specified dict objects with each fwd pass.\n",
    "    \"\"\"\n",
    "    if \"layer\" in name:\n",
    "        activations[name].append(out[0].detach().cpu())\n",
    "    else:\n",
    "        activations[name].append(out.detach().cpu())\n",
    "def register_activation_hooks(\n",
    "        model: nn.Module,\n",
    "        layers_to_save: List[str]\n",
    ") -> DefaultDict[List, torch.Tensor]:\n",
    "    \"\"\"Registers forward hooks in specified layers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model:\n",
    "        PyTorch model\n",
    "    layers_to_save:\n",
    "        Module names within ``model`` whose activations we want to save.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    activations_dict:\n",
    "        dict of lists containing activations of specified layers in\n",
    "        ``layers_to_save``.\n",
    "    \"\"\"\n",
    "    activations_dict = collections.defaultdict(list)\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if any(name.endswith(l) for l in layers_to_save):\n",
    "            module.register_forward_hook(\n",
    "                partial(save_activations, activations_dict, name)\n",
    "            )\n",
    "    return activations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "767be7c3-3e7d-414b-8ab7-2537aad82fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, module in model.named_modules(): \n",
    "    module._forward_hooks = OrderedDict()\n",
    "    # print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f56796b-d63a-4684-84d0-8f6717617ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in vllm_model.named_modules(): \n",
    "    module._forward_hooks = OrderedDict()\n",
    "    # print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "093842a8-2a27-4c22-a6d3-d0b4a0e6d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_save = [f\"layers.{i}\" for i in range(model.config.num_hidden_layers)] + [\".norm\", \"lm_head\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3fcf6db-d53e-4d37-ad03-9b3675bd5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_saved_activations = register_activation_hooks(model, layers_to_save=layers_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f893ed51-562a-4dfc-8796-08b7f4036381",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[7,42]]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f2007f1-7a09-4bfc-bc19-a486e3a63f86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _ = model(input_ids, \n",
    "              position_ids=torch.tensor([[0,1]]).cuda(), \n",
    "              attention_mask=torch.tensor([[[[1,0],\n",
    "                                             [1,1]]]]).bool().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b891d82-2813-4194-992f-a67fe1101ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_saved_activations = register_activation_hooks(vllm_model, layers_to_save=layers_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4afeb43a-c192-4ce2-a33f-630ca0c59d48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _ = vllm_model(input_ids=input_ids.flatten(), \n",
    "                  positions=torch.tensor([0,1]).cuda(), \n",
    "                  kv_caches=[None]*model.config.num_hidden_layers, \n",
    "                  attn_metadata=attn_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8c371be-e8db-4e5e-beb9-3c3de78ab107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.1 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.2 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.3 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.4 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.5 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.6 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.7 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.8 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.9 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.10 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.11 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.12 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.13 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.14 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.15 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.16 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.17 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.18 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.19 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.20 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.21 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.22 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.23 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.24 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.25 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.26 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.27 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.28 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.29 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.30 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.layers.31 torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n",
      "model.norm torch.Size([1, 2, 4096]) torch.Size([2, 4096]) True\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m hf_saved_activations\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(k, v[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, \u001b[43mvllm_saved_activations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m      3\u001b[0m           torch\u001b[38;5;241m.\u001b[39mallclose(v[\u001b[38;5;241m0\u001b[39m], vllm_saved_activations[k][\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for k,v in hf_saved_activations.items():\n",
    "    print(k, v[0].shape, vllm_saved_activations[k][0].shape,\n",
    "          torch.allclose(v[0], vllm_saved_activations[k][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17a5da10-71ab-47f1-b843-671e2b13032e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5273, -0.7305, -1.4844,  ..., -0.4082, -0.2871,  3.0781],\n",
       "         [ 0.0854, -0.0349, -0.0530,  ...,  0.0356, -0.0376,  0.1953]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.norm(hf_saved_activations['model.layers.31'][0].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb5ef8af-58de-482e-b3d8-c78371d44ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5273, -0.7305, -1.4844,  ..., -0.4082, -0.2871,  3.0781],\n",
       "        [ 0.0854, -0.0349, -0.0530,  ...,  0.0356, -0.0376,  0.1953]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vllm_model.model.norm(vllm_saved_activations['model.layers.31'][0].cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9187ed5-5e80-4724-b578-b8c1423b0df8",
   "metadata": {},
   "source": [
    "#### Compare model\n",
    "\n",
    "Note: `lm_head` is not run with vllm model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "161d0cef-3c9a-402f-a8e2-7375b93e83b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out1 = model(input_ids, \n",
    "             position_ids=torch.tensor([[0,1]]).cuda(), \n",
    "             attention_mask=torch.tensor([[[[1,0],[1,1]]]]).bool().cuda()).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86281556-da6d-49ce-9328-49165adc6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [SequenceGroupMetadata(\"request_id\", True, {0:SequenceData([7,42])}, sampling_params=SamplingParams(temperature=0.0), block_tables=None)]\n",
    "attn_metadata = llm.llm_engine.model_executor.driver_worker.model_runner._prepare_prompt(s)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a1d441a-af87-4991-99fb-84dd1b806326",
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = vllm_model(input_ids=input_ids.flatten(), \n",
    "                  positions=torch.tensor([0,1]).cuda(), \n",
    "                  kv_caches=[None]*model.config.num_hidden_layers, \n",
    "                  attn_metadata=attn_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3da33ebd-0ba6-4e2e-a806-0ad9926f5e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.8438, -0.5234,  4.2500,  ..., -3.7656, -5.1250, -2.0312],\n",
       "         [ 1.7031,  0.5000, -0.0359,  ...,  2.2656,  2.9688,  1.8672]]],\n",
       "       device='cuda:0', grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a64f322-b7a9-433f-8e0e-4e116c96e518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5273, -0.7305, -1.4844,  ..., -0.4082, -0.2871,  3.0781],\n",
       "        [ 0.0854, -0.0349, -0.0530,  ...,  0.0356, -0.0376,  0.1953]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44e5541c-3904-40a7-a8f4-ca9375d3a056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0039,  0.0032, -0.0071,  ...,  0.0053, -0.0082,  0.0070],\n",
       "        [-0.0315,  0.0466, -0.0023,  ..., -0.0211,  0.0173,  0.0334],\n",
       "        [-0.0125,  0.0037,  0.0195,  ..., -0.0271,  0.0143, -0.0082],\n",
       "        ...,\n",
       "        [-0.0281, -0.0195, -0.0024,  ...,  0.0123, -0.0117, -0.0237],\n",
       "        [ 0.0229,  0.0255,  0.0315,  ...,  0.0067, -0.0092, -0.0058],\n",
       "        [ 0.0080, -0.0088,  0.0063,  ..., -0.0293, -0.0200,  0.0337]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e9b2b6e1-fda3-4260-9f31-687669d03c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0039,  0.0032, -0.0071,  ...,  0.0053, -0.0082,  0.0070],\n",
       "        [-0.0315,  0.0466, -0.0023,  ..., -0.0211,  0.0173,  0.0334],\n",
       "        [-0.0125,  0.0037,  0.0195,  ..., -0.0271,  0.0143, -0.0082],\n",
       "        ...,\n",
       "        [-0.0281, -0.0195, -0.0024,  ...,  0.0123, -0.0117, -0.0237],\n",
       "        [ 0.0229,  0.0255,  0.0315,  ...,  0.0067, -0.0092, -0.0058],\n",
       "        [ 0.0080, -0.0088,  0.0063,  ..., -0.0293, -0.0200,  0.0337]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vllm_model.lm_head.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ceca3c-ee2d-49c5-b54f-478b60945cff",
   "metadata": {},
   "source": [
    "### Compare Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e7ca28d-df4e-4c9e-8d68-244dc4d98eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b9546560-2af1-4c77-986f-0a60c57d7505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.sequence import SequenceGroupMetadata, SequenceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b450e2d2-418c-432c-a0cd-502bbfad2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import collections\n",
    "from typing import DefaultDict, Tuple, List, Dict\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def save_activations(\n",
    "        activations: DefaultDict,\n",
    "        name: str,\n",
    "        module: nn.Module,\n",
    "        inp: Tuple,\n",
    "        out: torch.Tensor\n",
    ") -> None:\n",
    "    \"\"\"PyTorch Forward hook to save outputs at each forward\n",
    "    pass. Mutates specified dict objects with each fwd pass.\n",
    "    \"\"\"\n",
    "    if any(l in name for l in [\"qkv\", \"q_proj\",\"k_proj\",\"v_proj\", \"o_proj\", \"gate_up_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]):\n",
    "        if len(out) > 1:\n",
    "            out = out[0]\n",
    "    activations[name].append(out.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "85dfb724-1c57-47c4-8c4a-27feafa568e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_activation_hooks(\n",
    "        model: nn.Module,\n",
    "        layers_to_save: List[str]\n",
    ") -> DefaultDict[List, torch.Tensor]:\n",
    "    \"\"\"Registers forward hooks in specified layers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model:\n",
    "        PyTorch model\n",
    "    layers_to_save:\n",
    "        Module names within ``model`` whose activations we want to save.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    activations_dict:\n",
    "        dict of lists containing activations of specified layers in\n",
    "        ``layers_to_save``.\n",
    "    \"\"\"\n",
    "    activations_dict = collections.defaultdict(list)\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if any(l in name for l in layers_to_save):\n",
    "            module.register_forward_hook(\n",
    "                partial(save_activations, activations_dict, name)\n",
    "            )\n",
    "    return activations_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03cf752-84c9-43af-9d0d-523dffb3388a",
   "metadata": {},
   "source": [
    "### Regular Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b5497346-31de-4afb-8426-d23d1710870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "13b67693-aa60-4e15-9869-f31460cdf642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = LLM(model=\"meta-llama/Llama-2-7b-hf\", tokenizer=\"meta-llama/Llama-2-7b-hf\", dtype=\"bfloat16\", gpu_memory_utilization=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6ab9240b-71b2-4240-adb3-701519684cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vllm_model = llm.llm_engine.model_executor.driver_worker.model_runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16b05929-810a-44d8-b248-6f840012c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in hf_model.named_modules(): \n",
    "#     module._forward_hooks = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0e0c861d-35f1-4363-87de-8da91719e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_saved_activations = register_activation_hooks(hf_model, \n",
    "#                                               layers_to_save=[\"embed\", \"q_proj\", \"k_proj\", \"v_proj\", \n",
    "#                                                               \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a34afa2-817e-4501-9e21-46c87c122cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = torch.tensor([[7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8426d20-f88f-415d-9960-580261293f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     _ = hf_model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7211678-23ef-401f-8be8-bd4df96a8f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vllm_saved_activations = register_activation_hooks(vllm_model, \n",
    "#                                               layers_to_save=[\"embed\", \"qkv_proj\", \"o_proj\", \"gate_up_proj\", \"down_proj\", \"lm_head\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfd28a6a-50b3-49d6-84e8-ac25da0eec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = torch.tensor([7]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9abf4e90-116c-4323-adeb-42da5a4f8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kv_caches = [None] * len(vllm_model.model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be47df25-1121-444e-8d01-e66630189407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positions = torch.tensor([0]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "127619bd-045e-48ea-bdbe-b657cacb8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = [SequenceGroupMetadata(\"1\", True, {0:SequenceData([7,42,1003])}, sampling_params=SamplingParams(temperature=0.0), block_tables=None)]\n",
    "# attn_metadata = llm.llm_engine.model_executor.driver_worker.model_runner._prepare_prompt(s)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3359649-a4b2-45a8-8974-1792eff1a60c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     _ = vllm_model(inp, positions, kv_caches, attn_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2290b72-5979-4f4e-a74a-57b9dd89469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_embed_output = hf_saved_activations['model.embed_tokens'][0]\n",
    "# vllm_embed_output = vllm_saved_activations['model.embed_tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a8b064f1-1c3d-4f18-b6b8-da1271878434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (hf_embed_output - vllm_embed_output).norm() / hf_embed_output.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19678f1b-daef-42f7-ade7-579648cc7895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deltas = {k:[] for k in [\"q\",\"k\",\"v\",\"o\",\"gate\",\"up\",\"down\"]}\n",
    "\n",
    "# for layer in range(len(vllm_model.model.layers)):\n",
    "#     hf_q_output = hf_saved_activations[f'model.layers.{layer}.self_attn.q_proj'][0]\n",
    "#     vllm_q_output = vllm_saved_activations[f'model.layers.{layer}.self_attn.qkv_proj'][0][:,:4096]\n",
    "#     deltas[\"q\"].append((hf_q_output - vllm_q_output).norm(p=2))\n",
    "    \n",
    "#     hf_k_output = hf_saved_activations[f'model.layers.{layer}.self_attn.k_proj'][0]\n",
    "#     vllm_k_output = vllm_saved_activations[f'model.layers.{layer}.self_attn.qkv_proj'][0][:,4096:4096*2]\n",
    "#     deltas[\"k\"].append((hf_k_output - vllm_k_output).norm(p=2))\n",
    "    \n",
    "#     hf_v_output = hf_saved_activations[f'model.layers.{layer}.self_attn.v_proj'][0]\n",
    "#     vllm_v_output = vllm_saved_activations[f'model.layers.{layer}.self_attn.qkv_proj'][0][:,4096*2:]\n",
    "#     deltas[\"v\"].append((hf_v_output - vllm_v_output).norm(p=2))\n",
    "    \n",
    "#     hf_o_output = hf_saved_activations[f'model.layers.{layer}.self_attn.o_proj'][0]\n",
    "#     vllm_o_output = vllm_saved_activations[f'model.layers.{layer}.self_attn.o_proj'][0]\n",
    "#     deltas[\"o\"].append((hf_o_output - vllm_o_output).norm(p=2))\n",
    "    \n",
    "#     hf_gate_output = hf_saved_activations[f'model.layers.{layer}.mlp.gate_proj'][0]\n",
    "#     vllm_gate_output = vllm_saved_activations[f'model.layers.{layer}.mlp.gate_up_proj'][0][:,:11008]\n",
    "#     deltas[\"gate\"].append((hf_gate_output - vllm_gate_output).norm(p=2))\n",
    "    \n",
    "#     hf_up_output = hf_saved_activations[f'model.layers.{layer}.mlp.up_proj'][0]\n",
    "#     vllm_up_output = vllm_saved_activations[f'model.layers.{layer}.mlp.gate_up_proj'][0][:,11008:]\n",
    "#     deltas[\"up\"].append((hf_up_output - vllm_up_output).norm(p=2))\n",
    "    \n",
    "#     hf_down_output = hf_saved_activations[f'model.layers.{layer}.mlp.down_proj'][0]\n",
    "#     vllm_down_output = vllm_saved_activations[f'model.layers.{layer}.mlp.down_proj'][0]\n",
    "#     deltas[\"down\"].append((hf_down_output - vllm_down_output).norm(p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c23a286-44b4-450a-81bd-fe82352e5691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deltas = {k:torch.stack(v).float().numpy() for k,v in deltas.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ecb6b53-6514-437b-a692-7060a1073e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(12,7))\n",
    "\n",
    "# # Loop through the dictionary and plot each line with a label\n",
    "# for key, values in deltas.items():\n",
    "#     ax.plot(values, label=key)\n",
    "#     ax.set_xticks(range(len(values)))\n",
    "\n",
    "# # Adding a legend to distinguish the lines\n",
    "# ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "75bbd723-ba8e-429f-aa16-0a777604414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(hf_saved_activations, \"/home/ubuntu/models/debug/hf_saved_activations\")\n",
    "# torch.save(vllm_saved_activations, \"/home/ubuntu/models/debug/vllm_saved_activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd3e3f-9fcf-4854-8f06-cfc6488fb815",
   "metadata": {},
   "source": [
    "### Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "09740976-a219-4148-af45-9cbcbd9b0b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules(): \n",
    "    module._forward_hooks = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "302e1a59-0b32-49be-8249-554ef4704717",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_saved_activations = register_activation_hooks(model, \n",
    "                                              layers_to_save=[\"embed\", \"q_proj\", \"k_proj\", \"v_proj\", \n",
    "                                                              \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ac4cbb47-941a-48ef-b497-227e9c25ccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor([[7,42]]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "890a098d-2da9-4fe8-8f08-48ea9c75e30d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _ = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "500b4a00-1155-453d-ad3f-13924f57da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in vllm_model.named_modules(): \n",
    "    module._forward_hooks = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b48d0307-7160-4146-a0b6-281e9775bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_saved_activations = register_activation_hooks(vllm_model, \n",
    "                                              layers_to_save=[\"embed\", \"qkv_proj\",\"q_proj\", \"k_proj\", \"v_proj\", \n",
    "                                                              \"o_proj\", \"gate_proj\", \"up_proj\",\n",
    "                                                              \"gate_up_proj\", \"down_proj\", \"lm_head\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1dd40d7b-54b9-448e-80e4-d5ce2566b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor([7,42]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a29434c5-d674-4072-ba53-83a6c64eeab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_caches = [None] * len(vllm_model.model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "044d13c0-0feb-405a-8f46-df022d780fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = torch.tensor([0,1]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fd142820-099c-44ca-81be-4a426eaa80b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = [SequenceGroupMetadata(\"request_id\", True, {0:SequenceData([7,42])}, sampling_params=SamplingParams(temperature=0.0), block_tables=None)]\n",
    "attn_metadata = llm.llm_engine.model_executor.driver_worker.model_runner._prepare_prompt(s)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "180e049a-e8d9-45de-836d-f42577953b61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _ = vllm_model(inp, positions, kv_caches, attn_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4b340558-3242-46f4-887d-d57a3969dac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_saved_activations['model.layers.0.self_attn.q_proj'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8783794c-ec6b-47c3-9b31-d8c00037da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vllm_saved_activations['model.layers.0.self_attn.qkv_proj'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "712ada76-5a1e-4592-b356-94f3a9f19464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_embed_output = hf_saved_activations['model.embed_tokens'][0]\n",
    "# vllm_embed_output = vllm_saved_activations['model.embed_tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eddd20ff-dfd1-4cf1-80f4-be3fed05bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (hf_embed_output - vllm_embed_output).norm() / hf_embed_output.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8736f8d8-10b6-471b-a4a4-1ccb340ad2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = {k:[] for k in [\"q\",\"k\",\"v\",\"o\",\"gate\",\"up\",\"down\"]}\n",
    "\n",
    "for layer in range(len(vllm_model.model.layers)):\n",
    "    hf_q_output = hf_saved_activations[f'model.layers.{layer}.self_attn.q_proj'][0]\n",
    "    # vllm_q_output = vllm_saved_activations[f'model.layers.{layer}.self_attn.qkv_proj'][0][:,:4096]\n",
    "    vllm_q_output = vllm_saved_activations[f'model.layers.{layer}.self_attn.q_proj'][0]\n",
    "    deltas[\"q\"].append((hf_q_output - vllm_q_output).norm(p=2))\n",
    "    \n",
    "    hf_k_output = hf_saved_activations[f'model.layers.{layer}.self_attn.k_proj'][0]\n",
    "    # vllm_k_output = vllm_saved_activations[f'model.layers.{layer}.self_attn.qkv_proj'][0][:,4096:4096*2]\n",
    "    vllm_k_output = vllm_saved_activations[f'model.layers.{layer}.self_attn.k_proj'][0]\n",
    "    deltas[\"k\"].append((hf_k_output - vllm_k_output).norm(p=2))\n",
    "    \n",
    "    hf_v_output = hf_saved_activations[f'model.layers.{layer}.self_attn.v_proj'][0]\n",
    "    # vllm_v_output = vllm_saved_activations[f'model.layers.{layer}.self_attn.qkv_proj'][0][:,4096*2:]\n",
    "    vllm_v_output = vllm_saved_activations[f'model.layers.{layer}.self_attn.v_proj'][0]\n",
    "    deltas[\"v\"].append((hf_v_output - vllm_v_output).norm(p=2))\n",
    "    \n",
    "    hf_o_output = hf_saved_activations[f'model.layers.{layer}.self_attn.o_proj'][0]\n",
    "    vllm_o_output = vllm_saved_activations[f'model.layers.{layer}.self_attn.o_proj'][0]\n",
    "    deltas[\"o\"].append((hf_o_output - vllm_o_output).norm(p=2))\n",
    "    \n",
    "    hf_gate_output = hf_saved_activations[f'model.layers.{layer}.mlp.gate_proj'][0]\n",
    "    # vllm_gate_output = vllm_saved_activations[f'model.layers.{layer}.mlp.gate_up_proj'][0][:,:11008]\n",
    "    vllm_gate_output = vllm_saved_activations[f'model.layers.{layer}.mlp.gate_proj'][0]\n",
    "    deltas[\"gate\"].append((hf_gate_output - vllm_gate_output).norm(p=2))\n",
    "    \n",
    "    hf_up_output = hf_saved_activations[f'model.layers.{layer}.mlp.up_proj'][0]\n",
    "    # vllm_up_output = vllm_saved_activations[f'model.layers.{layer}.mlp.gate_up_proj'][0][:,11008:]\n",
    "    vllm_up_output = vllm_saved_activations[f'model.layers.{layer}.mlp.up_proj'][0]\n",
    "    deltas[\"up\"].append((hf_up_output - vllm_up_output).norm(p=2))\n",
    "    \n",
    "    hf_down_output = hf_saved_activations[f'model.layers.{layer}.mlp.down_proj'][0]\n",
    "    vllm_down_output = vllm_saved_activations[f'model.layers.{layer}.mlp.down_proj'][0]\n",
    "    deltas[\"down\"].append((hf_down_output - vllm_down_output).norm(p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "033f6a59-08d2-48c0-9c76-510bfc344d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = {k:torch.stack(v).float().numpy() for k,v in deltas.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "98ebb0db-c3bc-49e8-93ac-342849991919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " 'k': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " 'v': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " 'o': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " 'gate': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " 'up': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " 'down': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3ca8f38a-abff-4ca2-90a6-8b969768d4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,7))\n",
    "\n",
    "# Loop through the dictionary and plot each line with a label\n",
    "for key, values in deltas.items():\n",
    "    ax.plot(values, label=key)\n",
    "    ax.set_xticks(range(len(values)))\n",
    "    \n",
    "# Adding a legend to distinguish the lines\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fb39a463-0340-410f-acd5-fbcd8f0c214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(hf_saved_activations, \"/home/ubuntu/models/debug/quant_hf_saved_activations\")\n",
    "torch.save(vllm_saved_activations, \"/home/ubuntu/models/debug/quant_vllm_saved_activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c63e3-5af5-4550-92b6-13d737d60816",
   "metadata": {},
   "source": [
    "### Loaded vs Saved Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cbda28c-cab0-476e-924b-6a6cced74626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quant_state(absmax, shape):\n",
    "    quant_state = QuantState(absmax, dtype=torch.bfloat16)\n",
    "    quant_state.shape = torch.Size(shape)\n",
    "    quant_state.blocksize = 64\n",
    "    quant_state.quant_type = \"nf4\"\n",
    "    quant_state.code = quant_map\n",
    "    return quant_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3ecb93c-cce9-44a7-91a8-a5f3e7aa1dff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cheking: model.embed_tokens.weight\n",
      "Cheking: model.layers.0.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.0.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.0.self_attn.o_proj.weight\n",
      "Cheking: model.layers.0.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.0.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.0.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.0.mlp.down_proj.weight\n",
      "Cheking: model.layers.0.mlp.down_proj.absmax\n",
      "Cheking: model.layers.0.input_layernorm.weight\n",
      "Cheking: model.layers.0.post_attention_layernorm.weight\n",
      "Cheking: model.layers.1.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.1.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.1.self_attn.o_proj.weight\n",
      "Cheking: model.layers.1.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.1.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.1.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.1.mlp.down_proj.weight\n",
      "Cheking: model.layers.1.mlp.down_proj.absmax\n",
      "Cheking: model.layers.1.input_layernorm.weight\n",
      "Cheking: model.layers.1.post_attention_layernorm.weight\n",
      "Cheking: model.layers.2.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.2.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.2.self_attn.o_proj.weight\n",
      "Cheking: model.layers.2.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.2.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.2.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.2.mlp.down_proj.weight\n",
      "Cheking: model.layers.2.mlp.down_proj.absmax\n",
      "Cheking: model.layers.2.input_layernorm.weight\n",
      "Cheking: model.layers.2.post_attention_layernorm.weight\n",
      "Cheking: model.layers.3.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.3.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.3.self_attn.o_proj.weight\n",
      "Cheking: model.layers.3.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.3.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.3.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.3.mlp.down_proj.weight\n",
      "Cheking: model.layers.3.mlp.down_proj.absmax\n",
      "Cheking: model.layers.3.input_layernorm.weight\n",
      "Cheking: model.layers.3.post_attention_layernorm.weight\n",
      "Cheking: model.layers.4.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.4.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.4.self_attn.o_proj.weight\n",
      "Cheking: model.layers.4.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.4.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.4.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.4.mlp.down_proj.weight\n",
      "Cheking: model.layers.4.mlp.down_proj.absmax\n",
      "Cheking: model.layers.4.input_layernorm.weight\n",
      "Cheking: model.layers.4.post_attention_layernorm.weight\n",
      "Cheking: model.layers.5.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.5.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.5.self_attn.o_proj.weight\n",
      "Cheking: model.layers.5.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.5.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.5.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.5.mlp.down_proj.weight\n",
      "Cheking: model.layers.5.mlp.down_proj.absmax\n",
      "Cheking: model.layers.5.input_layernorm.weight\n",
      "Cheking: model.layers.5.post_attention_layernorm.weight\n",
      "Cheking: model.layers.6.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.6.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.6.self_attn.o_proj.weight\n",
      "Cheking: model.layers.6.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.6.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.6.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.6.mlp.down_proj.weight\n",
      "Cheking: model.layers.6.mlp.down_proj.absmax\n",
      "Cheking: model.layers.6.input_layernorm.weight\n",
      "Cheking: model.layers.6.post_attention_layernorm.weight\n",
      "Cheking: model.layers.7.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.7.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.7.self_attn.o_proj.weight\n",
      "Cheking: model.layers.7.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.7.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.7.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.7.mlp.down_proj.weight\n",
      "Cheking: model.layers.7.mlp.down_proj.absmax\n",
      "Cheking: model.layers.7.input_layernorm.weight\n",
      "Cheking: model.layers.7.post_attention_layernorm.weight\n",
      "Cheking: model.layers.8.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.8.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.8.self_attn.o_proj.weight\n",
      "Cheking: model.layers.8.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.8.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.8.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.8.mlp.down_proj.weight\n",
      "Cheking: model.layers.8.mlp.down_proj.absmax\n",
      "Cheking: model.layers.8.input_layernorm.weight\n",
      "Cheking: model.layers.8.post_attention_layernorm.weight\n",
      "Cheking: model.layers.9.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.9.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.9.self_attn.o_proj.weight\n",
      "Cheking: model.layers.9.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.9.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.9.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.9.mlp.down_proj.weight\n",
      "Cheking: model.layers.9.mlp.down_proj.absmax\n",
      "Cheking: model.layers.9.input_layernorm.weight\n",
      "Cheking: model.layers.9.post_attention_layernorm.weight\n",
      "Cheking: model.layers.10.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.10.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.10.self_attn.o_proj.weight\n",
      "Cheking: model.layers.10.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.10.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.10.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.10.mlp.down_proj.weight\n",
      "Cheking: model.layers.10.mlp.down_proj.absmax\n",
      "Cheking: model.layers.10.input_layernorm.weight\n",
      "Cheking: model.layers.10.post_attention_layernorm.weight\n",
      "Cheking: model.layers.11.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.11.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.11.self_attn.o_proj.weight\n",
      "Cheking: model.layers.11.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.11.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.11.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.11.mlp.down_proj.weight\n",
      "Cheking: model.layers.11.mlp.down_proj.absmax\n",
      "Cheking: model.layers.11.input_layernorm.weight\n",
      "Cheking: model.layers.11.post_attention_layernorm.weight\n",
      "Cheking: model.layers.12.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.12.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.12.self_attn.o_proj.weight\n",
      "Cheking: model.layers.12.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.12.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.12.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.12.mlp.down_proj.weight\n",
      "Cheking: model.layers.12.mlp.down_proj.absmax\n",
      "Cheking: model.layers.12.input_layernorm.weight\n",
      "Cheking: model.layers.12.post_attention_layernorm.weight\n",
      "Cheking: model.layers.13.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.13.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.13.self_attn.o_proj.weight\n",
      "Cheking: model.layers.13.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.13.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.13.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.13.mlp.down_proj.weight\n",
      "Cheking: model.layers.13.mlp.down_proj.absmax\n",
      "Cheking: model.layers.13.input_layernorm.weight\n",
      "Cheking: model.layers.13.post_attention_layernorm.weight\n",
      "Cheking: model.layers.14.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.14.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.14.self_attn.o_proj.weight\n",
      "Cheking: model.layers.14.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.14.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.14.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.14.mlp.down_proj.weight\n",
      "Cheking: model.layers.14.mlp.down_proj.absmax\n",
      "Cheking: model.layers.14.input_layernorm.weight\n",
      "Cheking: model.layers.14.post_attention_layernorm.weight\n",
      "Cheking: model.layers.15.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.15.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.15.self_attn.o_proj.weight\n",
      "Cheking: model.layers.15.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.15.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.15.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.15.mlp.down_proj.weight\n",
      "Cheking: model.layers.15.mlp.down_proj.absmax\n",
      "Cheking: model.layers.15.input_layernorm.weight\n",
      "Cheking: model.layers.15.post_attention_layernorm.weight\n",
      "Cheking: model.layers.16.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.16.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.16.self_attn.o_proj.weight\n",
      "Cheking: model.layers.16.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.16.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.16.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.16.mlp.down_proj.weight\n",
      "Cheking: model.layers.16.mlp.down_proj.absmax\n",
      "Cheking: model.layers.16.input_layernorm.weight\n",
      "Cheking: model.layers.16.post_attention_layernorm.weight\n",
      "Cheking: model.layers.17.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.17.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.17.self_attn.o_proj.weight\n",
      "Cheking: model.layers.17.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.17.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.17.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.17.mlp.down_proj.weight\n",
      "Cheking: model.layers.17.mlp.down_proj.absmax\n",
      "Cheking: model.layers.17.input_layernorm.weight\n",
      "Cheking: model.layers.17.post_attention_layernorm.weight\n",
      "Cheking: model.layers.18.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.18.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.18.self_attn.o_proj.weight\n",
      "Cheking: model.layers.18.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.18.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.18.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.18.mlp.down_proj.weight\n",
      "Cheking: model.layers.18.mlp.down_proj.absmax\n",
      "Cheking: model.layers.18.input_layernorm.weight\n",
      "Cheking: model.layers.18.post_attention_layernorm.weight\n",
      "Cheking: model.layers.19.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.19.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.19.self_attn.o_proj.weight\n",
      "Cheking: model.layers.19.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.19.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.19.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.19.mlp.down_proj.weight\n",
      "Cheking: model.layers.19.mlp.down_proj.absmax\n",
      "Cheking: model.layers.19.input_layernorm.weight\n",
      "Cheking: model.layers.19.post_attention_layernorm.weight\n",
      "Cheking: model.layers.20.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.20.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.20.self_attn.o_proj.weight\n",
      "Cheking: model.layers.20.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.20.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.20.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.20.mlp.down_proj.weight\n",
      "Cheking: model.layers.20.mlp.down_proj.absmax\n",
      "Cheking: model.layers.20.input_layernorm.weight\n",
      "Cheking: model.layers.20.post_attention_layernorm.weight\n",
      "Cheking: model.layers.21.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.21.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.21.self_attn.o_proj.weight\n",
      "Cheking: model.layers.21.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.21.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.21.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.21.mlp.down_proj.weight\n",
      "Cheking: model.layers.21.mlp.down_proj.absmax\n",
      "Cheking: model.layers.21.input_layernorm.weight\n",
      "Cheking: model.layers.21.post_attention_layernorm.weight\n",
      "Cheking: model.layers.22.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.22.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.22.self_attn.o_proj.weight\n",
      "Cheking: model.layers.22.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.22.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.22.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.22.mlp.down_proj.weight\n",
      "Cheking: model.layers.22.mlp.down_proj.absmax\n",
      "Cheking: model.layers.22.input_layernorm.weight\n",
      "Cheking: model.layers.22.post_attention_layernorm.weight\n",
      "Cheking: model.layers.23.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.23.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.23.self_attn.o_proj.weight\n",
      "Cheking: model.layers.23.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.23.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.23.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.23.mlp.down_proj.weight\n",
      "Cheking: model.layers.23.mlp.down_proj.absmax\n",
      "Cheking: model.layers.23.input_layernorm.weight\n",
      "Cheking: model.layers.23.post_attention_layernorm.weight\n",
      "Cheking: model.layers.24.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.24.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.24.self_attn.o_proj.weight\n",
      "Cheking: model.layers.24.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.24.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.24.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.24.mlp.down_proj.weight\n",
      "Cheking: model.layers.24.mlp.down_proj.absmax\n",
      "Cheking: model.layers.24.input_layernorm.weight\n",
      "Cheking: model.layers.24.post_attention_layernorm.weight\n",
      "Cheking: model.layers.25.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.25.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.25.self_attn.o_proj.weight\n",
      "Cheking: model.layers.25.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.25.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.25.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.25.mlp.down_proj.weight\n",
      "Cheking: model.layers.25.mlp.down_proj.absmax\n",
      "Cheking: model.layers.25.input_layernorm.weight\n",
      "Cheking: model.layers.25.post_attention_layernorm.weight\n",
      "Cheking: model.layers.26.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.26.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.26.self_attn.o_proj.weight\n",
      "Cheking: model.layers.26.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.26.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.26.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.26.mlp.down_proj.weight\n",
      "Cheking: model.layers.26.mlp.down_proj.absmax\n",
      "Cheking: model.layers.26.input_layernorm.weight\n",
      "Cheking: model.layers.26.post_attention_layernorm.weight\n",
      "Cheking: model.layers.27.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.27.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.27.self_attn.o_proj.weight\n",
      "Cheking: model.layers.27.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.27.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.27.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.27.mlp.down_proj.weight\n",
      "Cheking: model.layers.27.mlp.down_proj.absmax\n",
      "Cheking: model.layers.27.input_layernorm.weight\n",
      "Cheking: model.layers.27.post_attention_layernorm.weight\n",
      "Cheking: model.layers.28.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.28.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.28.self_attn.o_proj.weight\n",
      "Cheking: model.layers.28.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.28.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.28.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.28.mlp.down_proj.weight\n",
      "Cheking: model.layers.28.mlp.down_proj.absmax\n",
      "Cheking: model.layers.28.input_layernorm.weight\n",
      "Cheking: model.layers.28.post_attention_layernorm.weight\n",
      "Cheking: model.layers.29.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.29.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.29.self_attn.o_proj.weight\n",
      "Cheking: model.layers.29.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.29.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.29.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.29.mlp.down_proj.weight\n",
      "Cheking: model.layers.29.mlp.down_proj.absmax\n",
      "Cheking: model.layers.29.input_layernorm.weight\n",
      "Cheking: model.layers.29.post_attention_layernorm.weight\n",
      "Cheking: model.layers.30.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.30.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.30.self_attn.o_proj.weight\n",
      "Cheking: model.layers.30.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.30.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.30.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.30.mlp.down_proj.weight\n",
      "Cheking: model.layers.30.mlp.down_proj.absmax\n",
      "Cheking: model.layers.30.input_layernorm.weight\n",
      "Cheking: model.layers.30.post_attention_layernorm.weight\n",
      "Cheking: model.layers.31.self_attn.qkv_proj.weight\n",
      "Cheking: model.layers.31.self_attn.qkv_proj.absmax\n",
      "Cheking: model.layers.31.self_attn.o_proj.weight\n",
      "Cheking: model.layers.31.self_attn.o_proj.absmax\n",
      "Cheking: model.layers.31.mlp.gate_up_proj.weight\n",
      "Cheking: model.layers.31.mlp.gate_up_proj.absmax\n",
      "Cheking: model.layers.31.mlp.down_proj.weight\n",
      "Cheking: model.layers.31.mlp.down_proj.absmax\n",
      "Cheking: model.layers.31.input_layernorm.weight\n",
      "Cheking: model.layers.31.post_attention_layernorm.weight\n",
      "Cheking: model.norm.weight\n",
      "Cheking: lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check if weights are loaded correctly into vllm model.\n",
    "pack_factor = 2\n",
    "for n, p in model.named_parameters():\n",
    "\n",
    "    print(\"Cheking:\", n)\n",
    "    \n",
    "    if 'qkv_proj' in n:\n",
    "        if 'absmax' in n: continue\n",
    "        \n",
    "        # Loaded qkv\n",
    "        qkv_weight = model.get_parameter(n)\n",
    "        qkv_absmax = model.get_parameter(n.replace(\".weight\", \".absmax\"))\n",
    "        qkv_shape = [qkv_weight.shape[0], qkv_weight.shape[1] * pack_factor]\n",
    "        q_shape   = [qkv_weight.shape[0], qkv_weight.shape[1] * pack_factor // 3]\n",
    "        \n",
    "        absmax = qkv_absmax.contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, qkv_shape)\n",
    "        \n",
    "        W_dq = bnb.functional.dequantize_4bit(qkv_weight.contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "        # Saved q proj\n",
    "        q_proj_weight_name = n.replace(\"qkv_proj\", \"q_proj\")\n",
    "        q_proj_absmax_name = n.replace(\"qkv_proj\", \"q_proj\").replace(\".weight\", \".absmax\")\n",
    "        \n",
    "        absmax = quantized_state_dict[q_proj_absmax_name].cuda().contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, q_shape)\n",
    "        \n",
    "        W_q_proj_dq = bnb.functional.dequantize_4bit(quantized_state_dict[q_proj_weight_name].contiguous().view(-1,1).cuda(), quant_state=quant_state)\n",
    "\n",
    "        # Compare with HF model state dict\n",
    "        param = Params4bit(hf_state_dict[q_proj_weight_name].t(), blocksize=64, compress_statistics=False, quant_type='nf4').cuda()\n",
    "        W_q_proj_dq_hf = dequantize_4bit(param.data, param.quant_state)\n",
    "        \n",
    "        # Saved k proj\n",
    "        k_proj_weight_name = n.replace(\"qkv_proj\", \"k_proj\")\n",
    "        k_proj_absmax_name = n.replace(\"qkv_proj\", \"k_proj\").replace(\".weight\", \".absmax\")\n",
    "        \n",
    "        absmax = quantized_state_dict[k_proj_absmax_name].cuda().contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, q_shape)\n",
    "        \n",
    "        W_k_proj_dq = bnb.functional.dequantize_4bit(quantized_state_dict[k_proj_weight_name].contiguous().view(-1,1).cuda(), quant_state=quant_state)\n",
    "\n",
    "        # Compare with HF model state dict\n",
    "        param = Params4bit(hf_state_dict[k_proj_weight_name].t(), blocksize=64, compress_statistics=False, quant_type='nf4').cuda()\n",
    "        W_k_proj_dq_hf = dequantize_4bit(param.data, param.quant_state)        \n",
    "\n",
    "        # Saved v proj\n",
    "        v_proj_weight_name = n.replace(\"qkv_proj\", \"v_proj\")\n",
    "        v_proj_absmax_name = n.replace(\"qkv_proj\", \"v_proj\").replace(\".weight\", \".absmax\")\n",
    "\n",
    "        absmax = quantized_state_dict[v_proj_absmax_name].cuda().contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, q_shape)\n",
    "\n",
    "        W_v_proj_dq = bnb.functional.dequantize_4bit(quantized_state_dict[v_proj_weight_name].contiguous().view(-1,1).cuda(), quant_state=quant_state)\n",
    "\n",
    "         # Compare with HF model state dict\n",
    "        param = Params4bit(hf_state_dict[v_proj_weight_name].t(), blocksize=64, compress_statistics=False, quant_type='nf4').cuda()\n",
    "        W_v_proj_dq_hf = dequantize_4bit(param.data, param.quant_state)       \n",
    "\n",
    "        # Check whether loaded vs saved weights are equal after dequantization.\n",
    "        assert torch.equal(W_dq, torch.cat([W_q_proj_dq, W_k_proj_dq, W_v_proj_dq], dim=1))\n",
    "\n",
    "        assert torch.equal(W_dq, torch.cat([W_q_proj_dq_hf, W_k_proj_dq_hf, W_v_proj_dq_hf], dim=1))\n",
    "    \n",
    "    \n",
    "    elif 'gate_up_proj' in n:\n",
    "        if 'absmax' in n: continue\n",
    "            \n",
    "        # Loaded gate_up\n",
    "        gate_up_weight = model.get_parameter(n)\n",
    "        gate_up_absmax = model.get_parameter(n.replace(\".weight\", \".absmax\"))\n",
    "        gate_up_shape = [gate_up_weight.shape[0], gate_up_weight.shape[1] * pack_factor]\n",
    "        gate_shape    = [gate_up_weight.shape[0], gate_up_weight.shape[1] * pack_factor // 2]\n",
    "\n",
    "        absmax = gate_up_absmax.contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, gate_up_shape)\n",
    "        \n",
    "        W_dq = bnb.functional.dequantize_4bit(gate_up_weight.contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "        # Saved gate_proj\n",
    "        gate_proj_weight_name = n.replace(\"gate_up_proj\", \"gate_proj\")\n",
    "        gate_proj_absmax_name = n.replace(\"gate_up_proj\", \"gate_proj\").replace(\".weight\", \".absmax\")\n",
    "        \n",
    "        absmax = quantized_state_dict[gate_proj_absmax_name].cuda().contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, gate_shape)\n",
    "        \n",
    "        W_gate_proj_dq = bnb.functional.dequantize_4bit(quantized_state_dict[gate_proj_weight_name].contiguous().view(-1,1).cuda(), quant_state=quant_state)\n",
    "\n",
    "        # Saved up_proj\n",
    "        up_proj_weight_name = n.replace(\"gate_up_proj\", \"up_proj\")\n",
    "        up_proj_absmax_name = n.replace(\"gate_up_proj\", \"up_proj\").replace(\".weight\", \".absmax\")\n",
    "        \n",
    "        absmax = quantized_state_dict[up_proj_absmax_name].cuda().contiguous().view(-1)\n",
    "        quant_state = create_quant_state(absmax, gate_shape)\n",
    "\n",
    "        W_up_proj_dq = bnb.functional.dequantize_4bit(quantized_state_dict[up_proj_weight_name].contiguous().view(-1,1).cuda(), quant_state=quant_state)\n",
    "\n",
    "        # Check whether loaded vs saved weights are equal after dequantization.\n",
    "        assert torch.equal(W_dq, torch.cat([W_gate_proj_dq, W_up_proj_dq], dim=1))\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        # Check whether loaded vs saved weights are equal after dequantization.\n",
    "        assert torch.equal(quantized_state_dict[n].data, p.data.cpu())\n",
    "        \n",
    "        # Loaded gate_up\n",
    "        if any(l in n for l in [\"o_proj\", \"down_proj\"]):\n",
    "            if \"weight\" in n:\n",
    "                weight = model.get_parameter(n)\n",
    "                absmax = model.get_parameter(n.replace(\".weight\", \".absmax\"))\n",
    "                shape = [weight.shape[0], weight.shape[1] * pack_factor]\n",
    "                absmax = absmax.contiguous().view(-1)\n",
    "                quant_state = create_quant_state(absmax, shape)\n",
    "                W_dq = bnb.functional.dequantize_4bit(weight.contiguous().view(-1,1), quant_state=quant_state)\n",
    "\n",
    "                # Compare with HF model state dict\n",
    "                param = Params4bit(hf_state_dict[n].t(), blocksize=64, compress_statistics=False, quant_type='nf4').cuda()\n",
    "                assert torch.equal(W_dq, dequantize_4bit(param.data, param.quant_state))\n",
    "                \n",
    "        else:\n",
    "            # Compare with HF model state dict\n",
    "            assert torch.equal(quantized_state_dict[n].data, hf_state_dict[n])\n",
    "\n",
    "    \n",
    "    if any(l in n for l in [\"qkv_proj\", \"o_proj\", \"gate_up_proj\", \"down_proj\"]) and \"weight\" in n:\n",
    "        module = model.get_submodule(n.rpartition(\".\")[0])\n",
    "        input_size = module.weight.shape[0]\n",
    "        x = torch.randn(1,input_size).cuda().to(torch.bfloat16)\n",
    "        out1 = module(x)\n",
    "        if len(out1) > 1: out1 = out1[0]\n",
    "        out2 = x @ W_dq\n",
    "        \n",
    "        # Check forward pass is correct.\n",
    "        assert torch.equal(out1, out2)\n",
    "\n",
    "    # print(p.view(-1)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3e381a01-7457-4cf6-8994-bc9561af3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check activations VLLM bnb vs HF bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207aba1d-b5e5-42cb-b136-bd2fcdb06694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74e0f2-b15e-4457-8ee2-85e67325aad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2aa61-6684-4ce2-b8c7-c31a3a500697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c4344-3699-4f0f-b3a5-ed2045c2c14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde15183-ee7a-4c32-9406-8a5b4238f79d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32091d7a-06b4-488f-a37d-21d37fd1a59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e6dd3-f798-4ea0-8642-6448cffacaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b433344-8ff8-432c-8638-776c9e2545c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258bca3-acae-4213-b12b-d53b6a201751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5694bb7-06bf-4b63-9ddf-f633e3a3f690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279b99cc-49cf-4b7e-a5c7-95c7f98bcba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
